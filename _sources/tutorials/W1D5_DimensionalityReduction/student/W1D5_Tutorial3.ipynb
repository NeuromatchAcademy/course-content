{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D5_DimensionalityReduction/student/W1D5_Tutorial3.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Dimensionality Reduction & Reconstruction\n",
    "**Week 1, Day 5: Dimensionality Reduction**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Alex Cayco Gajic, John Murray\n",
    "\n",
    "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook we'll learn to apply PCA for dimensionality reduction, using a classic dataset that is often used to benchmark machine learning algorithms: MNIST. We'll also learn how to use PCA for reconstruction and denoising.\n",
    "\n",
    "Overview:\n",
    "- Perform PCA on MNIST\n",
    "- Calculate the variance explained\n",
    "- Reconstruct data with different numbers of PCs\n",
    "- (Bonus) Examine denoising using PCA\n",
    "\n",
    "You can learn more about MNIST dataset [here](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 1: PCA for dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:07.330697Z",
     "iopub.status.busy": "2021-06-25T15:09:07.330085Z",
     "iopub.status.idle": "2021-06-25T15:09:07.494852Z",
     "shell.execute_reply": "2021-06-25T15:09:07.494174Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 1: PCA for dimensionality reduction\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"\", width=730, height=410, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"oO0bbInoO_0\", width=730, height=410, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:07.504792Z",
     "iopub.status.busy": "2021-06-25T15:09:07.504255Z",
     "iopub.status.idle": "2021-06-25T15:09:07.808089Z",
     "shell.execute_reply": "2021-06-25T15:09:07.807302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:07.817093Z",
     "iopub.status.busy": "2021-06-25T15:09:07.816518Z",
     "iopub.status.idle": "2021-06-25T15:09:07.862607Z",
     "shell.execute_reply": "2021-06-25T15:09:07.863052Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets   # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:07.888328Z",
     "iopub.status.busy": "2021-06-25T15:09:07.877221Z",
     "iopub.status.idle": "2021-06-25T15:09:07.895499Z",
     "shell.execute_reply": "2021-06-25T15:09:07.895042Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "\n",
    "def plot_variance_explained(variance_explained):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    variance_explained (numpy array of floats) : Vector of variance explained\n",
    "                                                 for each PC\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
    "           '--k')\n",
    "  plt.xlabel('Number of components')\n",
    "  plt.ylabel('Variance explained')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_MNIST_reconstruction(X, X_reconstructed):\n",
    "  \"\"\"\n",
    "  Plots 9 images in the MNIST dataset side-by-side with the reconstructed\n",
    "  images.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats)               : Data matrix each column\n",
    "                                              corresponds to a different\n",
    "                                              random variable\n",
    "    X_reconstructed (numpy array of floats) : Data matrix each column\n",
    "                                              corresponds to a different\n",
    "                                              random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  ax = plt.subplot(121)\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.title('Data')\n",
    "  plt.clim([0, 250])\n",
    "  ax = plt.subplot(122)\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(np.real(X_reconstructed[k, :]), (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.clim([0, 250])\n",
    "  plt.title('Reconstructed')\n",
    "  plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_MNIST_sample(X):\n",
    "  \"\"\"\n",
    "  Plots 9 images in the MNIST dataset.\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                 different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  plt.clim([0, 250])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_MNIST_weights(weights):\n",
    "  \"\"\"\n",
    "  Visualize PCA basis vector weights for MNIST. Red = positive weights,\n",
    "  blue = negative weights, white = zero weight.\n",
    "\n",
    "  Args:\n",
    "     weights (numpy array of floats) : PCA basis vector\n",
    "\n",
    "  Returns:\n",
    "     Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  cmap = plt.cm.get_cmap('seismic')\n",
    "  plt.imshow(np.real(np.reshape(weights, (28, 28))), cmap=cmap)\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  plt.clim(-.15, .15)\n",
    "  plt.colorbar(ticks=[-.15, -.1, -.05, 0, .05, .1, .15])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def add_noise(X, frac_noisy_pixels):\n",
    "  \"\"\"\n",
    "  Randomly corrupts a fraction of the pixels by setting them to random values.\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats)  : Data matrix\n",
    "     frac_noisy_pixels (scalar) : Fraction of noisy pixels\n",
    "\n",
    "  Returns:\n",
    "     (numpy array of floats)    : Data matrix + noise\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X_noisy = np.reshape(X, (X.shape[0] * X.shape[1]))\n",
    "  N_noise_ixs = int(X_noisy.shape[0] * frac_noisy_pixels)\n",
    "  noise_ixs = np.random.choice(X_noisy.shape[0], size=N_noise_ixs,\n",
    "                               replace=False)\n",
    "  X_noisy[noise_ixs] = np.random.uniform(0, 255, noise_ixs.shape)\n",
    "  X_noisy = np.reshape(X_noisy, (X.shape[0], X.shape[1]))\n",
    "\n",
    "  return X_noisy\n",
    "\n",
    "\n",
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto a new basis.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  Y = np.matmul(X, W)\n",
    "\n",
    "  return Y\n",
    "\n",
    "\n",
    "def get_sample_cov_matrix(X):\n",
    "  \"\"\"\n",
    "  Returns the sample covariance matrix of data X.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Covariance matrix\n",
    "\"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
    "  eigenvectors to be in first two quadrants (if 2D).\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats)    :   Vector of eigenvalues\n",
    "    evectors (numpy array of floats) :   Corresponding matrix of eigenvectors\n",
    "                                         each column corresponds to a different\n",
    "                                         eigenvalue\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
    "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
    "  \"\"\"\n",
    "\n",
    "  index = np.flip(np.argsort(evals))\n",
    "  evals = evals[index]\n",
    "  evectors = evectors[:, index]\n",
    "  if evals.shape[0] == 2:\n",
    "    if np.arccos(np.matmul(evectors[:, 0],\n",
    "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 0] = -evectors[:, 0]\n",
    "    if np.arccos(np.matmul(evectors[:, 1],\n",
    "                           1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 1] = -evectors[:, 1]\n",
    "\n",
    "  return evals, evectors\n",
    "\n",
    "\n",
    "def pca(X):\n",
    "  \"\"\"\n",
    "  Performs PCA on multivariate data. Eigenvalues are sorted in decreasing order\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                   different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)    : Data projected onto the new basis\n",
    "    (numpy array of floats)    : Vector of eigenvalues\n",
    "    (numpy array of floats)    : Corresponding matrix of eigenvectors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = get_sample_cov_matrix(X)\n",
    "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "  evals, evectors = sort_evals_descending(evals, evectors)\n",
    "  score = change_of_basis(X, evectors)\n",
    "\n",
    "  return score, evectors, evals\n",
    "\n",
    "\n",
    "def plot_eigenvalues(evals, limit=True):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "     (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "  plt.xlabel('Component')\n",
    "  plt.ylabel('Eigenvalue')\n",
    "  plt.title('Scree plot')\n",
    "  if limit:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Perform PCA on MNIST\n",
    "\n",
    "The MNIST dataset consists of a 70,000 images of individual handwritten digits. Each image is a 28x28 pixel grayscale image. For convenience, each 28x28 pixel image is often unravelled into a single 784 (=28*28) element vector, so that the whole dataset is represented as a 70,000 x 784 matrix. Each row represents a different image, and each column represents a different pixel.\n",
    " \n",
    "Enter the following cell to load the MNIST dataset and plot the first nine images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:07.899770Z",
     "iopub.status.busy": "2021-06-25T15:09:07.899199Z",
     "iopub.status.idle": "2021-06-25T15:09:37.942780Z",
     "shell.execute_reply": "2021-06-25T15:09:37.941798Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784', as_frame = False)\n",
    "X = mnist.data\n",
    "plot_MNIST_sample(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has an extrinsic dimensionality of 784, much higher than the 2-dimensional examples used in the previous tutorials! To make sense of this data, we'll use dimensionality reduction. But first, we need to determine the intrinsic dimensionality $K$ of the data. One way to do this is to look for an \"elbow\" in the scree plot, to determine which eigenvalues are signficant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Scree plot of MNIST\n",
    "\n",
    "In this exercise you will examine the scree plot in the MNIST dataset.\n",
    "\n",
    "**Steps:**\n",
    "- Perform PCA on the dataset and examine the scree plot. \n",
    "- When do the eigenvalues appear (by eye) to reach zero? (**Hint:** use `plt.xlim` to zoom into a section of the plot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:37.948654Z",
     "iopub.status.busy": "2021-06-25T15:09:37.947410Z",
     "iopub.status.idle": "2021-06-25T15:09:37.950649Z",
     "shell.execute_reply": "2021-06-25T15:09:37.950150Z"
    }
   },
   "outputs": [],
   "source": [
    "help(pca)\n",
    "help(plot_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:37.954753Z",
     "iopub.status.busy": "2021-06-25T15:09:37.953606Z",
     "iopub.status.idle": "2021-06-25T15:09:37.955299Z",
     "shell.execute_reply": "2021-06-25T15:09:37.955709Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: perform PCA and plot the eigenvalues\n",
    "#################################################\n",
    "\n",
    "# perform PCA\n",
    "# score, evectors, evals = ...\n",
    "# plot the eigenvalues\n",
    "# plot_eigenvalues(evals, limit=False)\n",
    "# plt.xlim(...)  # limit x-axis up to 100 for zooming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:37.959740Z",
     "iopub.status.busy": "2021-06-25T15:09:37.959238Z",
     "iopub.status.idle": "2021-06-25T15:09:40.527298Z",
     "shell.execute_reply": "2021-06-25T15:09:40.527767Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_a876e927.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=558 height=414 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_a876e927_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Calculate the variance explained\n",
    "\n",
    "The scree plot suggests that most of the eigenvalues are near zero, with fewer than 100 having large values. Another common way to determine the intrinsic dimensionality is by considering the variance explained. This can be examined with a cumulative plot of the fraction of the total variance explained by the top $K$ components, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{var explained} = \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{i=1}^N \\lambda_i}\n",
    "\\end{equation}\n",
    "\n",
    "The intrinsic dimensionality is often quantified by the $K$ necessary to explain a large proportion of the total variance of the data (often a defined threshold, e.g., 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Plot the explained variance\n",
    "\n",
    "In this exercise you will plot the explained variance.\n",
    "\n",
    "**Steps:**\n",
    "- Fill in the function below to calculate the fraction variance explained as a function of the number of principal componenets. **Hint:** use `np.cumsum`.\n",
    "- Plot the variance explained using `plot_variance_explained`.\n",
    "\n",
    "**Questions:**\n",
    "- How many principal components are required to explain 90% of the variance?\n",
    "- How does the intrinsic dimensionality of this dataset compare to its extrinsic dimensionality?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.532291Z",
     "iopub.status.busy": "2021-06-25T15:09:40.531724Z",
     "iopub.status.idle": "2021-06-25T15:09:40.536042Z",
     "shell.execute_reply": "2021-06-25T15:09:40.535520Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plot_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.541482Z",
     "iopub.status.busy": "2021-06-25T15:09:40.540283Z",
     "iopub.status.idle": "2021-06-25T15:09:40.542113Z",
     "shell.execute_reply": "2021-06-25T15:09:40.542562Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_variance_explained(evals):\n",
    "  \"\"\"\n",
    "  Calculates variance explained from the eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)       : Vector of variance explained\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: calculate the explained variance using the equation\n",
    "  ## from Section 2.\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: calculate explaine variance!\")\n",
    "  #################################################\n",
    "\n",
    "  # cumulatively sum the eigenvalues\n",
    "  csum = ...\n",
    "  # normalize by the sum of eigenvalues\n",
    "  variance_explained = ...\n",
    "\n",
    "  return variance_explained\n",
    "\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: call the function and plot the variance explained\n",
    "#################################################\n",
    "\n",
    "# calculate the variance explained\n",
    "variance_explained = ...\n",
    "\n",
    "# Uncomment to plot the variance explained\n",
    "# plot_variance_explained(variance_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.561531Z",
     "iopub.status.busy": "2021-06-25T15:09:40.560932Z",
     "iopub.status.idle": "2021-06-25T15:09:40.790940Z",
     "shell.execute_reply": "2021-06-25T15:09:40.790441Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_0f5f51b9.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_0f5f51b9_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Reconstruct data with different numbers of PCs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Video 2: Data Reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.803017Z",
     "iopub.status.busy": "2021-06-25T15:09:40.802431Z",
     "iopub.status.idle": "2021-06-25T15:09:40.959414Z",
     "shell.execute_reply": "2021-06-25T15:09:40.956237Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Data Reconstruction\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"\", width=730, height=410, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"ZCUhW26AdBQ\", width=730, height=410, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen that the top 100 or so principal components of the data can explain most of the variance. We can use this fact to perform *dimensionality reduction*, i.e., by storing the data using only 100 components rather than the samples of all 784 pixels. Remarkably, we will be able to reconstruct much of the structure of the data using only the top 100 components. To see this, recall that to perform PCA we projected the data $\\bf X$ onto the eigenvectors of the covariance matrix:\n",
    "\\begin{equation}\n",
    "\\bf S = X W\n",
    "\\end{equation}\n",
    "Since $\\bf W$ is an orthogonal matrix, ${\\bf W}^{-1} = {\\bf W}^T$. So by multiplying by ${\\bf W}^T$ on each side we can rewrite this equation as  \n",
    "\\begin{equation}\n",
    "{\\bf X = S W}^T.\n",
    "\\end{equation}\n",
    "This now gives us a way to reconstruct the data matrix from the scores and loadings. To reconstruct the data from a low-dimensional approximation, we just have to truncate these matrices.  Let's call ${\\bf S}_{1:K}$ and ${\\bf W}_{1:K}$ as keeping only the first $K$ columns of this matrix. Then our reconstruction is:\n",
    "\\begin{equation}\n",
    "{\\bf \\hat X = S}_{1:K} ({\\bf W}_{1:K})^T.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Data reconstruction\n",
    "\n",
    "Fill in the function below to reconstruct the data using different numbers of principal components. \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "* Fill in the following function to reconstruct the data based on the weights and scores. Don't forget to add the mean!\n",
    "* Make sure your function works by reconstructing the data with all $K=784$ components. The two images should look identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.969212Z",
     "iopub.status.busy": "2021-06-25T15:09:40.967914Z",
     "iopub.status.idle": "2021-06-25T15:09:40.971050Z",
     "shell.execute_reply": "2021-06-25T15:09:40.970605Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plot_MNIST_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.977364Z",
     "iopub.status.busy": "2021-06-25T15:09:40.976121Z",
     "iopub.status.idle": "2021-06-25T15:09:40.977990Z",
     "shell.execute_reply": "2021-06-25T15:09:40.978439Z"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct_data(score, evectors, X_mean, K):\n",
    "  \"\"\"\n",
    "  Reconstruct the data based on the top K components.\n",
    "\n",
    "  Args:\n",
    "    score (numpy array of floats)    : Score matrix\n",
    "    evectors (numpy array of floats) : Matrix of eigenvectors\n",
    "    X_mean (numpy array of floats)   : Vector corresponding to data mean\n",
    "    K (scalar)                       : Number of components to include\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Matrix of reconstructed data\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: Reconstruct the original data in X_reconstructed\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: reconstructing data function!\")\n",
    "  #################################################\n",
    "\n",
    "  # Reconstruct the data from the score and eigenvectors\n",
    "  # Don't forget to add the mean!!\n",
    "  X_reconstructed =  ...\n",
    "\n",
    "  return X_reconstructed\n",
    "\n",
    "\n",
    "K = 784\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: Calculate the mean and call the function, then plot\n",
    "## the original and the recostructed data\n",
    "#################################################\n",
    "\n",
    "# Reconstruct the data based on all components\n",
    "X_mean = ...\n",
    "X_reconstructed = ...\n",
    "\n",
    "# Plot the data and reconstruction\n",
    "# plot_MNIST_reconstruction(X, X_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:40.983747Z",
     "iopub.status.busy": "2021-06-25T15:09:40.983179Z",
     "iopub.status.idle": "2021-06-25T15:09:42.560500Z",
     "shell.execute_reply": "2021-06-25T15:09:42.558438Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_e3395916.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=557 height=289 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_e3395916_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo: Reconstruct the data matrix using different numbers of PCs\n",
    "\n",
    "Now run the code below and experiment with the slider to reconstruct the data matrix using different numbers of principal components.\n",
    "\n",
    "**Steps**\n",
    "* How many principal components are necessary to reconstruct the numbers (by eye)? How does this relate to the intrinsic dimensionality of the data?\n",
    "* Do you see any information in the data with only a single principal component?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make sure you execute this cell to enable the widget!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:42.583143Z",
     "iopub.status.busy": "2021-06-25T15:09:42.582522Z",
     "iopub.status.idle": "2021-06-25T15:09:43.468182Z",
     "shell.execute_reply": "2021-06-25T15:09:43.467574Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "\n",
    "def refresh(K=100):\n",
    "  X_reconstructed = reconstruct_data(score, evectors, X_mean, K)\n",
    "  plot_MNIST_reconstruction(X, X_reconstructed)\n",
    "  plt.title('Reconstructed, K={}'.format(K))\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, K=(1, 784, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Visualization of the weights\n",
    "\n",
    "Next, let's take a closer look at the first principal component by visualizing its corresponding weights. \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "* Enter `plot_MNIST_weights` to visualize the weights of the first basis vector.\n",
    "* What structure do you see? Which pixels have a strong positive weighting? Which have a strong negative weighting? What kinds of images would this basis vector differentiate?\n",
    "* Try visualizing the second and third basis vectors. Do you see any structure? What about the 100th basis vector? 500th? 700th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.475035Z",
     "iopub.status.busy": "2021-06-25T15:09:43.473740Z",
     "iopub.status.idle": "2021-06-25T15:09:43.476755Z",
     "shell.execute_reply": "2021-06-25T15:09:43.476404Z"
    }
   },
   "outputs": [],
   "source": [
    "help(plot_MNIST_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.480435Z",
     "iopub.status.busy": "2021-06-25T15:09:43.479347Z",
     "iopub.status.idle": "2021-06-25T15:09:43.481787Z",
     "shell.execute_reply": "2021-06-25T15:09:43.481279Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: plot the weights calling the plot_MNIST_weights function\n",
    "#################################################\n",
    "\n",
    "# Plot the weights of the first principal component\n",
    "# plot_MNIST_weights(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.501123Z",
     "iopub.status.busy": "2021-06-25T15:09:43.500579Z",
     "iopub.status.idle": "2021-06-25T15:09:43.765739Z",
     "shell.execute_reply": "2021-06-25T15:09:43.765246Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_f358e413.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=499 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_f358e413_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "* In this tutorial, we learned how to use PCA for dimensionality reduction by selecting the top principal components. This can be useful as the intrinsic dimensionality ($K$) is often less than the extrinsic dimensionality ($N$) in neural data. $K$ can be inferred by choosing the number of eigenvalues necessary to capture some fraction of the variance.\n",
    "* We also learned how to reconstruct an approximation of the original data using the top $K$ principal components. In fact, an alternate formulation of PCA is to find the $K$ dimensional space that minimizes the reconstruction error.\n",
    "* Noise tends to inflate the apparent intrinsic dimensionality, however the higher components reflect noise rather than new structure in the data. PCA can be used for denoising data by removing noisy higher components.\n",
    "* In MNIST, the weights corresponding to the first principal component appear to discriminate between a 0 and 1. We will discuss the implications of this for data visualization in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus: Examine denoising using PCA\n",
    "\n",
    "In this lecture, we saw that PCA finds an optimal low-dimensional basis to minimize the reconstruction error. Because of this property, PCA can be useful for denoising corrupted samples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Add noise to the data\n",
    "In this exercise you will add salt-and-pepper noise to the original data and see how that affects the eigenvalues. \n",
    "\n",
    "**Steps:**\n",
    "- Use the function `add_noise` to add noise to 20% of the pixels.\n",
    "- Then, perform PCA and plot the variance explained. How many principal components are required to explain 90% of the variance? How does this compare to the original data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.770033Z",
     "iopub.status.busy": "2021-06-25T15:09:43.769529Z",
     "iopub.status.idle": "2021-06-25T15:09:43.773275Z",
     "shell.execute_reply": "2021-06-25T15:09:43.772855Z"
    }
   },
   "outputs": [],
   "source": [
    "help(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.778114Z",
     "iopub.status.busy": "2021-06-25T15:09:43.776712Z",
     "iopub.status.idle": "2021-06-25T15:09:43.778730Z",
     "shell.execute_reply": "2021-06-25T15:09:43.779160Z"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Insert your code here to:\n",
    "# Add noise to the data\n",
    "# Plot noise-corrupted data\n",
    "# Perform PCA on the noisy data\n",
    "# Calculate and plot the variance explained\n",
    "###################################################################\n",
    "np.random.seed(2020)  # set random seed\n",
    "X_noisy = ...\n",
    "# score_noisy, evectors_noisy, evals_noisy = ...\n",
    "# variance_explained_noisy = ...\n",
    "# plot_MNIST_sample(X_noisy)\n",
    "# plot_variance_explained(variance_explained_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:43.783288Z",
     "iopub.status.busy": "2021-06-25T15:09:43.782813Z",
     "iopub.status.idle": "2021-06-25T15:09:49.602867Z",
     "shell.execute_reply": "2021-06-25T15:09:49.601928Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_d4a41b8c.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_d4a41b8c_0.png>\n",
    "\n",
    "<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_d4a41b8c_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Denoising\n",
    "\n",
    "Next, use PCA to perform denoising by projecting the noise-corrupted data onto the basis vectors found from the original dataset. By taking the top K components of this projection, we can reduce noise in dimensions orthogonal to the K-dimensional latent space. \n",
    "\n",
    "**Steps:**\n",
    "- Subtract the mean of the noise-corrupted data.\n",
    "- Project the data onto the basis found with the original dataset (`evectors`, not `evectors_noisy`) and take the top $K$ components. \n",
    "- Reconstruct the data as normal, using the top 50 components. \n",
    "- Play around with the amount of noise and K to build intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:49.611519Z",
     "iopub.status.busy": "2021-06-25T15:09:49.610967Z",
     "iopub.status.idle": "2021-06-25T15:09:49.615143Z",
     "shell.execute_reply": "2021-06-25T15:09:49.614427Z"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Insert your code here to:\n",
    "# Subtract the mean of the noise-corrupted data\n",
    "# Project onto the original basis vectors evectors\n",
    "# Reconstruct the data using the top 50 components\n",
    "# Plot the result\n",
    "###################################################################\n",
    "\n",
    "X_noisy_mean = ...\n",
    "projX_noisy = ...\n",
    "X_reconstructed = ...\n",
    "# plot_MNIST_reconstruction(X_noisy, X_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {
     "iopub.execute_input": "2021-06-25T15:09:49.619545Z",
     "iopub.status.busy": "2021-06-25T15:09:49.618994Z",
     "iopub.status.idle": "2021-06-25T15:09:51.489951Z",
     "shell.execute_reply": "2021-06-25T15:09:51.490432Z"
    }
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D5_DimensionalityReduction/solutions/W1D5_Tutorial3_Solution_e3ee8262.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=557 height=289 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D5_DimensionalityReduction/static/W1D5_Tutorial3_Solution_e3ee8262_0.png>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D5_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}