{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shared_Tutorial3_BAYESDAY_colab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/fix_notebook1/tutorials/Bayes/BayesDay_Tutorial_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ICwW1AANyqVc"
      },
      "source": [
        "## Neuromatch Academy 2020 -- Bayes Day (dry run)\n",
        "# Tutorial 3 - Bayesian decision theory & Cost functions\n",
        "\n",
        "Please execute the cell below to initialize the notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "JkdIcrE1yqVd",
        "colab": {}
      },
      "source": [
        "# @title\n",
        "\n",
        "import time                        # import time \n",
        "import numpy as np                 # import numpy\n",
        "import scipy as sp                 # import scipy\n",
        "import math                        # import basic math functions\n",
        "import random                      # import basic random number generator functions\n",
        "\n",
        "import matplotlib.pyplot as plt    # import matplotlib\n",
        "import matplotlib as mpl\n",
        "from IPython import display        \n",
        "from scipy.optimize import minimize\n",
        "\n",
        "fig_w, fig_h = (6, 4)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "plt.style.use('ggplot')\n",
        "mpl.rc('figure', max_open_warning = 0)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LqkrbIEeyqVl"
      },
      "source": [
        "---\n",
        "### Tutorial Objectives\n",
        "\n",
        "In this notebook we'll have a look at the impact of different cost functions (0-1 Loss, Absolute Error, and Mean Squared Error) on Bayesian Decision Theory.\n",
        "\n",
        "Particularly, we will implement the following cost functions:\n",
        "  - the Mean Squared Error\n",
        "  - the Zero-One Loss \n",
        "  - the absolute error\n",
        "  \n",
        "and we will compare these to the mean, mode and median of our posterior distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zy7q1uL2yqVl"
      },
      "source": [
        "---\n",
        "### EXERCISE 1: Calculate the posterior from a Gaussian Likelihood and Mixture of Gaussians Prior\n",
        "   \n",
        "We now want to create a prior matrix that is the result of a mixture of gaussians.\n",
        "\n",
        "We provide you with `my_gaussian` functions, and a code skeleton to plot the resulting prior\n",
        "\n",
        "**Suggestions**\n",
        "\n",
        "  Using the equation for the un-normalised Gaussian `my_gaussian`:\n",
        "* Generate a Gaussian with mean 0 and standard deviation 0.5\n",
        "* Generate another Gaussian with mean 0 and standard deviation 3\n",
        "* Combine the two Gaussians to make a new prior by adding the two Gaussians together with mixing parameter $\\alpha$ = 0.05. Make it such that the peakier Gaussian has 95% of the weight (don't forget to normalize afterwards)\n",
        "* Generate a Likelihood with mean -2.7 and standard deviation 1\n",
        "* Compute the Posterior using Bayes rule\n",
        "* Plot the resulting Prior, Likelihood and Posterior using the plotting code snippet already provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "88AkTkeMsy9Z",
        "colab": {}
      },
      "source": [
        "def my_gaussian(x_points, mu, sigma):\n",
        "  \"\"\"\n",
        "  DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "  Returns un-normalized Gaussian estimated at points `x_points`, with parameters: `mu` and `sigma`\n",
        "\n",
        "  Args : \n",
        "    x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
        "    mu (scalar) - mean of the Gaussian\n",
        "    sigma (scalar) - std of the gaussian\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats): un-normalized Gaussian (i.e. without constant) evaluated at `x`\n",
        "  \"\"\"\n",
        "  return np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
        "\n",
        "x=np.arange(-5,5,0.01)\n",
        "\n",
        "###############################################################################\n",
        "## Insert your code here to:\n",
        "##        Create a Gaussian prior made of two Gaussian\n",
        "##        Both with mean 0 and std 0.5 and 3 respectively\n",
        "##        Make the combined prior (made of the two Gaussians) by weighing it\n",
        "##        using a mixing parameter alpha = 0.05 such that the peakier Gaussian has\n",
        "##        weight 0.95\n",
        "##        Implement a likelihood with mean -2.7 and standard deviation 1\n",
        "##        Calculate the posterior using Bayes rule\n",
        "##        Use the code snippet provided to plot the functions\n",
        "###############################################################################\n",
        "\n",
        "# prior_combined = ...\n",
        "\n",
        "# likelihood = ...\n",
        "\n",
        "# posterior = ...\n",
        "\n",
        "# plt.figure\n",
        "# plt.plot(x, prior_combined, '-r', linewidth=2, label='Prior')\n",
        "# plt.plot(x, likelihood, '-b', linewidth=2, label='Likelihood')\n",
        "# plt.plot(x, posterior, '-g', linewidth=4, label='Posterior')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFba0iKR_QSJ",
        "colab_type": "text"
      },
      "source": [
        "`<TODO>`: Sample image goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lzYSxCRuyqVw"
      },
      "source": [
        "---\n",
        "### EXERCISE 2: Compute and compare the different Loss functions\n",
        "    \n",
        "We now want to calculate the expected Loss for different loss functions, and compare these to the mean, median and mode of the posterior we calculated above.\n",
        "\n",
        "As a reminder, the loss functions are defined as follows:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\text{MeanSquaredError} = (\\hat{x} - x)^2\n",
        "\\end{eqnarray}\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\text{AbsoluteError} = |\\hat{x} - x|\n",
        "\\end{eqnarray}\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\text{ZeroOneError} = \\begin{cases}\n",
        "    0,& \\text{if } \\hat{x} = x\\\\\n",
        "    1,              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{eqnarray}\n",
        "\n",
        "and the Expected Loss is:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    \\mathbb{E}[\\text{Loss}] = \\int L[\\hat{x},x] \\odot  p(x|\\tilde{x}) dx\n",
        "\\end{eqnarray}\n",
        "\n",
        "where $L[\\hat{x},x]$ is the loss function, and $p(x|\\tilde{x})$ is the posterior computed in exercise 1.\n",
        "\n",
        "**Suggestions**\n",
        "\n",
        "  - Calculate the Mean Squared Error (MSE) Loss between x_hat estimate & all x values\n",
        "  - Compute the Expected MSE Loss ($\\mathbb{E}[MSE Loss]$) using your posterior & your MSE Loss (Marginalize)\n",
        "  - Calculate the Absolute Error Loss between $\\hat x$ estimate & all $x$ values\n",
        "  - Compute the Expected Absolute Error Loss ($\\mathbb{E}[Abs E. Loss]$) using your posterior & your Absolute Error Loss (Marginalize)\n",
        "  - Calculate the Zero-One Loss between x_hat estimate & all x values (use `np.isclose()` to compare $\\hat x$ to $x$)\n",
        "  - Compute the Expected Zero-One Loss ($\\mathbb{E}[01 Loss]$) using your posterior & your Zero-One Loss (Marginalize)\n",
        "  - Calculate the x position that minimizes the Expected Loss for MSE, Absolute Loss, and ZeroOne Loss\n",
        "\n",
        "  What do you conclude when comparing the mean, median, and mode of your posterior to the minimized cost functions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bz3kr6RA1IDp",
        "colab": {}
      },
      "source": [
        "def moments_myfunc(x_points, function):\n",
        "    \"\"\"\n",
        "    DO NOT EDIT THIS FUNCTION !!!\n",
        "\n",
        "    Returns the mean, median and mode of an arbitrary function\n",
        "\n",
        "    Args : \n",
        "    x_points (numpy array of floats) - x-axis values\n",
        "    function (numpy array of floats) - y-axis values of the function evaluated at `x_points`\n",
        "\n",
        "    Returns:\n",
        "    (tuple of 3 scalars): mean, median, mode\n",
        "    \"\"\"\n",
        "\n",
        "    # Calc mode of arbitrary function\n",
        "    mode = x_points[np.argmax(function)]\n",
        "\n",
        "    # Calc mean of arbitrary function\n",
        "    mean = np.sum(x_points * function)\n",
        "\n",
        "    # Calc median of arbitrary function\n",
        "    cdf_function = np.zeros_like(x_points)\n",
        "    accumulator = 0\n",
        "    for i in np.arange(x.shape[0]):\n",
        "        accumulator = accumulator + posterior[i]\n",
        "        cdf_function[i] = accumulator\n",
        "    idx = np.argmin(np.abs(cdf_function - 0.5))\n",
        "    median = x_points[idx]\n",
        "\n",
        "    return mean, median, mode\n",
        "\n",
        "x=np.arange(-5,5,0.01)\n",
        "\n",
        "mean, median, mode = moments_myfunc(x, posterior)\n",
        "print(f\"Posterior mean is : {mean:.2f}, Posterior median is : {median:.2f}, Posterior mode is : {mode:.2f}\")\n",
        "\n",
        "ExpectedLoss_MSE = np.zeros_like(x)\n",
        "ExpectedLoss_Abse= np.zeros_like(x)\n",
        "ExpectedLoss_01  = np.zeros_like(x)\n",
        "\n",
        "###############################################################################\n",
        "## Complete the code below to:\n",
        "##        Calculate the Mean Squared Error (MSE) Loss between x_hat estimate & all x values\n",
        "##        Compute the Expected MSE Loss using your posterior & your MSE Loss (Marginalize)\n",
        "##      \n",
        "##        Calculate the Absolute Error Loss between x_hat estimate & all x values\n",
        "##        Compute the Expected Absolute Error Loss using your posterior & your Absolute Error Loss (Marginalize)\n",
        "##        \n",
        "##        Calculate the Zero-One Loss between x_hat estimate & all x values (use np.isclose() to compare and fine x_hat == x)\n",
        "##        Compute the Expected Zero-One Loss using your posterior & your Zero-One Loss (Marginalize)\n",
        "##        \n",
        "##        Calculate the x position that minimizes the Expected Loss for MSE, Absolute Loss, and ZeroOne Loss\n",
        "##        What do you conclude when comparing the mean, median, and mode of your posterior to the minimized cost functions?\n",
        "###############################################################################\n",
        "\n",
        "# # Looping over potential x_hats ('decisions')\n",
        "# for idx in np.arange(x.shape[0]):\n",
        "#   estimate = x[idx] #This is you current x_hat estimate for which you want to estimate the Expected Loss\n",
        "\n",
        "#   MSELoss = ... #Calculate the Mean Squared Error Loss between estimate & x\n",
        "#   ExpectedLoss_MSE[idx] = ...\n",
        "\n",
        "#   AbsELoss = ... #Calculate the Absolute Error Loss between estimate & x\n",
        "#   ExpectedLoss_Abse[idx] = ...\n",
        "\n",
        "#   ZeroOneLoss = ... #Calculate the 0-1 Loss between estimate & x\n",
        "#   ExpectedLoss_01[idx] = ...\n",
        "\n",
        "# min_MSE = ...\n",
        "# min_Abse = ...\n",
        "# min_01 = ...\n",
        "\n",
        "# print(f\"Minimum of MSE is : {min_MSE:.2f}, Minimum of Abs_error is : {min_Abse:.2f}, Minimum of 01_loss is : {min_01:.2f}\")\n",
        "\n",
        "# # Plotting snippet\n",
        "# fig, ax = plt.subplots(2,figsize=(13,13), sharex=True)\n",
        "# ax[0].plot(x, ExpectedLoss_MSE, '-r', linewidth=2, label='Mean Squared Error')\n",
        "# ax[0].axvline(min_MSE, ls='dashed', color='red', label='Min MSE')\n",
        "# ax[0].plot(x, ExpectedLoss_Abse, '-b', linewidth=2, label='Absolute Error')\n",
        "# ax[0].axvline(min_Abse, ls='dashdot', color='blue', label='Min Abs_error')\n",
        "# ax[0].plot(x, ExpectedLoss_01, '-g', linewidth=4, label='Zero One Loss')\n",
        "# ax[0].axvline(min_01, ls='dotted', color='green', label='Min 01_Loss')\n",
        "# ax[0].set_ylabel('Error Loss')\n",
        "# ax[0].set_xlabel('Orientation (Degrees)')\n",
        "# ax[0].legend()\n",
        "\n",
        "# ax[1].plot(x, prior_combined, '-r', linewidth=2, label='Prior')\n",
        "# ax[1].plot(x, likelihood, '-b', linewidth=2, label='Likelihood')\n",
        "# ax[1].plot(x, posterior, '-g', linewidth=4, label='Posterior')\n",
        "# ax[1].axvline(mean, ls='dashed', color='red', label='Mean')\n",
        "# ax[1].axvline(median, ls='dashdot', color='blue', label='Median')\n",
        "# ax[1].axvline(mode, ls='dotted', color='green', label='Mode')\n",
        "# ax[1].set_ylabel('Probability')\n",
        "# ax[1].set_xlabel('Orientation (Degrees)')\n",
        "# ax[1].legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}