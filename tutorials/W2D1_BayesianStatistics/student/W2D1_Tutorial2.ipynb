{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W2D1_BayesianStatistics/student/W2D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 2, Day 1, Tutorial 2\n",
    "# Causal inference with mixture of Gaussians\n",
    "\n",
    "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matt Krause\n",
    "\n",
    "__Content reviewers:__ Matt Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "The previous notebook introduced Gaussians and Bayes' rule, allowing us to model very simple combinations of auditory and visual input. In this and the following notebook, we will use those building blocks to explore more complicated sensory integration and ventriloquism! \n",
    "\n",
    "In this notebook, you will:\n",
    "1. Learn more about the problem setting, which we wil also use in Tutorial 3,\n",
    "2. Implement a mixture-of-Gaussian prior, and\n",
    "3. Explore how that prior produces more complex posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "422497ee-b213-4f13-db03-85476ca822b7"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Introduction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='GdIwJWsW9-s', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "##Setup  \n",
    "Please execute the cells below to initialize the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "    \"\"\"\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Returns normalized Gaussian estimated at points `x_points`, with parameters `mu` and `sigma`\n",
    "\n",
    "    Args:\n",
    "      x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
    "      mu (scalar) - mean of the Gaussian\n",
    "      sigma (scalar) - standard deviation of the gaussian\n",
    "    Returns:\n",
    "      (numpy array of floats): normalized Gaussian (i.e. without constant) evaluated at `x`\n",
    "    \"\"\"\n",
    "    px = np.exp(- 1/2/sigma**2 * (mu - x_points) ** 2)\n",
    "\n",
    "    px = px / px.sum() # this is the normalization part with a very strong assumption, that\n",
    "                       # x_points cover the big portion of probability mass around the mean.\n",
    "                       # Please think/discuss when this would be a dangerous assumption.\n",
    "\n",
    "    return px\n",
    "\n",
    "def plot_mixture_prior(x, gaussian1, gaussian2, combined):\n",
    "    \"\"\"\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Plots a prior made of a mixture of gaussians\n",
    "\n",
    "    Args:\n",
    "      x (numpy array of floats):         points at which the likelihood has been evaluated\n",
    "      gaussian1 (numpy array of floats): normalized probabilities for Gaussian 1 evaluated at each `x`\n",
    "      gaussian2 (numpy array of floats): normalized probabilities for Gaussian 2 evaluated at each `x`\n",
    "      posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, gaussian1, '--b', LineWidth=2, label='Gaussian 1')\n",
    "    ax.plot(x, gaussian2, '-.b', LineWidth=2, label='Gaussian 2')\n",
    "    ax.plot(x, combined, '-r', LineWidth=2, label='Gaussian Mixture')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xlabel('Orientation (Degrees)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 1: Motivating example\n",
    "\n",
    "Ventriloquists produce the illusion that their puppets are talking because:\n",
    "1. We observe the visual input of the puppet moving its mouth, as if speaking.\n",
    "2. The speech that the puppeteer generates originates near the puppet's mouth. \n",
    "\n",
    "Since we are accustomed to voices coming from moving mouths, we tend to interpret the voice as coming directly from the puppet itself rather than from the puppeteer (who is also hiding his/her own mouth movements). In the remaining tutorials, we will study how this illusion breaks down as the distance between the visual stimulus (the puppet's mouth) and the auditory stimulus (the puppeter's concealed speech) changes. \n",
    "\n",
    "Imagine an experiment where participants are shown a puppet moving its mouth at a location directly in front of them (at position 0Ëš). The subjects are told that 75% of the time, the voice they hear originates from the puppet. On the remaining 25% of trials, sounds come from elsewhere. Participants learn this over multiple trials, after which a curtain is dropped in front of the puppeteer and the puppet. \n",
    "\n",
    "Next, we present only the auditory stimulus at varying locations and we ask participants to report where the source of the sound is located. The participants have access to two pieces of information:\n",
    "\n",
    "*   The prior information about sound localization, learned during the trials before the curtain fell.\n",
    "*   Their noisy sensory estimates about where a particular sound originates. \n",
    "\n",
    "Our eventual goal, which we achieve in Tutorial 3, is to predict the subjects' responses: when do subjects ascribe a sound to the puppet, and when do they believe it originated elsewhere? Doing so requires building a prior that captures the participant's knowledge and expectations, which we wil do in the exercises that follow here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 2: Mixture-of-Gaussians Prior\n",
    "\n",
    "In the previous tutorial, you learned how to create a single Gaussian prior that could represent one of these possibilties. A broad Gaussian with a large $\\sigma$ could represent sounds originating from nearly anywhere, while a narrow Gaussian with $\\mu$ near zero could represent sounds orginating from the puppet. \n",
    "\n",
    "Here, we will combine those into a mixure-of-Gaussians probability density function (PDF) that captures both possibilties. We will control how the Gaussians are mixed by summing them together with a 'mixing' or weight parameter $p_{common}$, set to a value between 0 and 1, like so:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\text{Mixture} = \\bigl[\\; p_{common} \\times \\mathcal{N}(\\mu_{common},\\sigma_{common}) \\; \\bigr] + \\bigl[ \\;\\underbrace{(1-p_{common})}_{p_{independent}} \\times \\mathcal{N}(\\mu_{independent},\\sigma_{independent}) \\; \\bigr]\n",
    "\\end{eqnarray}\n",
    "\n",
    "$p_{common}$ denotes the probability that auditory stimulus shares a \"common\" source with the learnt visual input; in other words, the probability that the \"puppet\" is speaking. You might think that we need to include a separate weight for the possibility that sound is \"independent\" from the puppet. nHowever, since there are only two, mutually-exclusive possibilties, we can replace $p_{independent}$ with $(1 - p_{common})$ since, by the law of total probability, $p_{common} + p_{independent}$ must equal one. \n",
    "\n",
    "Using the formula above, complete the code to build this mixture-of-Gaussians PDF: \n",
    "* Generate a Gaussian with mean 0 and standard deviation 0.5 to be the 'common' part of the Gaussian mixture prior. (This is already done for you below).\n",
    "* Generate another Gaussian with mean 0 and standard deviation 3 to serve as the 'independent' part. \n",
    "* Combine the two Gaussians to make a new prior by mixing the two Gaussians with mixing parameter $p_{common}$ = 0.75 so that the peakier \"common-cause\" Gaussian has 75% of the weight. Don't forget to normalize afterwards! \n",
    "\n",
    "Hints:\n",
    "* Code for the `my_gaussian` function from Tutorial 1 is available for you to use. Its documentation is below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Helper function(s)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "outputId": "5e888a13-7abb-4cbc-a623-ae600d677ead"
   },
   "outputs": [],
   "source": [
    "help(my_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 1: Implement the prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def mixture_prior(x, mean=0, sigma_common=0.5, sigma_independent=3, p_common=0.75):\n",
    "\n",
    "  ###############################################################################\n",
    "  ## Insert your code here to:\n",
    "  #   * Create a second gaussian representing the independent-cause component\n",
    "  #   * Combine the two priors, using the mixing weight p_common. Don't forget\n",
    "  #      to normalize the result so it remains a proper probability density function\n",
    "  #\n",
    "  #   * Comment the line below to test out your function\n",
    "  raise NotImplementedError(\"Please complete Exercise 1\")\n",
    "  ###############################################################################\n",
    "\n",
    "  gaussian_common = my_gaussian(x, mean, sigma_common)\n",
    "  gaussian_independent = ...\n",
    "  mixture = ...\n",
    "\n",
    "  return gaussian_common, gaussian_independent, mixture\n",
    "\n",
    "\n",
    "x = np.arange(-10, 11, 0.1)\n",
    "\n",
    "# Uncomment the lines below to visualize out your solution\n",
    "# common, independent, mixture = mixture_prior(x)\n",
    "# plot_mixture_prior(x, common, independent, mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "text",
    "outputId": "371c906d-ddd0-42f4-bd7d-6170c8818f94"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial2_Solution_2c2af4a2.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial2_Solution_2c2af4a2_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "# Section 3: Bayes Theorem with Complex Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "79136cf7-30de-475f-83ba-29887aef24fb"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Mixture-of-Gaussians and Bayes' Theorem\n",
    "video = YouTubeVideo(id='LWKM35te0WI', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we have created a mixture of Gaussians prior that embodies the participants' expectations about sound location, we want to compute the posterior probability, which represents the subjects' beliefs about a specific sound's origin. \n",
    "\n",
    "To do so we will compute the posterior by using *Bayes Theorem* to combine the mixture-of-gaussians prior and varying auditory Gaussian likelihood. This works exactly the same as in Tutorial 1: we simply multiply the prior and likelihood pointwise, then normalize the resulting distribution so it sums to 1. (The closed-form solution from Exercise 2B, however, no longer applies to this more complicated prior). \n",
    "\n",
    "Here, we provide you with the code mentioned in the video (lucky!). Instead, use the interactive demo to explore how a mixture-of-Gaussians prior and Gaussian likelihood interact. For simplicity, we have fixed the prior mean to be zero. We also recommend starting with same other prior parameters used in Exercise 1: $\\sigma_{common} = 0.5, \\sigma_{independent} = 3, p_{common}=0.75$; vary the likelihood instead. \n",
    "\n",
    "Unlike the demo in Tutorial 1, you should see several qualitatively different effects on the posterior, depending on the relative position and width of likelihood. Pay special attention to both the overall shape of the posterior and the location of the peak. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Interactive Demo 1: Mixture-of-Gaussian prior and the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456,
     "referenced_widgets": [
      "2ae86f7825fa42089f00b630d0408324",
      "6f60bf5e489b432fb238f743a1092427",
      "3e2a788493a34bb3b5343a621de06716",
      "b1d6aa7aa27143b7bed7d2131f93315c",
      "ce3f0d132c0348cb9a4cdbe1a1a55617",
      "33aceea223704d10a3f953895c208fef",
      "818c70bbc9f04aac8e319e0b74d7a402",
      "8e60f63452ac4122aa23a52ed6c7a31e",
      "54528ae871ba42f0aafb59a6d4a743cd",
      "e7aab8621eaf473fb04c9048ffc70158",
      "cc50a31c3bb644b48d2a68140b7a92ff",
      "6b62f20ee2204461b4fc2e6058ee8100",
      "abac0add4423425c8892ef4f985c7e06",
      "c7628a32462b47f8bd8a3d7c4f2b5cfe",
      "b74e7dd69dfc4dd882d38e020b51898f",
      "8e96eb6720c9422f9ba320457afe1df0",
      "39b28819bf8a4c0bb051c91b718200e5",
      "9dedcc20a79845779a6447ceb77051e9",
      "e22f2c8ff4ec4e37a6a856d5ccbed3c9"
     ]
    },
    "colab_type": "code",
    "outputId": "12577f11-30f0-484f-c043-d409104ab8d5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "fig_domain = np.arange(-10, 11, 0.1)\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def refresh(sigma_common=0.5, sigma_independent=3, p_common=0.75, mu_auditory=3, sigma_auditory=1.5):\n",
    "    _, _, prior = mixture_prior(fig_domain, 0, sigma_common, sigma_independent, p_common)\n",
    "    likelihood = my_gaussian(fig_domain, mu_auditory, sigma_auditory)\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    posterior /= posterior.sum()\n",
    "\n",
    "    plt.plot(fig_domain, prior, label=\"Mixture Prior\")\n",
    "    plt.plot(fig_domain, likelihood, label=\"Likelihood\")\n",
    "    plt.plot(fig_domain, posterior, label=\"Posterior\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "_ = widgets.interact(refresh,\n",
    "    sigma_common=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_common\", style=style),\n",
    "    sigma_independent=widgets.FloatSlider(value=3, min=0.01, max=10, step=0.5, description=\"sigma_independent:\", style=style),\n",
    "    p_common=widgets.FloatSlider(value=0.75, min=0, max=1, steps=0.01, description=\"p_common\"),\n",
    "    mu_auditory=widgets.FloatSlider(value=2, min=-10, max=10, step=0.1, description=\"mu_auditory:\", style=style),\n",
    "    sigma_auditory=widgets.FloatSlider(value=0.5, min=0.01, max=10, step=0.5, description=\"sigma_auditory:\", style=style),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "text",
    "outputId": "17a71ae3-6fd7-47ba-ce59-34e72ad104b2"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial2_Solution_4d414185.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "outputId": "f4d716db-7d2b-44d8-ee9e-1191a3b4b5e6"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Outro\n",
    "video = YouTubeVideo(id='UgeAtE8xZT8', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this tutorial, we introduced the ventriloquism setting that will form the basis of Tutorials 3 and 4 as well. We built a mixture-of-Gaussians prior that captures the participants' subjective experiences. In the next tutorials, we will use these to perform causal inference and predict the subject's responses to indvidual stimuli. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
