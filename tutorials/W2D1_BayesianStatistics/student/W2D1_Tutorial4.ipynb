{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/W2D1-postcourse-bugfix/tutorials/W2D1_BayesianStatistics/student/W2D1_Tutorial4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# NMA 2020 W2D1 -- (Bonus) Tutorial 4: Bayesian Decision Theory & Cost functions\n",
    "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matthew Krause\n",
    "\n",
    "__Content reviewers:__ Matthew Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "*This tutorial is optional! Please do not feel pressured to finish it!*\n",
    "\n",
    "In the previous tutorials, we investigated the posterior, which describes  beliefs based on a combination of current evidence and prior experience. This tutorial focuses on Bayesian Decision Theory, which combines the posterior with **cost functions** that allow us to quantify the potential impact of making a decision or choosing an action based on that posterior. Cost functions are therefore critical for turning probabilities into actions!\n",
    "\n",
    "In Tutorial 3, we used the mean of the posterior $p(x | \\tilde x)$ as a proxy for the response $\\hat x$ for the participants. What prompted us to use the mean of the posterior as a **decision rule**? In this tutorial we will see how different common decision rules such as the choosing the mean, median or mode of the posterior distribution correspond to minimizing different cost functions.\n",
    "\n",
    "In this tutorial, you will\n",
    "  1. Implement three commonly-used cost functions: mean-squared error, absolute error, and zero-one loss\n",
    "  2. Discover the concept of expected loss, and\n",
    "  3. Choose optimal locations on the posterior that minimize these cost functions. You will verify that it these locations can be found analytically as well as empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "824205d4-e193-45ab-d748-bbd5c015f970"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Introduction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='z2DF4H_sa-k', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "Please execute the cell below to initialize the notebook environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "--- \n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "  \"\"\"Returns un-normalized Gaussian estimated at points `x_points`\n",
    "\n",
    "  DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "  Args :\n",
    "    x_points (numpy array of floats) - points at which the gaussian is evaluated\n",
    "    mu (scalar) - mean of the Gaussian\n",
    "    sigma (scalar) - std of the gaussian\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats): un-normalized Gaussian (i.e. without constant) evaluated at `x`\n",
    "  \"\"\"\n",
    "  return np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
    "\n",
    "def visualize_loss_functions(mse=None, abse=None, zero_one=None):\n",
    "  \"\"\"Visualize loss functions\n",
    "    Args:\n",
    "      - mse (func) that returns mean-squared error\n",
    "      - abse: (func) that returns absolute_error\n",
    "      - zero_one: (func) that returns zero-one loss\n",
    "    All functions should be of the form f(x, x_hats). See Exercise #1.\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "  x = np.arange(-3, 3.25, 0.25)\n",
    "\n",
    "  fig, ax = plt.subplots(1)\n",
    "\n",
    "  if mse is not None:\n",
    "    ax.plot(x, mse(0, x), linewidth=2, label=\"Mean Squared Error\")\n",
    "  if abse is not None:\n",
    "    ax.plot(x, abse(0, x), linewidth=2, label=\"Absolute Error\")\n",
    "  if zero_one_loss is not None:\n",
    "    ax.plot(x, zero_one_loss(0, x), linewidth=2, label=\"Zero-One Loss\")\n",
    "\n",
    "  ax.set_ylabel('Cost')\n",
    "  ax.set_xlabel('Predicted Value ($\\hat{x}$)')\n",
    "  ax.set_title(\"Loss when the true value $x$=0\")\n",
    "  ax.legend()\n",
    "  plt.show()\n",
    "\n",
    "def moments_myfunc(x_points, function):\n",
    "    \"\"\"Returns the mean, median and mode of an arbitrary function\n",
    "\n",
    "    DO NOT EDIT THIS FUNCTION !!!\n",
    "\n",
    "    Args :\n",
    "      x_points (numpy array of floats) - x-axis values\n",
    "      function (numpy array of floats) - y-axis values of the function evaluated at `x_points`\n",
    "\n",
    "    Returns:\n",
    "       (tuple of 3 scalars): mean, median, mode\n",
    "    \"\"\"\n",
    "\n",
    "    # Calc mode of an arbitrary function\n",
    "    mode = x_points[np.argmax(function)]\n",
    "\n",
    "    # Calc mean of an arbitrary function\n",
    "    mean = np.sum(x_points * function)\n",
    "\n",
    "    # Calc median of an arbitrary function\n",
    "    cdf_function = np.zeros_like(x_points)\n",
    "    accumulator = 0\n",
    "    for i in np.arange(x.shape[0]):\n",
    "        accumulator = accumulator + posterior[i]\n",
    "        cdf_function[i] = accumulator\n",
    "    idx = np.argmin(np.abs(cdf_function - 0.5))\n",
    "    median = x_points[idx]\n",
    "\n",
    "    return mean, median, mode\n",
    "\n",
    "def loss_plot(x, loss, min_loss, loss_label, show=False, ax=None):\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(x, loss, '-C1', linewidth=2, label=loss_label)\n",
    "  ax.axvline(min_loss, ls='dashed', color='C1', label='Minimum')\n",
    "  ax.set_ylabel('Expected Loss')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  ax.legend()\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "def loss_plot_subfigures(x,\n",
    "              MSEloss, min_MSEloss, loss_MSElabel,\n",
    "              ABSEloss, min_ABSEloss, loss_ABSElabel,\n",
    "              ZeroOneloss, min_01loss, loss_01label):\n",
    "\n",
    "  fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "  fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(fig_w*2, fig_h*2), sharex=True)\n",
    "\n",
    "  ax[0, 0].plot(x, MSEloss, '-C1', linewidth=2, label=loss_MSElabel)\n",
    "  ax[0, 0].axvline(min_MSEloss, ls='dashed', color='C1', label='Minimum')\n",
    "  ax[0, 0].set_ylabel('Expected Loss')\n",
    "  ax[0, 0].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 0].set_title(\"Mean Squared Error\")\n",
    "  ax[0, 0].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,0])\n",
    "\n",
    "  ax[0, 1].plot(x, ABSEloss, '-C0', linewidth=2, label=loss_ABSElabel)\n",
    "  ax[0, 1].axvline(min_ABSEloss, ls='dashdot', color='C0', label='Minimum')\n",
    "  ax[0, 1].set_ylabel('Expected Loss')\n",
    "  ax[0, 1].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 1].set_title(\"Absolute Error\")\n",
    "  ax[0, 1].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,1])\n",
    "\n",
    "\n",
    "  ax[0, 2].plot(x, ZeroOneloss, '-C2', linewidth=2, label=loss_01label)\n",
    "  ax[0, 2].axvline(min_01loss, ls='dotted', color='C2', label='Minimum')\n",
    "  ax[0, 2].set_ylabel('Expected Loss')\n",
    "  ax[0, 2].set_xlabel('Orientation (Degrees)')\n",
    "  ax[0, 2].set_title(\"0-1 Loss\")\n",
    "  ax[0, 2].legend()\n",
    "\n",
    "  pmoments_plot(x, posterior, ax=ax[1,2])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def pmoments_plot(x, posterior,\n",
    "                  prior=None, likelihood=None, show=False, ax=None):\n",
    "\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "  if prior:\n",
    "    ax.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
    "  if likelihood:\n",
    "    ax.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
    "  ax.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
    "\n",
    "  mean, median, mode = moments_myfunc(x, posterior)\n",
    "\n",
    "  ax.axvline(mean, ls='dashed', color='C1', label='Mean')\n",
    "  ax.axvline(median, ls='dashdot', color='C0', label='Median')\n",
    "  ax.axvline(mode, ls='dotted', color='C2', label='Mode')\n",
    "  ax.set_ylabel('Probability')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  ax.legend()\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def generate_example_pdfs():\n",
    "  \"\"\"Generate example probability distributions as in T2\"\"\"\n",
    "  x=np.arange(-5, 5, 0.01)\n",
    "\n",
    "  prior_mean = 0\n",
    "  prior_sigma1 = .5\n",
    "  prior_sigma2 = 3\n",
    "  prior1 = my_gaussian(x, prior_mean, prior_sigma1)\n",
    "  prior2 = my_gaussian(x, prior_mean, prior_sigma2)\n",
    "\n",
    "  alpha = 0.05\n",
    "  prior_combined = (1-alpha) * prior1 + (alpha * prior2)\n",
    "  prior_combined = prior_combined / np.sum(prior_combined)\n",
    "\n",
    "  likelihood_mean = -2.7\n",
    "  likelihood_sigma = 1\n",
    "  likelihood = my_gaussian(x, likelihood_mean, likelihood_sigma)\n",
    "  likelihood = likelihood / np.sum(likelihood)\n",
    "\n",
    "  posterior = prior_combined * likelihood\n",
    "  posterior = posterior / np.sum(posterior)\n",
    "\n",
    "  return x, prior_combined, likelihood, posterior\n",
    "\n",
    "def plot_posterior_components(x, prior, likelihood, posterior):\n",
    "  with plt.xkcd():\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, prior, '-C1', linewidth=2, label='Prior')\n",
    "    plt.plot(x, likelihood, '-C0', linewidth=2, label='Likelihood')\n",
    "    plt.plot(x, posterior, '-C2', linewidth=4, label='Posterior')\n",
    "    plt.legend()\n",
    "    plt.title('Sample Output')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Posterior Distribution\n",
    "\n",
    "This notebook will use a model similar to the puppet & puppeteer sound experiment developed in Tutorial 2, but with different probabilities for $p_{common}$, $p_{independent}$, $\\sigma_{common}$ and $\\sigma_{independent}$. Specifically, our model will consist of these components, combined according to Bayes' rule:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\textrm{Prior} &=& \\begin{cases} \\mathcal{N_{common}}(0, 0.5) & 95\\% \\textrm{ weight}\\\\\n",
    "                                 \\mathcal{N_{independent}}(0, 3.0) &  5\\% \\textrm{ weight} \\\\\n",
    "                    \\end{cases}\\\\\\\\\n",
    "\\textrm{Likelihood} &= &\\mathcal{N}(-2.7, 1.0)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We will use this posterior as an an example through this notebook. Please run the cell below to import and plot the model. You do not need to edit anything. These parameter values were deliberately chosen for illustration purposes: there is nothing intrinsically special about them, but they make several of the exercises easier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "outputId": "ef01110e-546a-487c-e1db-b13cd08292f3"
   },
   "outputs": [],
   "source": [
    "x, prior, likelihood, posterior = generate_example_pdfs()\n",
    "plot_posterior_components(x, prior, likelihood, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 1: The Cost Functions\n",
    "\n",
    "Next, we will implement the cost functions. \n",
    "A cost function determines the \"cost\" (or penalty) of estimating $\\hat{x}$ when the true or correct quantity is really $x$ (this is essentially the cost of the error between the true stimulus value: $x$ and our estimate: $\\hat x$ -- Note that the error can be defined in different ways):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\textrm{Mean Squared Error} &=& (x - \\hat{x})^2 \\\\ \n",
    "\\textrm{Absolute Error} &=& \\big|x - \\hat{x}\\big| \\\\ \n",
    "\\textrm{Zero-One Loss} &=& \\begin{cases}\n",
    "                            0,& \\text{if } x = \\hat{x} \\\\\n",
    "                            1,              & \\text{otherwise}\n",
    "                            \\end{cases}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "In the cell below, fill in the body of these cost function. Each function should take one single value for $x$ (the true stimulus value : $x$) and one or more possible value estimates: $\\hat{x}$. \n",
    "\n",
    "Return an array containing the costs associated with predicting $\\hat{x}$ when the true value is $x$. Once you have written all three functions, uncomment the final line to visulize your results.\n",
    "\n",
    " _Hint:_ These functions are easy to write (1 line each!) but be sure *all* three functions return arrays of `np.float` rather than another data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 1: Implement the cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def mse(x, x_hats):\n",
    "  \"\"\"Mean-squared error cost function\n",
    "    Args:\n",
    "      x (scalar): One true value of $x$\n",
    "      x_hats (scalar or ndarray): Estimate of x\n",
    "    Returns:\n",
    "      same shape/type as x_hats): MSE costs associated with\n",
    "      predicting x_hats instead of x$\n",
    "  \"\"\"\n",
    "\n",
    "  ##############################################################################\n",
    "  # Complete the MSE cost function\n",
    "  #\n",
    "  ### Comment out the line below to test your function\n",
    "  raise NotImplementedError(\"You need to complete the MSE cost function!\")\n",
    "  ##############################################################################\n",
    "\n",
    "  my_mse = ...\n",
    "  return my_mse\n",
    "\n",
    "\n",
    "def abs_err(x, x_hats):\n",
    "  \"\"\"Absolute error cost function\n",
    "    Args:\n",
    "      x (scalar): One true value of $x$\n",
    "      x_hats (scalar or ndarray): Estimate of x\n",
    "    Returns:\n",
    "      (same shape/type as x_hats): absolute error costs associated with\n",
    "      predicting x_hats instead of x$\n",
    "  \"\"\"\n",
    "\n",
    "  ##############################################################################\n",
    "  # Complete the absolute error cost function\n",
    "  #\n",
    "  ### Comment out the line below to test your function\n",
    "  raise NotImplementedError(\"You need to complete the absolute error function!\")\n",
    "  ##############################################################################\n",
    "\n",
    "  my_abs_err = ...\n",
    "  return my_abs_err\n",
    "\n",
    "\n",
    "def zero_one_loss(x, x_hats):\n",
    "  \"\"\"Zero-One loss cost function\n",
    "    Args:\n",
    "      x (scalar): One true value of $x$\n",
    "      x_hats (scalar or ndarray): Estimate of x\n",
    "    Returns:\n",
    "      (same shape/type as x_hats) of the 0-1 Loss costs associated with predicting x_hat instead of x\n",
    "  \"\"\"\n",
    "\n",
    "  ##############################################################################\n",
    "  # Complete the zero-one loss cost function\n",
    "  #\n",
    "  ### Comment out the line below to test your function\n",
    "  raise NotImplementedError(\"You need to complete the 0-1 loss cost function!\")\n",
    "  ##############################################################################\n",
    "\n",
    "  my_zero_one_loss = ...\n",
    "  return my_zero_one_loss\n",
    "\n",
    "\n",
    "## When you are done with the functions above, uncomment the line below to\n",
    "## visualize them\n",
    "# visualize_loss_functions(mse, abs_err, zero_one_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "text",
    "outputId": "e5e05e54-c2ce-4c81-84d3-c7a866c15209"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_8da2f3c2.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=416 height=272 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_8da2f3c2_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 2: Expected Loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "a8523a74-2cfa-433f-e54e-3352ce482d51"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Expected Loss\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='FTBpCfylV_Y', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "A posterior distribution tells us about the confidence or credibility we assign to different choices. A cost function describes the penalty we incur when choosing an incorrect option. These concepts can be combined into an *expected loss* function. Expected loss is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbb{E}[\\text{Loss} | \\hat{x}] = \\int L[\\hat{x},x] \\odot  p(x|\\tilde{x}) dx\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $L[ \\hat{x}, x]$ is the loss function, $p(x|\\tilde{x})$ is the posterior, and $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication), and $\\mathbb{E}[\\text{Loss} | \\hat{x}]$ is the expected loss. \n",
    "\n",
    "In this exercise, we will calculate the expected loss for the: means-squared error, the absolute error, and the zero-one loss over our bimodal posterior $p(x | \\tilde x)$. \n",
    "\n",
    "**Suggestions:**\n",
    "* We already pre-completed the code (commented-out) to calculate the mean-squared error, absolute error, and zero-one loss between $x$ and an estimate $\\hat x$ using the functions you created in exercise 1\n",
    "* Calculate the expected loss ($\\mathbb{E}[MSE Loss]$) using your posterior (imported above as `posterior`) & each of the loss functions described above (MSELoss, ABSELoss, and Zero-oneLoss).\n",
    "* Find the x position that minimizes the expected loss for each cost function and plot them using the `loss_plot` function provided (commented-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 2: Finding the expected loss empirically via integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def expected_loss_calculation(x, posterior):\n",
    "\n",
    "  ExpectedLoss_MSE = np.zeros_like(x)\n",
    "  ExpectedLoss_ABSE = np.zeros_like(x)\n",
    "  ExpectedLoss_01 = np.zeros_like(x)\n",
    "\n",
    "  for idx in np.arange(x.shape[0]):\n",
    "    estimate = x[idx]\n",
    "\n",
    "    ###################################################################\n",
    "    ## Insert code below to find the expected loss under each loss function\n",
    "    ##\n",
    "    ## remove the raise when the function is complete\n",
    "    raise NotImplementedError(\"Calculate the expected loss over all x values!\")\n",
    "    ###################################################################\n",
    "\n",
    "    MSELoss = mse(estimate, x)\n",
    "    ExpectedLoss_MSE[idx] = ...\n",
    "\n",
    "    ABSELoss = abs_err(estimate, x)\n",
    "    ExpectedLoss_ABSE[idx] = ...\n",
    "\n",
    "    ZeroOneLoss = zero_one_loss(estimate, x)\n",
    "    ExpectedLoss_01[idx] = ...\n",
    "\n",
    "  ###################################################################\n",
    "  ## Now, find the `x` location that minimizes expected loss\n",
    "  ##\n",
    "  ## remove the raise when the function is complete\n",
    "  raise NotImplementedError(\"Finish the Expected Loss calculation\")\n",
    "  ###################################################################\n",
    "\n",
    "  min_MSE = ...\n",
    "  min_ABSE = ...\n",
    "  min_01 = ...\n",
    "\n",
    "  return (ExpectedLoss_MSE, ExpectedLoss_ABSE, ExpectedLoss_01,\n",
    "          min_MSE, min_ABSE, min_01)\n",
    "\n",
    "## Uncomment the lines below to plot the expected loss as a function of the estimates\n",
    "#ExpectedLoss_MSE, ExpectedLoss_ABSE, ExpectedLoss_01,  min_MSE, min_ABSE, min_01 = expected_loss_calculation(x, posterior)\n",
    "#loss_plot(x, ExpectedLoss_MSE, min_MSE, f\"Mean Squared Error = {min_MSE:.2f}\")\n",
    "#loss_plot(x, ExpectedLoss_ABSE, min_ABSE, f\"Absolute Error = {min_ABSE:.2f}\")\n",
    "#loss_plot(x, ExpectedLoss_01, min_01, f\"Zero-One Error = {min_01:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "colab_type": "text",
    "outputId": "cfca1ee7-2e20-42a6-cce3-1f2861f1016e"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_3a9250ef.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_0.png>\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_1.png>\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial4_Solution_3a9250ef_2.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Analytical Solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "a2058e1a-71c5-45a7-833d-6074b9512727"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Analytical Solutions\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='wmDD51N9rs0', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the previous exercise, we found the minimum expected loss via brute-force: we searched over all possible values of $x$ and found the one that minimized each of our loss functions. This is feasible for our small toy example, but can quickly become intractable. \n",
    "\n",
    "Fortunately, the three loss functions examined in this tutorial have are minimized at specific points on the posterior, corresponding to the itss mean, median, and mode. To verify this property, we have replotted the loss functions from Exercise 2 below, with the posterior on the same scale beneath. The mean, median, and mode are marked on the posterior. \n",
    "\n",
    "Which loss form corresponds to each summary statistics? \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "outputId": "5c045166-97d9-4a3b-cc3a-963bb8d560d6"
   },
   "outputs": [],
   "source": [
    "loss_plot_subfigures(x,\n",
    "                    ExpectedLoss_MSE, min_MSE, f\"Mean Squared Error = {min_MSE:.2f}\",\n",
    "                    ExpectedLoss_ABSE, min_ABSE, f\"Absolute Error = {min_ABSE:.2f}\",\n",
    "                    ExpectedLoss_01, min_01, f\"Zero-One Error = {min_01:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "text",
    "outputId": "704d3a6a-ea3a-4b1a-f83c-579585e9a4bf"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial4_Solution_8cdbd46a.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 4: Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "e5748d72-880b-4891-bc52-476eb10a6c42"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Outro\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='3nTvamDVx2s', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In this tutorial, we learned about three kinds of cost functions: mean-squared error, absolute error, and zero-one loss. We used expected loss to quantify the results of making a decision, and showed that optimizing under different cost functions led us to choose different locations on the posterior. Finally, we found that these optimal locations can be identified analytically, sparing us from a brute-force search. \n",
    "\n",
    "Here are some additional questions to ponder:\n",
    "*   Suppose your professor offered to grade your work with a zero-one loss or mean square error. \n",
    "    * When might you choose each?\n",
    "    * Which would be easier to learn from?\n",
    "* All of the loss functions we considered are symmetrical. Are there situations where an asymmetrical loss function might make sense? How about a negative one?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial4",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
