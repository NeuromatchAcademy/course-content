{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/W2D1-postcourse-bugfix/tutorials/W2D1_BayesianStatistics/student/W2D1_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 2, Day 1, Tutorial 1\n",
    "# Bayes rule with Gaussians\n",
    "\n",
    "__Content creators:__ Vincent Valton, Konrad Kording, with help from Matt Krause\n",
    "\n",
    "__Content reviewers:__ Matt Krause, Jesse Livezey, Karolina Stosio, Saeed Salehi, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Tutorial Objectives\n",
    "This is the first in a series of three main tutorials (+ one bonus tutorial) on Bayesian statistics. In these tutorials, we will develop a Bayesian model for localizing sounds based on audio and visual cues. This model will combine **prior** information about where sounds generally originate with sensory information about the **likelihood** that a specific sound came from a particular location. As we will see in subsequent lessons, the resulting **posterior distribution** not only allows us to make optimal decision about the sound's origin, but also lets us quantify how uncertain that decision is. Bayesian techniques are therefore useful **normative models**: the behavior of human or animal subjects can be compared against these models to determine how efficiently they make use of information. \n",
    "\n",
    "This notebook will introduce two fundamental building blocks for Bayesian statistics: the Gaussian distribution and the Bayes Theorem. You will: \n",
    "\n",
    "1. Implement a Gaussian distribution\n",
    "2. Use Bayes' Theorem to find the posterior from a Gaussian-distributed prior and likelihood. \n",
    "3. Change the likelihood mean and variance and observe how posterior changes.\n",
    "4. Advanced (*optional*): Observe what happens if the prior is a mixture of two gaussians?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "bccac551-e778-4f58-ea61-cdd93ffd4cde"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Introduction to Bayesian Statistics\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='K4sSKZtk-Sc', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup  \n",
    "Please execute the cells below to initialize the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "def my_plot_single(x, px):\n",
    "    \"\"\"\n",
    "    Plots normalized Gaussian distribution\n",
    "\n",
    "    Args:\n",
    "        x (numpy array of floats):     points at which the likelihood has been evaluated\n",
    "        px (numpy array of floats):    normalized probabilities for prior evaluated at each `x`\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "    if px is None:\n",
    "        px = np.zeros_like(x)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, px, '-', color='C2', LineWidth=2, label='Prior')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xlabel('Orientation (Degrees)')\n",
    "\n",
    "\n",
    "def posterior_plot(x, likelihood=None, prior=None, posterior_pointwise=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots normalized Gaussian distributions and posterior\n",
    "\n",
    "    Args:\n",
    "        x (numpy array of floats):         points at which the likelihood has been evaluated\n",
    "        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`\n",
    "        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`\n",
    "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "        ax: Axis in which to plot. If None, create new axis.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "    if likelihood is None:\n",
    "        likelihood = np.zeros_like(x)\n",
    "\n",
    "    if prior is None:\n",
    "        prior = np.zeros_like(x)\n",
    "\n",
    "    if posterior_pointwise is None:\n",
    "        posterior_pointwise = np.zeros_like(x)\n",
    "\n",
    "    if ax is None:\n",
    "      fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(x, likelihood, '-C1', LineWidth=2, label='Auditory')\n",
    "    ax.plot(x, prior, '-C0', LineWidth=2, label='Visual')\n",
    "    ax.plot(x, posterior_pointwise, '-C2', LineWidth=2, label='Posterior')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xlabel('Orientation (Degrees)')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_visual(mu_visuals, mu_posteriors, max_posteriors):\n",
    "    \"\"\"\n",
    "    Plots the comparison of computing the mean of the posterior analytically and\n",
    "    the max of the posterior empirically via multiplication.\n",
    "\n",
    "    Args:\n",
    "        mu_visuals (numpy array of floats): means of the visual likelihood\n",
    "        mu_posteriors (numpy array of floats):  means of the posterior, calculated analytically\n",
    "        max_posteriors (numpy array of floats): max of the posteriors, calculated via maxing the max_posteriors.\n",
    "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "    fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(fig_w, 2 * fig_h))\n",
    "\n",
    "    ax[0].plot(mu_visuals, max_posteriors, '-C2', label='mean')\n",
    "    ax[0].set_xlabel('Visual stimulus position')\n",
    "    ax[0].set_ylabel('Multiplied posterior mean')\n",
    "    ax[0].set_title('Sample output')\n",
    "\n",
    "    ax[1].plot(mu_visuals, mu_posteriors, '--', color='xkcd:gray', label='argmax')\n",
    "    ax[1].set_xlabel('Visual stimulus position')\n",
    "    ax[1].set_ylabel('Analytical posterior mean')\n",
    "    fig.tight_layout()\n",
    "    ax[1].set_title('Hurray for math!')\n",
    "\n",
    "\n",
    "def multimodal_plot(x, example_prior, example_likelihood,\n",
    "                    mu_visuals, posterior_modes):\n",
    "  \"\"\"Helper function for plotting Section 4 results\"\"\"\n",
    "\n",
    "  fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "  fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(fig_w, 2*fig_h), sharex=True)\n",
    "\n",
    "  # Plot the last instance that we tried.\n",
    "  posterior_plot(x,\n",
    "    example_prior,\n",
    "    example_likelihood,\n",
    "    compute_posterior_pointwise(example_prior, example_likelihood),\n",
    "    ax=ax[0]\n",
    "    )\n",
    "  ax[0].set_title('Example combination')\n",
    "\n",
    "  ax[1].plot(mu_visuals, posterior_modes, '-C2', label='argmax')\n",
    "  ax[1].set_xlabel('Visual stimulus position\\n(Mean of blue dist. above)')\n",
    "  ax[1].set_ylabel('Posterior mode\\n(Peak of green dist. above)')\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 1: The Gaussian Distribution\n",
    "\n",
    "Bayesian analysis operates on probability distributions. Although these can take many forms, the Gaussian distribution is a very common choice. Because of the central limit theorem, many quantities are Gaussian-distributed. Gaussians also have some mathematical properties that permit simple closed-form solutions to several important problems. \n",
    "\n",
    "In this exercise, you will implement a Gaussian by filling in the missing portion of `my_gaussian` below. Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center. Its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$ or its square, the **variance** $\\sigma^2$. (Be careful not to use one when the other is required). \n",
    "\n",
    "The equation for a Gaussian is:\n",
    "$$\n",
    "\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "Also, don't forget that this is a probability distribution and should therefore sum to one. While this happens \"automatically\" when integrated from $-\\infty$ to $\\infty$, your version will only be computed over a finite number of points. You therefore need to explicitly normalize it yourself. \n",
    "\n",
    "Test out your implementation with a $\\mu = -1$ and $\\sigma = 1$. After you have it working, play with the  parameters to develop an intuition for how changing $\\mu$ and $\\sigma$ alter the shape of the Gaussian. This is important, because subsequent exercises will be built out of Gaussians. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 1: Implement a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def my_gaussian(x_points, mu, sigma):\n",
    "    \"\"\"\n",
    "    Returns normalized Gaussian estimated at points `x_points`, with parameters:\n",
    "     mean `mu` and std `sigma`\n",
    "\n",
    "    Args:\n",
    "        x_points (numpy array of floats): points at which the gaussian is\n",
    "                                          evaluated\n",
    "        mu (scalar): mean of the Gaussian\n",
    "        sigma (scalar): std of the gaussian\n",
    "\n",
    "    Returns:\n",
    "        (numpy array of floats) : normalized Gaussian evaluated at `x`\n",
    "    \"\"\"\n",
    "\n",
    "    ###################################################################\n",
    "    ## Add code to calcualte the gaussian px as a function of mu and sigma,\n",
    "    ## for every x in x_points\n",
    "    ## Function Hints: exp -> np.exp()\n",
    "    ##                 power -> z**2\n",
    "    ## remove the raise below to test your function\n",
    "    raise NotImplementedError(\"You need to implement the Gaussian function!\")\n",
    "    ###################################################################\n",
    "    px = ...\n",
    "\n",
    "    return px\n",
    "\n",
    "\n",
    "x = np.arange(-8, 9, 0.1)\n",
    "\n",
    "# Uncomment to plot the results\n",
    "# px = my_gaussian(x, -1, 1)\n",
    "# my_plot_single(x, px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "text",
    "outputId": "600f468b-7f03-45c2-a67f-c5af67121ba3"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial1_Solution_cce0848d.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial1_Solution_cce0848d_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 2. Bayes' Theorem and the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "f27d7154-1f82-4dd7-d21d-1d1ae4c9ae8c"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Bayes' theorem\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id='ewQPHQMcdBs', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "Bayes' rule tells us how to combine two sources of information: the prior (e.g., a noisy representation of our expectations about where the stimulus might come from) and the likelihood (e.g., a noisy representation of the stimulus position on a given trial), to obtain a posterior distribution taking into account both pieces of information. Bayes' rule states:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\text{Posterior} = \\frac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "When both the prior and likelihood are Gaussians, this translates into the following form:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{Likelihood} &=& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\\\\n",
    "\\text{Prior} &=& \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
    "\\text{Posterior} &\\propto& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\times \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
    "&&= \\mathcal{N}\\left( \\frac{\\sigma^2_{likelihood}\\mu_{prior}+\\sigma^2_{prior}\\mu_{likelihood}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}}, \\frac{\\sigma^2_{likelihood}\\sigma^2_{prior}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}} \\right) \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In these equations, $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$:\n",
    "$$\n",
    "\\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\; \\exp \\bigg( \\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
    "$$\n",
    "\n",
    "In Exercise 2A, we will use the first form of the posterior, where the two distributions are combined via pointwise multiplication.  Although this method requires more computation, it works for any type of probability distribution. In Exercise 2B, we will see that the closed-form solution shown on the line below produces the same result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 2A: Finding the posterior computationally\n",
    "\n",
    "Imagine an experiment where participants estimate the location of a noise-emitting object. To estimate its position, the participants can use two sources of information: \n",
    "  1. new noisy auditory information (the likelihood)\n",
    "  2. prior visual expectations of where the stimulus is likely to come from (visual prior). \n",
    "\n",
    "The auditory and visual information are both noisy, so participants will combine these sources of information to better estimate the position of the object.\n",
    "\n",
    "We will use Gaussian distributions to represent the auditory likelihood (in red), and a Gaussian visual prior (expectations - in blue). Using Bayes rule, you will combine them into a posterior distribution that summarizes the probability that the object is in each location. \n",
    "\n",
    "We have provided you with a ready-to-use plotting function, and a code skeleton.\n",
    "\n",
    "* Use `my_gaussian`, the answer to exercise 1, to generate an auditory likelihood with parameters $\\mu$ = 3 and $\\sigma$ = 1.5\n",
    "* Generate a visual prior with parameters $\\mu$ = -1 and $\\sigma$ = 1.5\n",
    "* Calculate the posterior using pointwise multiplication of the likelihood and prior. Don't forget to normalize so the posterior adds up to 1. \n",
    "* Plot the likelihood, prior and posterior using the predefined function `posterior_plot`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def compute_posterior_pointwise(prior, likelihood):\n",
    "  ##############################################################################\n",
    "  # Write code to compute the posterior from the prior and likelihood via\n",
    "  # pointwise multiplication. (You may assume both are defined over the same x-axis)\n",
    "  #\n",
    "  # Comment out the line below to test your solution\n",
    "  raise NotImplementedError(\"Finish the simulation code first\")\n",
    "  ##############################################################################\n",
    "\n",
    "  posterior = ...\n",
    "\n",
    "  return posterior\n",
    "\n",
    "\n",
    "def localization_simulation(mu_auditory=3.0, sigma_auditory=1.5,\n",
    "                            mu_visual=-1.0, sigma_visual=1.5):\n",
    "\n",
    "  ##############################################################################\n",
    "  ## Using the x variable below,\n",
    "  ##      create a gaussian called 'auditory' with mean 3, and std 1.5\n",
    "  ##      create a gaussian called 'visual' with mean -1, and std 1.5\n",
    "  #\n",
    "  #\n",
    "  ## Comment out the line below to test your solution\n",
    "  raise NotImplementedError(\"Finish the simulation code first\")\n",
    "  ###############################################################################\n",
    "  x = np.arange(-8, 9, 0.1)\n",
    "\n",
    "  auditory = ...\n",
    "  visual = ...\n",
    "  posterior = compute_posterior_pointwise(auditory, visual)\n",
    "\n",
    "  return x, auditory, visual, posterior\n",
    "\n",
    "\n",
    "# Uncomment the lines below to plot the results\n",
    "# x, auditory, visual, posterior_pointwise = localization_simulation()\n",
    "# posterior_plot(x, auditory, visual, posterior_pointwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "text",
    "outputId": "d24f3579-18f5-4ccd-b2bb-7d078ad9c7f8"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial1_Solution_67c48f25.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=280 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial1_Solution_67c48f25_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Interactive Demo: What affects the posterior?\n",
    "\n",
    "Now that we can compute the posterior of two Gaussians with *Bayes rule*, let's vary the parameters of those Gaussians to see how changing the prior and likelihood affect the posterior. \n",
    "\n",
    "**Hit the Play button or Ctrl+Enter in the cell below** and play with the sliders to get an intuition for how the means and standard deviations of prior and likelihood influence the posterior.\n",
    "\n",
    "When does the prior have the strongest influence over the posterior? When is it the weakest?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425,
     "referenced_widgets": [
      "1ff53dec265843afb986bf03530771ae",
      "05a3296faf284e43be80f3f46d171261",
      "19a5041ddd174440a8a6c678be876f0c",
      "fdc140da3de049739a78aca60ebede0e",
      "4b472618a4d7409b92723f2219dfb72d",
      "0b84e97b9e51464d9b0b421aff355edb",
      "ce8d924b5f50488fb86eaa5c875d3d31",
      "3a5f7bd213f442e5b3af1c8e84a4520b",
      "e46c2185e8404991a3912fe9b5382d61",
      "dd4fc70382994a4fa5295b91af329218",
      "7dc8a2b2d6f244b799a7f8cf14a26e58",
      "d4c0bfac0af7471999a5c0eba3094871",
      "c578a633433343718e1a5fc4d1763cd5",
      "5e28d4f613bd4adeb2fc4264323c748d",
      "a5352563108a4e1cbc498b5372e69a38",
      "fc0a0a90e46f44ac82f71ab5b924f84d"
     ]
    },
    "colab_type": "code",
    "outputId": "b8ba4c5f-cbe9-4122-e2e0-5406af4bd802"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "x = np.arange(-10, 11, 0.1)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def refresh(mu_auditory=3, sigma_auditory=1.5, mu_visual=-1, sigma_visual=1.5):\n",
    "    auditory = my_gaussian(x, mu_auditory, sigma_auditory)\n",
    "    visual = my_gaussian(x, mu_visual, sigma_visual)\n",
    "    posterior_pointwise = visual * auditory\n",
    "    posterior_pointwise /= posterior_pointwise.sum()\n",
    "\n",
    "    w_auditory = (sigma_visual** 2) / (sigma_auditory**2 + sigma_visual**2)\n",
    "    theoretical_prediction = mu_auditory * w_auditory + mu_visual * (1 - w_auditory)\n",
    "\n",
    "    ax = posterior_plot(x, auditory, visual, posterior_pointwise)\n",
    "    ax.plot([theoretical_prediction, theoretical_prediction],\n",
    "            [0, posterior_pointwise.max() * 1.2], '-.', color='xkcd:medium gray')\n",
    "    ax.set_title(f\"Gray line shows analytical mean of posterior: {theoretical_prediction:0.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "_ = widgets.interact(refresh,\n",
    "    mu_auditory=widgets.FloatSlider(value=2, min=-10, max=10, step=0.5, description=\"mu_auditory:\", style=style),\n",
    "    sigma_auditory=widgets.FloatSlider(value=0.5, min=0.5, max=10, step=0.5, description=\"sigma_auditory:\", style=style),\n",
    "    mu_visual=widgets.FloatSlider(value=-2, min=-10, max=10, step=0.5, description=\"mu_visual:\", style=style),\n",
    "    sigma_visual=widgets.FloatSlider(value=0.5, min=0.5, max=10, step=0.5, description=\"sigma_visual:\", style=style)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Video 3: Multiplying Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "85d7fbcc-3e73-4fa8-efcf-fd9163f1cad0"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "video = YouTubeVideo(id='AbXorOLBrws', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 2B: Finding the posterior analytically\n",
    "\n",
    "[If you are running short on time, feel free to skip the coding exercise below].\n",
    "\n",
    "As you may have noticed from the interactive demo, the product of two Gaussian distributions, like our prior and likelihood, remains a Gaussian, regardless of the parameters. We can directly compute the  parameters of that Gaussian from the means and variances of the prior and likelihood. For example, the posterior mean is given by:\n",
    "\n",
    "$$ \\mu_{posterior} = \\frac{\\mu_{auditory} \\cdot \\frac{1}{\\sigma_{auditory}^2} + \\mu_{visual} \\cdot \\frac{1}{\\sigma_{visual}^2}}{1/\\sigma_{auditory}^2 + 1/\\sigma_{visual}^2} \n",
    "$$\n",
    "\n",
    "This formula is a special case for two Gaussians, but is a very useful one because:\n",
    "*   The posterior has the same form (here, a normal distribution) as the prior, and\n",
    "*   There is simple, closed-form expression for its parameters.\n",
    "\n",
    "When these properties hold, we call them **conjugate distributions** or **conjugate priors** (for a particular likelihood). Working with conjugate distributions is very convenient; otherwise, it is often necessary to use computationally-intensive numerical methods to combine the prior and likelihood. \n",
    "\n",
    "In this exercise, we ask you to verify that property.  To do so, we will hold our auditory likelihood constant as an $\\mathcal{N}(3, 1.5)$ distribution, while considering visual priors with different means ranging from $\\mu=-10$ to $\\mu=10$. For each prior,\n",
    "\n",
    "* Compute the posterior distribution using the function you wrote in Exercise 2A. Next, find its mean. The mean of a probability distribution is $\\int_x p(x) dx$ or $\\sum_x x\\cdot p(x)$. \n",
    "* Compute the analytical posterior mean from auditory and visual using the equation above.\n",
    "* Use the provided plotting code to plot both estimates of the mean. \n",
    "\n",
    "Are the estimates of the posterior mean the same in both cases? \n",
    "\n",
    "Using these results, try to predict the posterior mean for the combination of a $\\mathcal{N}(-4,4)$ prior and and $\\mathcal{N}(4, 2)$ likelihood. Use the widget above to check your prediction. You can enter values directly by clicking on the numbers to the right of each slider; $\\sqrt{2} \\approx 1.41$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def compare_computational_analytical_means():\n",
    "  x = np.arange(-10, 11, 0.1)\n",
    "\n",
    "  # Fixed auditory likelihood\n",
    "  mu_auditory = 3\n",
    "  sigma_auditory = 1.5\n",
    "  likelihood = my_gaussian(x, mu_auditory, sigma_auditory)\n",
    "\n",
    "  # Varying visual prior\n",
    "  mu_visuals = np.linspace(-10, 10)\n",
    "  sigma_visual = 1.5\n",
    "\n",
    "  # Accumulate results here\n",
    "  mus_by_integration = []\n",
    "  mus_analytical = []\n",
    "\n",
    "  for mu_visual in mu_visuals:\n",
    "    prior = my_gaussian(x, mu_visual, sigma_visual)\n",
    "    posterior = compute_posterior_pointwise(prior, likelihood)\n",
    "\n",
    "    ############################################################################\n",
    "    ## Add code that will find the posterior mean via numerical integration\n",
    "    #\n",
    "    ############################################################################\n",
    "    mu_integrated = ...\n",
    "\n",
    "    ############################################################################\n",
    "    ## Add more code below that will calculate the posterior mean analytically\n",
    "    #\n",
    "    # Comment out the line below to test your solution\n",
    "    raise NotImplementedError(\"Please add code to find the mean both ways first\")\n",
    "    ############################################################################\n",
    "    mu_analytical = ...\n",
    "\n",
    "    mus_by_integration.append(mu_integrated)\n",
    "    mus_analytical.append(mu_analytical)\n",
    "\n",
    "  return mu_visuals, mus_analytical, mus_by_integration\n",
    "\n",
    "\n",
    "# Uncomment the lines below to visualize your results\n",
    "# mu_visuals, mu_analytical, mu_computational = compare_computational_analytical_means()\n",
    "# plot_visual(mu_visuals, mu_analytical, mu_computational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "text",
    "outputId": "04dc281e-c2dc-4664-a097-e3162905e94e"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial1_Solution_642e1b02.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=568 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial1_Solution_642e1b02_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Section 3: Conclusion\n",
    "\n",
    "This tutorial introduced the Gaussian distribution and used Bayes' Theorem to combine Gaussians representing priors and likelihoods. In the next tutorial, we will use these concepts to probe how subjects integrate sensory information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "outputId": "b050d942-8c2d-4d5d-fc1c-b3f478da343e"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Conclusion\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "video = YouTubeVideo(id='YC8GylOAAHs', width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Bonus Section: Multimodal Priors\n",
    "\n",
    "\n",
    "**Only do this if the first half-hour has not yet passed.**\n",
    "\n",
    "The preceeding exercises used a Gaussian prior, implying that participants expected the stimulus to come from a single location, though they might not know precisely where. However, suppose the subjects actually thought that sound might come from one of two distinct locations. Perhaps they can see two speakers (and know that speakers often emit noise). \n",
    "\n",
    "We could model this using a Gaussian prior with a large $\\sigma$ that covers both locations, but that would also make every point in between seem likely too.A better approach is to adjust the form of the prior so that it better matches the participants' experiences/expectations. In this optional exercise, we will build a bimodal (2-peaked) prior out of Gaussians and examine the resulting posterior and its peaks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Exercise 3: Implement and test a multimodal prior\n",
    "\n",
    "* Complete the `bimodal_prior` function below to create a bimodal prior, comprised of the sum of two Gaussians with means $\\mu = -3$ and $\\mu = 3$. Use $\\sigma=1$ for both Gaussians. Be sure to normalize the result so it is a proper probability distribution. \n",
    "\n",
    "* In Exercise 2, we used the mean location to summarize the posterior distribution. This is not always the best choice, especially for multimodal distributions. What is the mean of our new prior? Is it a particularly likely location for the stimulus? Instead, we will use the posterior **mode** to summarize the distribution. The mode is the *location* of the most probable part of the distribution. Complete `posterior_mode` below, to find it. (Hint: `np.argmax` returns the *index* of the largest element in an array).\n",
    "\n",
    "* Run the provided simulation and plotting code. Observe what happens to the posterior as the likelihood gets closer to the different peaks of the prior.\n",
    "* Notice what happens to the posterior when the likelihood is exactly in between the two modes of the prior (i.e., $\\mu_{Likelihood} = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def bimodal_prior(x, mu_1=-3, sigma_1=1, mu_2=3, sigma_2=1):\n",
    "  ################################################################################\n",
    "  ## Finish this function so that it returns a bimodal prior, comprised of the\n",
    "  # sum of two Gaussians\n",
    "  #\n",
    "  # Comment out the line below to test out your solution\n",
    "  raise NotImplementedError(\"Please implement the bimodal prior\")\n",
    "  ################################################################################\n",
    "  prior = ...\n",
    "\n",
    "  return prior\n",
    "\n",
    "\n",
    "def posterior_mode(x, posterior):\n",
    "  ################################################################################\n",
    "  ## Finish this function so that it returns the location of the mode\n",
    "  #\n",
    "  # Comment out the line below to test out your solution\n",
    "  raise NotImplementedError(\"Please implement the posterior mode\")\n",
    "  ################################################################################\n",
    "  mode = ...\n",
    "\n",
    "  return mode\n",
    "\n",
    "\n",
    "def multimodal_simulation(x, mus_visual, sigma_visual=1):\n",
    "  \"\"\"\n",
    "  Simulate an experiment where bimodal prior is held constant while\n",
    "  a Gaussian visual likelihood is shifted across locations.\n",
    "  Args:\n",
    "        x:            array of points at which prior/likelihood/posterior are evaluated\n",
    "        mus_visual:   array of means for the Gaussian likelihood\n",
    "        sigma_visual: scalar standard deviation for the Gaussian likelihood\n",
    "\n",
    "  Returns:\n",
    "    posterior_modes:  array containing the posterior mode for each mean in mus_visual\n",
    "  \"\"\"\n",
    "\n",
    "  prior = bimodal_prior(x, -3, 1, 3, 1)\n",
    "  posterior_modes = []\n",
    "\n",
    "  for mu in mus_visual:\n",
    "    likelihood = my_gaussian(x, mu, 3)\n",
    "    posterior = compute_posterior_pointwise(prior, likelihood)\n",
    "\n",
    "    p_mode = posterior_mode(x, posterior)\n",
    "    posterior_modes.append(p_mode)\n",
    "\n",
    "  return posterior_modes\n",
    "\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "mus = np.arange(-8, 8, 0.05)\n",
    "# Uncomment the lines below to visualize your results\n",
    "# posterior_modes = multimodal_simulation(x, mus, 1)\n",
    "# multimodal_plot(x,\n",
    "#                 bimodal_prior(x, -3, 1, 3, 1),\n",
    "#                 my_gaussian(x, 1, 1),\n",
    "#                 mus, posterior_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "colab_type": "text",
    "outputId": "1652973d-2186-4666-8a9d-f4ac766eaa7a"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W2D1_BayesianStatistics/solutions/W2D1_Tutorial1_Solution_c0fbf37f.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=424 height=568 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W2D1_BayesianStatistics/static/W2D1_Tutorial1_Solution_c0fbf37f_1.png>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D1_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
