{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/tutorials/W3D3_OptimalControl/instructor/W3D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/tutorials/W3D3_OptimalControl/instructor/W3D3_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Optimal Control for Continuous State\n",
    "**Week 3, Day 3: Optimal Control**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Zhengwei Wu, Shreya Saxena, Xaq Pitkow\n",
    "\n",
    "__Content reviewers:__ Karolina Stosio, Roozbeh Farhoodi, Saeed Salehi, Ella Batty, Spiros Chavlis, Matt Krause and Michael Waskom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2022 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this tutorial, we will implement a continuous control task. You will design control inputs for a linear dynamical system to reach a target state. The state here is continuous-valued, i.e., takes on any real number from $-\\infty$ to $\\infty$.\n",
    "\n",
    "You have already learned about control for binary states in Tutorial 1, and you have learned about stochastic dynamics, latent states, and measurements yesterday. Now we introduce you to the new concepts of designing a controller with full observation of the state (linear quadratic regulator - LQR), and under partial observability of the state (linear quadratic gaussian - LQG).\n",
    "\n",
    "The running example we consider throughout the tutorial is a cat trying to catch a mouse in space using its handy little jet pack to navigate. \n",
    "\n",
    "In this tutorial you will:\n",
    "\n",
    "* Apply the ideas of optimal control to solve a toy example: Astrocat catching mice in space\n",
    "* Design an optimal controller with a full observation of the state (linear quadratic regulator - LQR)\n",
    "* Design an optimal controller under partial observability of the state (linear quadratic gaussian - LQG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for all videos in this tutorial.\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/8j5rs/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "execution": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "\n",
    "def plot_vs_time(s, slabel, color, goal=None, ylabel=None):\n",
    "  plt.plot(s, color, label=slabel)\n",
    "  if goal is not None:\n",
    "    plt.plot(goal, 'm--', label='goal $g$')\n",
    "  plt.xlabel(\"Time\", fontsize=14)\n",
    "  plt.legend(loc=\"upper right\")\n",
    "\n",
    "  if ylabel:\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "\n",
    "\n",
    "def plot_kf_state_vs_time(latent_states, estimates, title, goal=None):\n",
    "\n",
    "  fig = plt.figure(figsize=(12, 4))\n",
    "  plt.suptitle(title, y=1.05)\n",
    "  gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2])\n",
    "\n",
    "  ax0 = plt.subplot(gs[0])\n",
    "  ax0.plot(latent_states, estimates, 'r.')\n",
    "  ax0.plot(latent_states, latent_states, 'b')\n",
    "  # ax0.set_xlim([min(estimates), max(estimates)])\n",
    "  # ax0.set_ylim([min(estimates), max(estimates)])\n",
    "  ax0.set_xlabel('Latent State')\n",
    "  ax0.set_ylabel('Estimated State')\n",
    "  ax0.set_aspect('equal')\n",
    "\n",
    "  ax1 = plt.subplot(gs[1])\n",
    "  ax1.plot(latent_states, 'b', label = 'Latent State')\n",
    "  ax1.plot(estimates, 'r', label = 'Estimated State')\n",
    "\n",
    "  if goal is not None:\n",
    "    ax1.plot(goal, 'm--', label = 'goal')\n",
    "\n",
    "  ax1.set_xlabel('Time')\n",
    "  ax1.set_ylabel('State')\n",
    "  ax1.legend(loc=\"upper right\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "#Â Global variables\n",
    "T = 50\n",
    "standard_normal_noise = np.random.randn(T)\n",
    "standard_normal_noise_meas = np.random.randn(T)\n",
    "gaussian = namedtuple('Gaussian', ['mean', 'cov'])\n",
    "\n",
    "class ExerciseError(AssertionError):\n",
    "  pass\n",
    "\n",
    "\n",
    "def test_lds_class(lds_class):\n",
    "  from math import isclose\n",
    "  ldsys = lds_class(ini_state=2., noise_var=0.)\n",
    "  if not isclose(ldsys.dynamics(.9)[1], 1.8):\n",
    "    raise ExerciseError(\"'dynamics' method is not correctly implemented!\")\n",
    "  if not isclose(ldsys.dynamics_openloop(.9, 2., np.zeros(ldsys.T)-1.)[1], -0.2):\n",
    "    raise ExerciseError(\"'dynamics_openloop' method is not correctly implemented!\")\n",
    "  if not isclose(ldsys.dynamics_closedloop(.9, 2., np.zeros(ldsys.T)+.3)[0][1], 3.):\n",
    "    raise ExerciseError(\"s[t] in 'dynamics_closedloop' method is not correctly implemented!\")\n",
    "  if not isclose(ldsys.dynamics_closedloop(.9, 2., np.zeros(ldsys.T)+.3)[1][0], .6):\n",
    "    raise ExerciseError(\"a[t] in 'dynamics_closedloop' method is not correctly implemented!\")\n",
    "  ldsys.noise_var = 1.\n",
    "  if isclose(ldsys.dynamics(.9)[1], 1.8):\n",
    "    raise ExerciseError(\"Did you forget to add noise to your s[t+1] in 'dynamics'?\")\n",
    "  if isclose(ldsys.dynamics_openloop(.9, 2., np.zeros(ldsys.T)-1.)[1], -0.2):\n",
    "    raise ExerciseError(\"Did you forget to add noise to your s[t+1] in 'dynamics_openloop'?\")\n",
    "  if isclose(ldsys.dynamics_closedloop(.9, 2., np.zeros(ldsys.T)+.3)[0][1], 3.):\n",
    "    raise ExerciseError(\"Did you forget to add noise to your s[t+1] in 'dynamics_closedloop'?\")\n",
    "  if not isclose(ldsys.dynamics_closedloop(.9, 2., np.zeros(ldsys.T)+.3)[1][0], .6):\n",
    "    raise ExerciseError(\"Your input a[t] should not be noisy in 'dynamics_closedloop'.\")\n",
    "\n",
    "  print('Well Done!')\n",
    "\n",
    "\n",
    "def test_lqr_class(lqr_class):\n",
    "  from math import isclose\n",
    "  lqreg = lqr_class(ini_state=2., noise_var=0.)\n",
    "  lqreg.goal = np.array([-2, -2])\n",
    "  s = np.array([1, 2])\n",
    "  a = np.array([3, 4])\n",
    "  if not isclose(lqreg.calculate_J_state(s), 25):\n",
    "    raise ExerciseError(\"'calculate_J_state' method is not correctly implemented!\")\n",
    "  if not isclose(lqreg.calculate_J_control(a), 25):\n",
    "    raise ExerciseError(\"'calculate_J_control' method is not correctly implemented!\")\n",
    "\n",
    "  print('Well Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Exploring a Linear Dynamical System (LDS) with Open-Loop and Closed-Loop Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Flying Through Space\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1Zv411B7WV\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"MLUTR8z16jI\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "In this example, a cat is trying to catch a mouse in space. The location of the mouse is the goal state $g$, here a static goal. Later on, we will make the goal time-varying, i.e., $g(t)$. The cat's location is the state of the system $s_t$. The state has its internal dynamics: think of the cat drifting slowly in space. These dynamics are such that the state at the next time step $s_{t+1}$ are a linear function of the current state $s_t$. There is some process noise affecting the state (think about engine corrosions on the little jetpack causing unintended movements) here modeled as Gaussian noise $w_t$.\n",
    "\n",
    "The control input or action $a_t$ is the action of the jet pack, which has an effect $Ba_t$ on the state at the next time step $s_{t+1}$. In this tutorial, we will be designing the action $a_t$ to reach the goal $g$, with known state dynamics.\n",
    "\n",
    "Thus, our linear discrete-time system evolves according to the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "s_{t+1} &= Ds_t + Ba_t + w_t \\tag{1}\\\\\n",
    "s_{0} &= s_{init}\n",
    "\\end{align}\n",
    "\n",
    "with \n",
    "\n",
    "$t$: time step, ranging from $1$ to $T$, where $T$ is the time horizon.\n",
    "\n",
    "$s_t$: state at time $t$.\n",
    "\n",
    "$a_t$: action at time $t$ (also known as \"control input\").\n",
    "\n",
    "$w_t$: gaussian noise at time $t$.\n",
    "\n",
    "$D$: transition matrix.\n",
    "\n",
    "$B$: input matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "For simplicity, we will consider the 1D case, where the matrices reduce to scalars, and the states, control and noise are one-dimensional as well. Specifically, $D$ and $B$ are scalars.\n",
    "\n",
    "We will consider the goal $g$ to be the origin, i.e., $g=0$, for Exercises 1 and 2.2. Later on, we will explore scenarios where the goal state changes over time $g(t)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stability**\n",
    "\n",
    "The system is stable, i.e., the output remains finite for any finite initial condition $s_{init}$, if $|D| < 1$. Note that if the state dynamics are stable, the state eventually reaches $0$ (No control is needed!). However, when $|D|>1$ or the goal $g \\neq 0$ selecting an appropriate sequence of actions becomes essential to completing the task.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Open-loop control and Closed-loop linear control**\n",
    "\n",
    "In *open-loop control*, $a_t$ is not a function of $s_t$. In *closed-loop linear control*, $a_t$ is a linear function of the state $s_t$. Specifically, $a_t$ is the control gain, $L_t$, multiplied by $s_t$, i.e., $a_t=L_t s_t$. \n",
    "\n",
    "In the next excercise, you will explore what happens when nothing is controlling the system, when the system is being controlled following an open-loop control policy, and when the system is under closed-loop linear control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1: Implement state evolution equations\n",
    "\n",
    "Implement the state evolution equations in the class methods as provided below, for the following cases:\n",
    "\n",
    "(a) no control: `def dynamics`\n",
    "\n",
    "(b) open-loop control: `def dynamics_openloop`\n",
    "\n",
    "(c) closed-loop control: `def dynamics_closedloop`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Tip:** refer to Equation (1) above. The provided code uses the same notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "\n",
    "class LDS:\n",
    "  \"\"\"\n",
    "    T: Length of timeline (global, fixed variable)\n",
    "    standard_normal_noise: Global noise of length T drawn from N(0, 1)\n",
    "    noise: Gaussian noise N(mean, var) = mean + sqrt(var) * standard_normal_noise\n",
    "    ...\n",
    "  \"\"\"\n",
    "  def __init__(self, ini_state: float, noise_var: float, static_noise=False):\n",
    "    self.ini_state = ini_state\n",
    "    self.noise_var = noise_var\n",
    "    self.T = T\n",
    "    self.static_noise = static_noise\n",
    "\n",
    "  def dynamics(self, D: float):\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    s[0] = self.ini_state\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      ####################################################################\n",
    "      ## Insert your code here to fill with the state dynamics equation\n",
    "      ## without any control input\n",
    "      ## complete the function and remove\n",
    "      raise NotImplementedError(\"Exercise: Please complete 'dynamics'\")\n",
    "      ####################################################################\n",
    "      # calculate the state of t+1\n",
    "      s[t + 1] = ...\n",
    "\n",
    "    return s\n",
    "\n",
    "  def dynamics_openloop(self, D: float, B: float, a: np.ndarray):\n",
    "\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    s[0] = self.ini_state\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      ####################################################################\n",
    "      ## Insert your code here to fill with the state dynamics equation\n",
    "      ## with open-loop control input a[t]\n",
    "      ## complete the function and remove\n",
    "      raise NotImplementedError(\"Please complete 'dynamics_openloop'\")\n",
    "      ####################################################################\n",
    "      # calculate the state of t+1\n",
    "      s[t + 1] = ...\n",
    "\n",
    "    return s\n",
    "\n",
    "  def dynamics_closedloop(self, D: float, B: float, L: np.ndarray):\n",
    "\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    a = np.zeros(T - 1)\n",
    "    s[0] = self.ini_state\n",
    "\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      ####################################################################\n",
    "      ## Insert your code here to fill with the state dynamics equation\n",
    "      ## with closed-loop control input as a function of control gain L.\n",
    "      ## complete the function and remove\n",
    "      raise NotImplementedError(\"Please complete 'dynamics_closedloop'\")\n",
    "      ####################################################################\n",
    "      # calculate the current action\n",
    "      a[t] = ...\n",
    "      # calculate the next state\n",
    "      s[t + 1] = ...\n",
    "\n",
    "    return s, a\n",
    "\n",
    "\n",
    "# Test your function\n",
    "test_lds_class(LDS)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class LDS:\n",
    "  \"\"\"\n",
    "    T: Length of timeline (global, fixed variable)\n",
    "    standard_normal_noise: Global noise of length T drawn from N(0, 1)\n",
    "    noise: Gaussian noise N(mean, var) = mean + sqrt(var) * standard_normal_noise\n",
    "    ...\n",
    "  \"\"\"\n",
    "  def __init__(self, ini_state: float, noise_var: float, static_noise=False):\n",
    "    self.ini_state = ini_state\n",
    "    self.noise_var = noise_var\n",
    "    self.T = T\n",
    "    self.static_noise = static_noise\n",
    "\n",
    "  def dynamics(self, D: float):\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    s[0] = self.ini_state\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      # calculate the state of t+1\n",
    "      s[t + 1] = D * s[t] + noise[t]\n",
    "\n",
    "    return s\n",
    "\n",
    "  def dynamics_openloop(self, D: float, B: float, a: np.ndarray):\n",
    "\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    s[0] = self.ini_state\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      # calculate the state of t+1\n",
    "      s[t + 1] = D * s[t] + B * a[t] + noise[t]\n",
    "\n",
    "    return s\n",
    "\n",
    "  def dynamics_closedloop(self, D: float, B: float, L: np.ndarray):\n",
    "\n",
    "    s = np.zeros(T)  # states initialization\n",
    "    a = np.zeros(T - 1)\n",
    "    s[0] = self.ini_state\n",
    "\n",
    "    if self.static_noise:\n",
    "      noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "    else:\n",
    "      noise = np.sqrt(self.noise_var) * np.random.randn(T)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "      # calculate the current action\n",
    "      a[t] = L[t] * s[t]\n",
    "      # calculate the next state\n",
    "      s[t + 1] = D * s[t] + B * a[t] + noise[t]\n",
    "\n",
    "    return s, a\n",
    "\n",
    "\n",
    "# Test your function\n",
    "test_lds_class(LDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 1.1: Explore no control vs. open-loop control vs. closed-loop control\n",
    "\n",
    "Once your code above passes the test, use the interactive demo below to visualize the effects of different kinds of control inputs.\n",
    "\n",
    "(a) For the no-control case, can you identify two distinct outcomes, depending on the value of D? Why?\n",
    "\n",
    "(b) The open-loop controller works well--or does it? Are there any problems in in challenging (high noise) conditions.\n",
    "\n",
    "(c) Does the closed-loop controller fare better with the noise? Vary the values of $L$ and find a range where it quickly reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "#@markdown Play around (attentively) with **`a`** and **`L`** to see the effect on the open-loop controlled and closed-loop controlled state.\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(D=widgets.FloatSlider(0.95, description=\"D\", min=0.85, max=1.05),\n",
    "                  L=widgets.FloatSlider(-0.3, description=\"L\", min=-0.6, max=0.),\n",
    "                  a=widgets.FloatSlider(-1., description=\"a\", min=-2., max=1.),\n",
    "                  B=widgets.FloatSlider(2., description=\"B\", min=1., max=3.),\n",
    "                  noise_var=widgets.FloatSlider(.1, description=\"noise_var\", min=0., max=.2),\n",
    "                  ini_state=widgets.FloatSlider(2., description=\"ini_state\", min=2., max=10.))\n",
    "\n",
    "def simulate_lds(D=0.95, L=-0.3, a=-1., B=2., noise_var=0.1, ini_state=2.):\n",
    "  \"\"\"\n",
    "    ...\n",
    "  \"\"\"\n",
    "  static_noise = True\n",
    "\n",
    "  # linear dynamical system\n",
    "  lds = LDS(ini_state, noise_var, static_noise)\n",
    "\n",
    "  # No control\n",
    "  s_no_control=lds.dynamics(D)\n",
    "\n",
    "  # Open loop control\n",
    "  at = np.append(a, np.zeros(T - 1))\n",
    "  s_open_loop = lds.dynamics_openloop(D, B, at)\n",
    "\n",
    "  # Closed loop control\n",
    "  Lt =  np.zeros(T) + L\n",
    "  s_closed_loop, a_closed_loop = lds.dynamics_closedloop(D, B, Lt)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(s_no_control, 'b', label='No control')\n",
    "  plt.plot(s_open_loop, 'g', label='Open Loop with a = {}'.format(a))\n",
    "  plt.plot(s_closed_loop, 'r', label='Closed Loop with L = {}'.format(L))\n",
    "  plt.plot(np.zeros(T), 'm--', label='goal')\n",
    "  plt.title('LDS State Evolution')\n",
    "  plt.ylabel('State', fontsize=14)\n",
    "  plt.xlabel('Time', fontsize=14)\n",
    "  plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "In Exercise 1.2, you should have noticed the following:\n",
    "\n",
    "* No control (blue): the dynamics parameter D controls how fast the dynamics decay\n",
    "  towards 0. For -1<D<1, the system is stable and therefore approaches zero quickly.\n",
    "  However, D>1 produces an unstable system, causing , you should have\n",
    "  noticed that the 'no control' state (blue curve) rapidly explodes\n",
    "  (i.e., heads off to infinity)\n",
    "\n",
    "* Open-loop control: While the open-loop state (green curve) often reaches the goal\n",
    "  quickly, it may not stay there. Under high noise conditions, it tends to\n",
    "  drift away from the goal, though you may not see this in every simulation.\n",
    "\n",
    "* Closed-loop control: The closed-loop state (red curve) reaches the goal and\n",
    "  stays there even in the presence of noise. It converges especially quickly for\n",
    "  Ls around 0.45\n",
    "\n",
    "  Remember that in closed-loop control,\n",
    "  we have a[t]=L[t] * s[t] $. Note that with a constant control gain $L[t]=L,\n",
    "  the state evolution equations can be rearranged to show that the stability of\n",
    "  the closed-loop system now depends on the value of D+BL. (See Equation 2, below).\n",
    "\n",
    "  If $|D+BL|<1$, our closed-loop system will be stable. More generally, you can\n",
    "  view the role of a closed-loop control input as changing the system *dynamics*\n",
    "  in an optimal way to reach the goal.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 1.2: Exploring the closed-loop setting further \n",
    "Execute the cell below to visualize the MSE between the state and goal, as a function of control gain $L$. You should see a U-shaped curve, with a minimum MSE. The control gain at which the minimum MSE is reached, is the 'optimal' constant control gain for minimizing MSE, here called the numerical optimum. \n",
    "\n",
    "A green dashed line is shown $L = -\\frac{D}{B}$ with $D=1.1$ and $B=2$. Why is this the theoretical optimal control gain for minimizing MSE of the state $s$ to the goal $g=0$? Examine how the states evolve with a constant gain $L$.\n",
    "\n",
    "\\begin{align}\n",
    "s_{t+1} &= Ds_t + Ba_t + w_t \\\\\n",
    "&= Ds_t + B(Ls_t) + w_t \\\\\n",
    "&= (D+BL)s_t + w_t \\tag{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to visualize MSE between state and goal, as a function of control gain\n",
    "def calculate_plot_mse():\n",
    "  D, B, noise_var, ini_state = 1.1, 2., 0.1, 2.\n",
    "  num_iterations = 50\n",
    "  num_candidates = 100\n",
    "\n",
    "  control_gain_array = np.linspace(0.1, 1., num_candidates)\n",
    "  mse_array = np.zeros([num_candidates, num_iterations])\n",
    "\n",
    "  for j in range(num_iterations):\n",
    "    for i in range(num_candidates):\n",
    "      lds = LDS(ini_state, noise_var)\n",
    "      L = - np.ones(T) * control_gain_array[i]\n",
    "      s, a = lds.dynamics_closedloop(D, B, L)\n",
    "      mse_array[i, j] = np.sum(s**2)\n",
    "\n",
    "  opt = -control_gain_array[np.argmin(np.mean(mse_array, axis=1))]\n",
    "  plt.figure()\n",
    "  plt.plot(-control_gain_array, np.mean(mse_array, axis=1), 'b')\n",
    "  plt.axvline(x=-D/B, color='g', linestyle='--')\n",
    "  plt.xlabel(\"control gain (L)\", fontsize=14)\n",
    "  plt.ylabel(\"MSE between state and goal\" , fontsize=14)\n",
    "  plt.title(f\"MSE vs control gain, L*={opt:0.2f}\", fontsize=20)\n",
    "  plt.show()\n",
    "  return opt\n",
    "\n",
    "opt = calculate_plot_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's visualize the evolution of the system as we change the control gain. \n",
    "\n",
    "Using the next demo, explore the behavior of the system when over- and under- ambitious values of $L$ are used. The initial position of the slider corresponds to the optimal control gain $L^*$, the one that gets us the minimum MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "#@markdown Explore different values of control gain **`L`** (close to optimal, over- and under- ambitious) \\\\\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(L=widgets.FloatSlider(opt, description=\"L\", min=-1.05, max=0.051))\n",
    "\n",
    "def simulate_L(L):\n",
    "\n",
    "  D, B, noise_var, ini_state = 1.1, 2., 0.1, 2.\n",
    "  static_noise  = True\n",
    "  lds = LDS(ini_state, noise_var, static_noise)\n",
    "  # Closed loop control with the numerical optimal control gain\n",
    "  Lt = np.ones(T) * L\n",
    "  s_closed_loop_choice, _ = lds.dynamics_closedloop(D, B, Lt)\n",
    "  # Closed loop control with the theoretical optimal control gain\n",
    "  L_theory = - D / B * np.ones(T)\n",
    "  s_closed_loop_theoretical, _ = lds.dynamics_closedloop(D, B, L_theory)\n",
    "  # Plotting closed loop state evolution with both theoretical and numerical optimal control gains\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plot_vs_time(s_closed_loop_theoretical,\n",
    "                'Closed Loop (Theoretical optimal control gain)','b')\n",
    "  plot_vs_time(s_closed_loop_choice,\n",
    "                f'Closed Loop (your choice of L = {L:.2f})', 'g',\n",
    "                goal=np.zeros(T), ylabel=\"State\")\n",
    "  plt.title(f'Closed-loop State Evolution. L* = {opt:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "In Demo 1.2, you should have seen that the optimal control gain (L = -0.45) takes a\n",
    "short amount of time to get to the goal, and then stays there. We can try to get\n",
    "to the goal in an even shorter time using an 'over-ambitious' control gain (L < -0.45), but\n",
    "this may actually overshoot the goal and may cause oscillations in the system,\n",
    "thus increasing the MSE. On the other hand, an 'under-ambitious' control gain\n",
    "takes a longer time to get to the goal and thus increases the MSE.\n",
    "Finally, at L>0, the system runs away to infinity.\n",
    "\n",
    "Why is L=-D/B optimal for reaching our goal? Recall that our next state is\n",
    "(D+B*L)*s[t] + noise. Plugging that L=-D/B causes that leading term to become zero,\n",
    "which is our goal. Since the noise has zero mean, it's not possible to do any better!\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Designing an optimal control input using a linear quadratic regulator (LQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Linear quadratic regulator (LQR)\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1sz411v7za\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"NZSwDy7wtIs\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1 Constraints on the system\n",
    "Now we will start imposing additional constraints on our system. For example, if you explored different values for $s_{init}$ above, you would have seen very large values for $a_t$ in order to get to the mouse in a short amount of time. However, perhaps the design of our jetpack makes it dangerous to use large amounts of fuel in a single timestep. We certainly do not want to explode, so we would like to keep the actions $a_t$ as small as possible while still maintaining good control.\n",
    "\n",
    "Moreover, in Exercise 1, we had restricted ourselves to a static control gain $L_t \\equiv L$. How would we vary it if we could?\n",
    "\n",
    "This leads us to a more principled way of designing the optimal control input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Setting up a cost function \n",
    "\n",
    "In a finite-horizon LQR problem,  the cost function is defined as: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "J({\\bf s},{\\bf a}) &=& J_{state}({\\bf s}) + \\rho J_{control}({\\bf a}) \\\\\n",
    " &=& \\sum_{t = 0}^{T} (s_{t}-g)^2 + \\rho \\sum_{t=0}^{T-1}a_{t}^2 \\tag{3}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\rho$ is the weight on the control effort cost, as compared to the cost of not being at the goal. Here, ${\\bf a} = \\{a_t\\}_{t=0}^{T-1}$, ${\\bf s} = \\{s_t\\}_{t=0}^{T}$. This is a quadratic cost function. In Exercise $2$, we will only explore $g=0$, in which case $J_{state}({\\bf s})$ can also be expressed as $\\sum_{t = 0}^{T} s_{t}^2$. In Exercise $3$, we will explore a non-zero time-varying goal.\n",
    "\n",
    "The goal of the LQR problem is to find control ${\\bf a}$ such that $J({\\bf s},{\\bf a})$ is minimized. The goal is then to find the control gain at each time point, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{argmin} _{\\{L_t\\}_{t=0}^{T-1}}  J({\\bf s},{\\bf a}) \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where $a_t = L_t s_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2 Solving LQR\n",
    "\n",
    "The solution to Equation (4), i.e., LQR for a finite time horizon, can be obtained via Dynamic Programming. For details, check out [this lecture by Stephen Boyd](https://stanford.edu/class/ee363/lectures/dlqr.pdf).\n",
    "\n",
    "For an infinite time horizon, one can obtain a closed-form solution using Riccati equations, and the solution for the control gain becomes time-invariant, i.e., $L_t \\equiv L$. We will use this in Exercise 4. For  details, check out [this other lecture by Stephen Boyd](https://stanford.edu/class/ee363/lectures/dlqr-ss.pdf).\n",
    "\n",
    "<br>\n",
    "\n",
    "Additional reference for entire section:\n",
    "\n",
    "[Bertsekas, Dimitri P. _Dynamic programming and optimal control_. Vol. 1. No. 2. Belmont, MA: Athena scientific, 1995](http://www.athenasc.com/dpbook.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Implement the cost function\n",
    "\n",
    "The cost function $J({\\bf s}, {\\bf a})$ can be divided into two parts: $J_{state}({\\bf s})$ and $J_{control}({\\bf a})$. \n",
    "\n",
    "Code up these two parts in the class methods `def calculate_J_state` and `def calculate_J_control` in the following helper class for LQR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "\n",
    "class LQR(LDS):\n",
    "  def __init__(self, ini_state, noise_var, static_noise=False):\n",
    "    super().__init__(ini_state, noise_var, static_noise)\n",
    "    self.T = T\n",
    "    self.goal = np.zeros(T)  # The class LQR only supports g=0\n",
    "\n",
    "  def control_gain_LQR(self, D, B, rho):\n",
    "    P = np.zeros(self.T)  # Dynamic programming variable\n",
    "    L = np.zeros(self.T - 1)  # control gain\n",
    "    P[-1] = 1\n",
    "    for t in range(self.T - 1):\n",
    "        P_t_1 = P[self.T - t - 1]\n",
    "        P[self.T - t-2] = (1 + P_t_1 * D**2 - D * P_t_1 * B / (rho + P_t_1 * B**2) * B * P_t_1 * D)\n",
    "        L[self.T - t-2] = - (1 / (rho + P_t_1 * B**2) * B * P_t_1 * D)\n",
    "    return L\n",
    "\n",
    "  def calculate_J_state(self, s:np.ndarray):\n",
    "    ########################################################################\n",
    "    ## Insert your code here to calculate J_state(s) (see Eq. 3)\n",
    "    ## complete the function and remove\n",
    "    raise NotImplementedError(\"Please complete 'calculate_J_state'\")\n",
    "    ########################################################################\n",
    "    # calculate the state\n",
    "    J_state = ...\n",
    "\n",
    "    return J_state\n",
    "\n",
    "  def calculate_J_control(self, a:np.ndarray):\n",
    "    ########################################################################\n",
    "    ## Insert your code here to calculate J_control(a) (see Eq. 3).\n",
    "    ## complete the function and remove\n",
    "    raise NotImplementedError(\"Please complete 'calculate_J_control'\")\n",
    "    ########################################################################\n",
    "    # calculate the control\n",
    "    J_control = ...\n",
    "\n",
    "    return J_control\n",
    "\n",
    "\n",
    "# Test class\n",
    "test_lqr_class(LQR)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class LQR(LDS):\n",
    "  def __init__(self, ini_state, noise_var, static_noise=False):\n",
    "    super().__init__(ini_state, noise_var, static_noise)\n",
    "    self.T = T\n",
    "    self.goal = np.zeros(T)  # The class LQR only supports g=0\n",
    "\n",
    "  def control_gain_LQR(self, D, B, rho):\n",
    "    P = np.zeros(self.T)  # Dynamic programming variable\n",
    "    L = np.zeros(self.T - 1)  # control gain\n",
    "    P[-1] = 1\n",
    "    for t in range(self.T - 1):\n",
    "        P_t_1 = P[self.T - t - 1]\n",
    "        P[self.T - t-2] = (1 + P_t_1 * D**2 - D * P_t_1 * B / (rho + P_t_1 * B**2) * B * P_t_1 * D)\n",
    "        L[self.T - t-2] = - (1 / (rho + P_t_1 * B**2) * B * P_t_1 * D)\n",
    "    return L\n",
    "\n",
    "  def calculate_J_state(self, s:np.ndarray):\n",
    "    # calculate the state\n",
    "    J_state = np.sum((s - self.goal)**2)\n",
    "\n",
    "    return J_state\n",
    "\n",
    "  def calculate_J_control(self, a:np.ndarray):\n",
    "    # calculate the control\n",
    "    J_control = np.sum(a**2)\n",
    "\n",
    "    return J_control\n",
    "\n",
    "# Test class\n",
    "test_lqr_class(LQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 2.2: LQR to the origin \n",
    "\n",
    "In this exercise, we will use your new LQR controller to track a static goal at $g=0$. Here, we will explore how varying $\\rho$ (the weight on the control effort cost) affects the state trajectory, actions selected, and control gain.\n",
    "\n",
    "1. Play around with the value of $\\rho$ and see the effects on the sequence of states, the magnitude of the actions, and variability of control gain.\n",
    "2. What do you notice on the control gain when $\\rho$ is too large? and too small?\n",
    "3. For different values of $\\rho$, how does the magnitude of the actions change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "#@markdown Explore different values of control gain **`L`** (close to optimal, over- and under- ambitious) \\\\\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(rho=widgets.FloatSlider(25., description=\"rho\", min=0., max=50.))\n",
    "\n",
    "def simulate_rho(rho=1.):\n",
    "    D, B, ini_state, noise_var = 1.1, 2., 1., .1  # state parameter\n",
    "    static_noise = True\n",
    "    lqr = LQR(ini_state, noise_var, static_noise)\n",
    "    L = lqr.control_gain_LQR(D, B, rho)\n",
    "    s_lqr, a_lqr = lqr.dynamics_closedloop(D, B, L)\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.suptitle('LQR Control for rho = {}'.format(rho), y=1.05)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plot_vs_time(s_lqr,'State evolution','b',goal=np.zeros(T))\n",
    "    plt.ylabel('State $s_t$')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plot_vs_time(a_lqr,'LQR Action','b')\n",
    "    plt.ylabel('Action $a_t$')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plot_vs_time(L,'Control Gain','b')\n",
    "    plt.ylabel('Control Gain $L_t$')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "* A small value for rho results in large values for |a[t]|.\n",
    "* A large value for rho leads to small values for |a[t]|.\n",
    "* The control gain becomes more time-varying (as opposed to fairly static)\n",
    "  for large rho.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: The tradeoff between state cost and control cost\n",
    "\n",
    "In Exercise 2.1, you implemented code to calculate for $J_{state}$ and $J_{control}$ in the class methods for the class LQR.\n",
    "\n",
    "We will now plot them against each other for varying values of $\\rho$ to explore the tradeoff between state cost and control cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to visualize the tradeoff between state and control cost\n",
    "def calculate_plot_costs():\n",
    "\n",
    "  D, B, noise_var, ini_state = 1.1, 2., 0.1, 10.\n",
    "  num_iterations = 50\n",
    "  num_candidates = 100\n",
    "\n",
    "  rho_array = np.linspace(0.2, 40, num_candidates)\n",
    "  J_state = np.zeros([num_candidates, num_iterations])\n",
    "  J_control = np.zeros([num_candidates, num_iterations])\n",
    "\n",
    "  for j in range(num_iterations):\n",
    "    for i in np.arange(len(rho_array)):\n",
    "      lqr = LQR(ini_state, noise_var)\n",
    "      L = lqr.control_gain_LQR(D, B, rho_array[i])\n",
    "      s_lqr, a_lqr = lqr.dynamics_closedloop(D, B, L)\n",
    "      J_state[i, j] = lqr.calculate_J_state(s_lqr)\n",
    "      J_control[i, j] = lqr.calculate_J_control(a_lqr)\n",
    "\n",
    "  J_state = np.mean(J_state, axis=1)\n",
    "  J_control = np.mean(J_control, axis=1)\n",
    "\n",
    "  fig = plt.figure(figsize=(6, 6))\n",
    "  plt.plot(J_state, J_control, '.b')\n",
    "  plt.xlabel(\"$J_{state} = \\sum_{t = 0}^{T} (s_{t}-g)^2$\", fontsize=14)\n",
    "  plt.ylabel(\"$J_{control} = \\sum_{t=0}^{T-1}a_{t}^2$\" , fontsize=14)\n",
    "  plt.title(\"Error vs control effort\", fontsize=20)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "calculate_plot_costs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should notice the bottom half of a 'C' shaped curve, forming the tradeoff between the state cost and the control cost under optimal closed-loop linear control.\n",
    "\n",
    "For a desired value of the state cost, we cannot reach a lower control cost than the curve in the above plot. Similarly, for a desired value of the control cost, we must accept that amount of state cost. For example, if you know that you have a limited amount of fuel, which determines your maximum control cost to be $J_{control}^{max}$. \n",
    "\n",
    "You will be able to show that you will not be able to track your state with higher accuracy than the corresponding $J_{state}$ as given by the graph above. This is thus an important curve when designing a system and exploring its control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: LQR for tracking a time-varying goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Tracking a moving goal\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1up4y1S7gg\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"HOoqM7kBWSY\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In a more realistic situation, the mouse would move around constantly. Suppose you were able to predict the movement of the mouse as it bounces from one place to another. This becomes your goal trajectory $g_t$.\n",
    "\n",
    "When the target state, denoted as $g_t$, is not $0$, the cost function becomes\n",
    "\n",
    "\\begin{equation}\n",
    "J({\\bf a}) = \\sum_{t = 0}^{T} (s_{t}- g_t) ^2 + \\rho \\sum_{t=0}^{T-1}(a_{t}-\\bar a_t)^2\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\bar a_t$ is the desired action based on the goal trajectory. In other words, the controller considers the goal for the next time step, and designs a preliminary control action that gets the state at the next time step to the desired goal. Specifically, without taking into account noise $w_t$, we would like to design $\\bar a_t$ such that $s_{t+1}=g_{t+1}$. Thus, from Equation $(1)$,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g_{t+1} &=& Ds_t + B \\bar a_t\\\\\n",
    "\\bar a_{t} &=& \\frac{- Ds_t + g_{t+1}}{B}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The final control action $a_t$ is produced by adding this desired action $\\bar a_t$ with the term with the control gain $L_t(s_t - g_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to include class\n",
    "#@markdown for LQR control to desired time-varying goal\n",
    "\n",
    "class LQR_tracking(LQR):\n",
    "  def __init__(self, ini_state, noise_var, goal):\n",
    "    super().__init__(ini_state, noise_var)\n",
    "    self.T = T\n",
    "    self.goal = goal\n",
    "\n",
    "  def dynamics_tracking(self, D, B, L):\n",
    "\n",
    "    s = np.zeros(self.T) # states intialization\n",
    "    s[0] = self.ini_state\n",
    "\n",
    "    noise = np.sqrt(self.noise_var) * standard_normal_noise\n",
    "\n",
    "    a = np.zeros(self.T) # control intialization\n",
    "    a_bar = np.zeros(self.T)\n",
    "    for t in range(self.T - 1):\n",
    "        a_bar[t] = ( - D * s[t] + self.goal[t + 1]) / B\n",
    "        a[t] =  L[t] * (s[t] - self.goal[t]) + a_bar[t]\n",
    "        s[t + 1] = D * s[t] + B * a[t] + noise[t]\n",
    "\n",
    "    return s, a, a_bar\n",
    "\n",
    "  def calculate_J_state(self,s):\n",
    "    J_state = np.sum((s-self.g)**2)\n",
    "    return J_state\n",
    "\n",
    "  def calculate_J_control(self, a, a_bar):\n",
    "    J_control = np.sum((a-a_bar)**2)\n",
    "    return J_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 3: LQR control to desired time-varying goal\n",
    "\n",
    "Use the demo below to explore how LQR tracks a time-varying goal. Starting with the sinusoidal goal function `sin`, investigate how the system reacts with different values of $\\rho$ and process noise variance. Next, explore other time-varying goals, such as a step function and ramp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "#@markdown Explore different values of control gain **`L`** (close to optimal, over- and under- ambitious) \\\\\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(rho=widgets.FloatSlider(20., description=\"rho\", min=0.1, max=40.),\n",
    "                  noise_var=widgets.FloatSlider(0.1, description=\"noise_var\", min=0., max=1.),\n",
    "                  goal_func=widgets.RadioButtons(options=['sin', 'step', 'ramp'],\n",
    "                                                 description='goal_func:',\n",
    "                                                 disabled=False))\n",
    "\n",
    "def simulate_tracking(rho, noise_var, goal_func):\n",
    "  D, B, ini_state = 1.1, 1., 0.\n",
    "  if goal_func == 'sin':\n",
    "      goal = np.sin(np.arange(T) * 2 * np.pi * 5 / T)\n",
    "  elif goal_func == 'step':\n",
    "      goal = np.zeros(T)\n",
    "      goal[int(T / 3):] = 1.\n",
    "  elif goal_func == 'ramp':\n",
    "      goal = np.zeros(T)\n",
    "      goal[int(T / 3):] = np.arange(T - int(T / 3)) / (T - int(T / 3))\n",
    "\n",
    "  lqr_time = LQR_tracking(ini_state, noise_var, goal)\n",
    "  L = lqr_time.control_gain_LQR(D, B, rho)\n",
    "  s_lqr_time, a_lqr_time, a_bar_lqr_time = lqr_time.dynamics_tracking(D, B, L)\n",
    "\n",
    "  plt.figure(figsize=(13, 5))\n",
    "  plt.suptitle('LQR Control for time-varying goal', y=1.05)\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_vs_time(s_lqr_time,'State evolution $s_t$','b',goal, ylabel=\"State\")\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_vs_time(a_lqr_time, 'Action $a_t$', 'b', ylabel=\"Action\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "In Exercise 3, you should have noticed that:\n",
    "* The system follows time varying goals rather well, with little change to the\n",
    "   cost function and the control equations.\n",
    "\n",
    "* Setting rho=0 leads to noise in the first part of the time series.\n",
    "  Here, we see that the control cost in fact acts as a regularizer.\n",
    "\n",
    "* Larger values of the process noise variance lead to a higher MSE between the\n",
    "  state and the desired goal.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Control of an partially observed state using a Linear Quadratic Gaussian (LQG) controller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.1 Introducing the LQG Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Linear Quadratic Gaussian (LQG) Control\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1xZ4y1u73B\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"c_D7iDLT_bw\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In practice, the controller does not have full access to the state. For example, your jet pack in space may be controlled by Mission Control back on earth!  In this case, noisy measurements $m_t$ of the state $s_t$ are taken via radar, and the controller needs to (1) estimate the true state, and (2) design an action based on this estimate. \n",
    "\n",
    "Fortunately, the separation principle tells us that it is optimal to do (1) and (2) separately. This makes our problem much easier, since we already know how to do each step.  \n",
    "\n",
    "1) *State Estimation*  \n",
    "Can we recover the state from the measurement? \n",
    "yesterday you learned that the states $\\hat{s}_t$ can be estimated from the measurements $m_t$ using the **Kalman filter**. \n",
    "\n",
    "2) *Design Action*  \n",
    "In Sections 2 and 3 above, we just learned about the LQR controller which designs an action based on the state. The separation principle tells us that it is sufficient to replace the use of the state in LQR with the *estimated* state, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "a_t = L_t \\hat s_t\n",
    "\\end{equation}\n",
    "\n",
    "The state dynamics will then be:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{t+1} = D s_t + B a_t + w_t\n",
    "\\end{equation}\n",
    "\n",
    "where $w_t$ is the process noise (proc_noise), and the observation / measurement is:\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = C s_t + v_t\n",
    "\\end{equation}\n",
    "\n",
    "with $C$ is the observation matrix and $v_t$ is the measurement noise (meas_noise).\n",
    "\n",
    "The combination of (1) state estimation and (2) action design using LQR is known as a **linear quadratic gaussian (LQG)**. Yesterday, you completed the code for the Kalman filter. Based on that, you will code up the LQG controller. For these exercises, we will return to using the goal $g=0$, as in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 4.1: The Kalman filter in conjunction with a linear closed-loop controller (LQG Control)\n",
    "\n",
    "Inspect the `KalmanFilter` class, its method `get_estimate(self, m)` will help you refresh your understanding of how the filtering mechanism works. The only difference from yesterday's implementation is that today's Kalman filter takes into account the action when computing the estimates.\n",
    "\n",
    "Also, inspect the `LQG` class. You will find that the only difference between this class and the `LQR` class is that the system now returns a noisy measurement of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class KalmanFilter():\n",
    "  def __init__(self, transition_matrix, transition_covariance, observation_matrix, observation_covariance, initial_state_mean, initial_state_covariance):\n",
    "    self.D = transition_matrix\n",
    "    self.Q = transition_covariance\n",
    "    self.C = observation_matrix\n",
    "    self.R = observation_covariance\n",
    "    self.prior = gaussian(initial_state_mean, initial_state_covariance)\n",
    "\n",
    "  def get_estimate(self, m, a):\n",
    "\n",
    "    predicted_estimate = self.D * self.prior.mean + a\n",
    "    predicted_covariance = self.D**2 * self.prior.cov + self.Q\n",
    "\n",
    "    innovation_estimate = m - self.C * predicted_estimate\n",
    "    innovation_covariance = self.C**2 * predicted_covariance + self.R\n",
    "\n",
    "    # Kalman gain is the weight given to the innovation (ie., the difference between the measurement and the predicted measurement)\n",
    "    K = predicted_covariance * self.C / innovation_covariance\n",
    "    updated_mean = predicted_estimate + K * innovation_estimate\n",
    "    updated_cov = (1 - K * self.C) * predicted_covariance\n",
    "    posterior = gaussian(updated_mean, updated_cov)\n",
    "\n",
    "    # Current posterior becomes next-step prior\n",
    "    self.prior = posterior\n",
    "\n",
    "    return posterior.mean\n",
    "\n",
    "\n",
    "class LQG():\n",
    "  def __init__(self, transition_matrix, transition_covariance, observation_matrix, observation_covariance, initial_state_mean, initial_state_covariance, ntrials=1, static_noise=False):\n",
    "    self.D = transition_matrix\n",
    "    self.Q = transition_covariance\n",
    "    self.C = observation_matrix\n",
    "    self.R = observation_covariance\n",
    "    self.static_noise = static_noise\n",
    "    self.ntrials = ntrials\n",
    "    self.t = 0\n",
    "    self.latent_states = np.zeros([T, ntrials])\n",
    "    self.latent_states[0] = initial_state_mean + np.sqrt(initial_state_covariance) * standard_normal_noise[0]\n",
    "\n",
    "  def step(self, action):\n",
    "    self.t += 1\n",
    "    if self.static_noise:\n",
    "      self.latent_states[self.t] = self.D * self.latent_states[self.t-1] + action + np.sqrt(self.Q) * standard_normal_noise[self.t-1]\n",
    "      measurement = self.C * self.latent_states[self.t] + np.sqrt(self.R) * standard_normal_noise_meas[self.t]\n",
    "    else:\n",
    "      self.latent_states[self.t] = self.D * self.latent_states[self.t-1] + action + np.sqrt(self.Q) * np.random.randn(self.ntrials)\n",
    "      measurement = self.C * self.latent_states[self.t] + np.sqrt(self.R) * np.random.randn(self.ntrials)\n",
    "    return measurement\n",
    "\n",
    "  def get_control_gain_infinite(self, rho):\n",
    "    P = np.zeros(T)\n",
    "    L = np.zeros(T - 1)\n",
    "    P[-1] = 1\n",
    "\n",
    "    for t in range(T - 1):\n",
    "        P_t_1 = P[T - t - 1]\n",
    "        P[T - t-2] = (1 + P_t_1 * self.D**2 - self.D * P_t_1 / (rho + P_t_1) * P_t_1 * self.D)\n",
    "        L[T - t-2] = - (1 / (rho + P_t_1)* P_t_1 * self.D)\n",
    "\n",
    "    return L[0]\n",
    "\n",
    "\n",
    "def control_policy_LQG(control_gain, estimated_state):\n",
    "  current_action =  control_gain * estimated_state\n",
    "  return current_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Use interactive demo below and observe the performance of the Kalman filter. Remember, the parameter `C` scales the observation matrix.\n",
    "\n",
    "What happens when `C=0`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(C = widgets.FloatSlider(1., description=\"C\", min=0., max=3.),\n",
    "                  proc_noise = widgets.FloatSlider(.1, description=\"proc_noise\", min=0.0, max=1.),\n",
    "                  meas_noise = widgets.FloatSlider(.2, description=\"meas_noise\", min=0.1, max=1.))\n",
    "\n",
    "def simulate_kf_no_control(C, proc_noise, meas_noise):\n",
    "\n",
    "  D = 0.9\n",
    "  ini_state_mean = 5.\n",
    "  ini_state_cov = .1\n",
    "  estimates = np.zeros(T)\n",
    "  estimates[0] = ini_state_mean\n",
    "\n",
    "  filter = KalmanFilter(transition_matrix=D,\n",
    "                        transition_covariance=proc_noise,\n",
    "                        observation_matrix=C,\n",
    "                        observation_covariance=meas_noise,\n",
    "                        initial_state_mean=ini_state_mean,\n",
    "                        initial_state_covariance=ini_state_cov)\n",
    "\n",
    "  system = LQG(transition_matrix=D,\n",
    "               transition_covariance=proc_noise,\n",
    "               observation_matrix=C,\n",
    "               observation_covariance=meas_noise,\n",
    "               initial_state_mean=ini_state_mean,\n",
    "               initial_state_covariance=ini_state_cov,\n",
    "               static_noise=False)\n",
    "\n",
    "  action = 0\n",
    "  for t in range(1, T):\n",
    "    measurement = system.step(action)\n",
    "    estimates[t] = filter.get_estimate(measurement, action)\n",
    "\n",
    "  plot_kf_state_vs_time(system.latent_states, estimates, 'State estimation with KF (Stable system without control input)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "You should have seen that the Kalman filter generally estimates the latent state\n",
    "accurately, even with fairly high noise levels, except when C=0.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 4.2: LQG controller output with varying control gains\n",
    "\n",
    "Now let's implement the Kalman filter with closed-loop feedback with the controller. We will first use an arbitrary control gain and a fixed value for measurement noise. We will then use the control gain that we calculated for the LQR system given different values for $\\rho$ (weight on the control effort).\n",
    "\n",
    "1. Visualize the system dynamics $s_t$ in closed-loop control with an arbitrary constant control gain. Vary this control gain.\n",
    "\n",
    "2. Play arround with the remaining sliders. What happens when the process noise is high (low)? How about the measurement noise?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(C = widgets.FloatSlider(1., description=\"C\", min=0., max=3.),\n",
    "                  L = widgets.FloatSlider(-.3, description=\"L\", min=-.5, max=0.),\n",
    "                  proc_noise = widgets.FloatSlider(.1, description=\"proc_noise\", min=0.0, max=1.),\n",
    "                  meas_noise = widgets.FloatSlider(.2, description=\"meas_noise\", min=0.1, max=1.))\n",
    "\n",
    "\n",
    "def simulate_kf_with_control(C, L, proc_noise, meas_noise):\n",
    "\n",
    "  D = 1.1\n",
    "  ini_state_mean = 5.\n",
    "  ini_state_cov = .1\n",
    "  estimates = np.zeros(T)\n",
    "  estimates[0] = ini_state_mean\n",
    "  control_gain = L\n",
    "\n",
    "  filter = KalmanFilter(transition_matrix=D,\n",
    "                        transition_covariance=proc_noise,\n",
    "                        observation_matrix=C,\n",
    "                        observation_covariance=meas_noise,\n",
    "                        initial_state_mean=ini_state_mean,\n",
    "                        initial_state_covariance=ini_state_cov)\n",
    "\n",
    "  system = LQG(transition_matrix=D,\n",
    "               transition_covariance=proc_noise,\n",
    "               observation_matrix=C,\n",
    "               observation_covariance=meas_noise,\n",
    "               initial_state_mean=ini_state_mean,\n",
    "               initial_state_covariance=ini_state_cov,\n",
    "               static_noise=True)\n",
    "\n",
    "  action = 0\n",
    "  for t in range(1, T):\n",
    "    measurement = system.step(action)\n",
    "    estimates[t] = filter.get_estimate(measurement, action)\n",
    "    action = control_policy_LQG(control_gain, estimates[t])\n",
    "\n",
    "  plot_kf_state_vs_time(system.latent_states, estimates, f'State estimation with KF (control gain = {control_gain})', goal = np.zeros(T))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 4.3: LQG controller with varying weight on the control effort costs\n",
    "\n",
    "Now let's see the performance of the LQG controller as the parameter $\\rho$ changes. We will use an LQG controller gain, where the control gain is from a system with an infinite horizon; in this case, the optimal control gain turns out to be a constant.\n",
    "\n",
    "Vary the value of $\\rho$ from $0$ to large values, to see the effect on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(rho = widgets.FloatSlider(25., description=\"rho\", min=0., max=50.))\n",
    "\n",
    "def simulate_kf_with_lqg(rho):\n",
    "\n",
    "    D=1.1\n",
    "    C=1.\n",
    "    ini_state_mean = 1.\n",
    "    ini_state_cov = 2.\n",
    "    proc_noise=0.1\n",
    "    meas_noise=0.2\n",
    "    estimates = np.zeros(T)\n",
    "    estimates[0] = ini_state_mean\n",
    "\n",
    "\n",
    "    filter = KalmanFilter(transition_matrix=D,\n",
    "                        transition_covariance=proc_noise,\n",
    "                        observation_matrix=C,\n",
    "                        observation_covariance=meas_noise,\n",
    "                        initial_state_mean=ini_state_mean,\n",
    "                        initial_state_covariance=ini_state_cov)\n",
    "\n",
    "    system = LQG(transition_matrix=D,\n",
    "                transition_covariance=proc_noise,\n",
    "                observation_matrix=C,\n",
    "                observation_covariance=meas_noise,\n",
    "                initial_state_mean=ini_state_mean,\n",
    "                initial_state_covariance=ini_state_cov,\n",
    "                static_noise=True)\n",
    "\n",
    "    control_gain = system.get_control_gain_infinite(rho)\n",
    "\n",
    "    action = 0\n",
    "    for t in range(1, T):\n",
    "      measurement = system.step(action)\n",
    "      estimates[t] = filter.get_estimate(measurement, action)\n",
    "      action = control_policy_LQG(control_gain, estimates[t])\n",
    "\n",
    "    plot_kf_state_vs_time(system.latent_states, estimates, f'State estimation with KF (LQG controller)', goal = np.zeros(T))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 4.4: How does the process noise and the measurement noise influence the controlled state and desired action?\n",
    "\n",
    "Process noise $w_t$ (proc_noise) and measurement noise $v_t$ (meas_noise) have very different effects on the controlled state. \n",
    "\n",
    "To visualize this, play with the sliders to get an intuition for how to process noise and measurement noise influences the controlled state. How are these two sources of noise different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "@widgets.interact(proc_noise = widgets.FloatSlider(.1, description=\"proc_noise\", min=0.1, max=1.),\n",
    "                  meas_noise = widgets.FloatSlider(.2, description=\"meas_noise\", min=0.1, max=1.))\n",
    "\n",
    "def lqg_slider(proc_noise, meas_noise):\n",
    "\n",
    "    D=1.1\n",
    "    C=1.\n",
    "    rho=1.\n",
    "    ini_state_mean = 1.\n",
    "    ini_state_cov = 2.\n",
    "    estimates = np.zeros(T)\n",
    "    estimates[0] = ini_state_mean\n",
    "\n",
    "    filter = KalmanFilter(transition_matrix=D,\n",
    "                        transition_covariance=proc_noise,\n",
    "                        observation_matrix=C,\n",
    "                        observation_covariance=meas_noise,\n",
    "                        initial_state_mean=ini_state_mean,\n",
    "                        initial_state_covariance=ini_state_cov)\n",
    "\n",
    "    system = LQG(transition_matrix=D,\n",
    "                transition_covariance=proc_noise,\n",
    "                observation_matrix=C,\n",
    "                observation_covariance=meas_noise,\n",
    "                initial_state_mean=ini_state_mean,\n",
    "                initial_state_covariance=ini_state_cov,\n",
    "                static_noise=True)\n",
    "\n",
    "    control_gain = system.get_control_gain_infinite(rho)\n",
    "\n",
    "    action = 0\n",
    "    for t in range(1, T):\n",
    "      measurement = system.step(action)\n",
    "      estimates[t] = filter.get_estimate(measurement, action)\n",
    "      action = control_policy_LQG(control_gain, estimates[t])\n",
    "\n",
    "    plot_kf_state_vs_time(system.latent_states, estimates, f'State estimation with KF (LQG controller)', goal = np.zeros(T))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "As you increase the process noise, you will notice that it becomes more\n",
    "difficult to keep the state close to the goal g=0, even though we may have very\n",
    "little measurement noise (thus can estimate the state exactly).\n",
    "\n",
    "On the other hand, as you increase the measurement noise, you will notice that\n",
    "it is harder to estimate the states, and this also may make it harder to keep the\n",
    "state close to the goal.\n",
    "\n",
    "Which has a larger effect? How does this affect the required action a[t]?\n",
    "We will quantify these in the next section.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.2 Noise effects on the LQG\n",
    "\n",
    "Finally, we will quantify how the state cost and control costs change when we change the process and measurement noise levels. To do so, we will run many simulations, stepping through levels of process and measurement noise, tracking MSE and cost of control for each. \n",
    "\n",
    "Observe the effects of increasing the process and measurement noises in an unstable system. How do you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to to quantify the dependence of state and control (Running many simulations takes time, you need to wait ~2 seconds before observing the consequences of changing the parameter D)\n",
    "\n",
    "display(HTML('''<style>.widget-label { min-width: 15ex !important; }</style>'''))\n",
    "\n",
    "D=1.1                           # transition matrix\n",
    "C = 1                           # observation matrix\n",
    "ini_state = 5                   # initial state mean\n",
    "ini_state_cov = 1               # initial state covariance\n",
    "rho = 1                         # control effort parameter\n",
    "n_iter = 50\n",
    "n_ops = 20\n",
    "process_noise_var = .1\n",
    "measurement_noise_var = .2\n",
    "\n",
    "# Implement LQG control over n_iter iterations, and record the MSE between state and goal\n",
    "MSE_array_N_meas = []\n",
    "MSE_array_N_proc = []\n",
    "Jcontrol_array_N_meas = []\n",
    "Jcontrol_array_N_proc = []\n",
    "\n",
    "meas_noise_array = np.linspace(0.1, 3, n_ops)\n",
    "proc_noise_array = np.linspace(0.1, 3, n_ops)\n",
    "\n",
    "# Try several proc noises, but same measurement var\n",
    "MSE_array_proc = []\n",
    "Jcontrol_array_proc = []\n",
    "\n",
    "for proc_noise in proc_noise_array:\n",
    "  transition_covariance = proc_noise\n",
    "  observation_covariance = measurement_noise_var\n",
    "\n",
    "  #Â Controller\n",
    "  lqg = LQG(D, transition_covariance, C, observation_covariance, ini_state, ini_state_cov, n_iter)\n",
    "  control_gain_lqg = lqg.get_control_gain_infinite(rho)\n",
    "\n",
    "  # Filtering\n",
    "  filter = KalmanFilter(D, transition_covariance, C, observation_covariance, ini_state, ini_state_cov)\n",
    "\n",
    "  filtered_state_means_impl = np.zeros([T, n_iter])\n",
    "  filtered_state_covariances_impl = np.zeros([T, n_iter])\n",
    "  measurement = np.zeros([T, n_iter])\n",
    "  action_cost = np.zeros([T, n_iter])\n",
    "\n",
    "  action = np.zeros(n_iter)\n",
    "  for t in range(1, T):\n",
    "    measurement[t] = lqg.step(action)\n",
    "\n",
    "    filter.get_estimate(measurement[t], action)\n",
    "\n",
    "    filtered_state_means_impl[t] = filter.prior.mean\n",
    "    filtered_state_covariances_impl = filter.prior.cov\n",
    "\n",
    "    action = control_gain_lqg * filter.prior.mean\n",
    "    action_cost[t] = rho * action**2\n",
    "\n",
    "  state_cost = lqg.latent_states**2\n",
    "\n",
    "  MSE_array_proc.append(np.cumsum(state_cost))\n",
    "  Jcontrol_array_proc.append(np.cumsum(action_cost))\n",
    "\n",
    "MSE_array_proc = np.array(MSE_array_proc)\n",
    "Jcontrol_array_proc = np.array(Jcontrol_array_proc)\n",
    "\n",
    "# Try several measurement noises, but same proc var\n",
    "MSE_array_meas = []\n",
    "Jcontrol_array_meas = []\n",
    "\n",
    "for meas_noise in meas_noise_array:\n",
    "  transition_covariance = process_noise_var\n",
    "  observation_covariance = meas_noise\n",
    "\n",
    "  #Â Controller\n",
    "  lqg = LQG(D, transition_covariance, C, observation_covariance, ini_state, ini_state_cov, n_iter)\n",
    "  control_gain_lqg = lqg.get_control_gain_infinite(rho)\n",
    "\n",
    "  # Filtering\n",
    "  filter = KalmanFilter(D, transition_covariance, C, observation_covariance, ini_state, ini_state_cov)\n",
    "\n",
    "  filtered_state_means_impl = np.zeros([T, n_iter])\n",
    "  filtered_state_covariances_impl = np.zeros([T, n_iter])\n",
    "  measurement = np.zeros([T, n_iter])\n",
    "  action_cost = np.zeros([T, n_iter])\n",
    "\n",
    "  action = np.zeros(n_iter)\n",
    "  for t in range(1, T):\n",
    "    measurement[t] = lqg.step(action)\n",
    "\n",
    "    filter.get_estimate(measurement[t], action)\n",
    "    filtered_state_means_impl[t] = filter.prior.mean\n",
    "    filtered_state_covariances_impl = filter.prior.cov\n",
    "\n",
    "    action = control_gain_lqg * filter.prior.mean\n",
    "    action_cost[t] = rho * action**2\n",
    "\n",
    "  state_cost = lqg.latent_states**2\n",
    "\n",
    "  MSE_array_meas.append(np.cumsum(state_cost))\n",
    "  Jcontrol_array_meas.append(np.cumsum(action_cost))\n",
    "\n",
    "MSE_array_meas = np.array(MSE_array_meas)\n",
    "Jcontrol_array_meas = np.array(Jcontrol_array_meas)\n",
    "\n",
    "# Compute statistics\n",
    "MSE_array_proc_mean = np.mean(np.array(MSE_array_proc), axis = 1)\n",
    "MSE_array_proc_std = np.std(np.array(MSE_array_proc), axis = 1)\n",
    "MSE_array_meas_mean = np.mean(np.array(MSE_array_meas), axis = 1)\n",
    "MSE_array_meas_std = np.std(np.array(MSE_array_meas), axis = 1)\n",
    "\n",
    "Jcontrol_array_proc_mean = np.mean(np.array(Jcontrol_array_proc), axis = 1)\n",
    "Jcontrol_array_proc_std = np.std(np.array(Jcontrol_array_proc), axis = 1)\n",
    "Jcontrol_array_meas_mean = np.mean(np.array(Jcontrol_array_meas), axis = 1)\n",
    "Jcontrol_array_meas_std = np.std(np.array(Jcontrol_array_meas), axis = 1)\n",
    "\n",
    "# Visualize the quantification\n",
    "f, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "axs[0,0].plot(proc_noise_array, MSE_array_proc_mean, 'r-')\n",
    "axs[0,0].fill_between(proc_noise_array, MSE_array_proc_mean+MSE_array_proc_std, MSE_array_proc_mean-MSE_array_proc_std, facecolor='tab:gray', alpha=0.5)\n",
    "axs[0,0].set_title('Effect of process noise')\n",
    "axs[0,0].set_ylabel('State Cost (MSE between state and goal)')\n",
    "\n",
    "axs[0,1].plot(meas_noise_array, MSE_array_meas_mean, 'r-')\n",
    "axs[0,1].fill_between(meas_noise_array, MSE_array_meas_mean+MSE_array_meas_std, MSE_array_meas_mean-MSE_array_meas_std, facecolor='tab:gray', alpha=0.5)\n",
    "axs[0,1].set_title('Effect of measurement noise')\n",
    "\n",
    "axs[1,0].plot(proc_noise_array, Jcontrol_array_proc_mean, 'r-')\n",
    "axs[1,0].fill_between(proc_noise_array, Jcontrol_array_proc_mean+Jcontrol_array_proc_std, Jcontrol_array_proc_mean-Jcontrol_array_proc_std, facecolor='tab:gray', alpha=0.5)\n",
    "axs[1,0].set_xlabel('Process Noise')\n",
    "axs[1,0].set_ylabel('Cost of Control')\n",
    "\n",
    "axs[1,1].plot(meas_noise_array, Jcontrol_array_meas_mean, 'r-')\n",
    "axs[1,1].fill_between(meas_noise_array, Jcontrol_array_meas_mean+Jcontrol_array_meas_std, Jcontrol_array_meas_mean-Jcontrol_array_meas_std, facecolor='tab:gray', alpha=0.5)\n",
    "axs[1,1].set_xlabel('Measurement Noise')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "While both sources of noise have an effect on the controlled state, the\n",
    "process noise has a much larger effect. As the process noise w[t] increases,\n",
    "state cost (MSE between state and goal) and  control cost increase drastically.\n",
    "You can get an intuition as to why using the sliders in the demo above.  To make\n",
    "matters worse, as the process noise gets larger, you will also need to put in\n",
    "more effort to keep the system close to the goal.\n",
    "\n",
    "The measurement noise v[t]  also has an effect on the accuracy of the\n",
    "controlled state. As this noise increases, the MSE between the state and goal\n",
    "increases. The cost of control in this case remains fairly constant with\n",
    "increasing levels of measurement noise.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have extended the idea of optimal policy to the Astrocat example. You have learned about how to design an optimal controller with a full observation of the state (linear quadratic regulator - LQR) and under partial observability of the state (linear quadratic gaussian - LQG)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "epMkHVHL7uo3"
   ],
   "include_colab_link": true,
   "name": "W3D3_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
