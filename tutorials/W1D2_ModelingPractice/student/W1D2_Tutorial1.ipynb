{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D2_ModelingPractice/student/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 1, Day 2, Tutorial 1\n",
    "# Modeling Practice: Framing the question\n",
    "\n",
    "__Content creators:__ Marius 't Hart, Paul Schrater, Gunnar Blohm\n",
    "\n",
    "__Content reviewers:__ Norma Kuhn, Saeed Salehi, Madineh Sarvestani, Spiros Chavlis, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Tutorial objectives\n",
    "Yesterday you gained some understanding of what models can buy us in neuroscience. But how do you build a model? Today, we will try to clarify the process of computational modeling, by building a simple model.\n",
    "\n",
    "We will investigate a simple phenomenon, working through the 10 steps of modeling ([Blohm et al., 2019](https://doi.org/10.1523/ENEURO.0352-19.2019)) in two notebooks: \n",
    "\n",
    "**Framing the question**\n",
    "\n",
    "1. finding a phenomenon and a question to ask about it\n",
    "2. understanding the state of the art\n",
    "3. determining the basic ingredients\n",
    "4. formulating specific, mathematically defined hypotheses\n",
    "\n",
    "**Implementing the model**\n",
    "\n",
    "5. selecting the toolkit\n",
    "6. planning the model\n",
    "7. implementing the model\n",
    "\n",
    "**Model testing**\n",
    "\n",
    "8. completing the model\n",
    "9. testing and evaluating the model\n",
    "\n",
    "**Publishing**\n",
    "\n",
    "10. publishing models\n",
    "\n",
    "Tutorial 1 (this notebook) will cover the steps 1-5, while Tutorial 2 will cover the steps 6-10.\n",
    "\n",
    "**TD**: All activities you should perform are labeled with **TD#.#**, which stands for \"To Do\", micro-tutorial number, activity number. They can be found in the Table of Content on the left side of the notebook. Make sure you complete all within a section before moving on!\n",
    "\n",
    "**Run**: Some code chunks' names start with \"Run to ... (do something)\". These chunks are purely to produce a graph or calculate a number. You do not need to look at or understand the code in those chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def my_moving_window(x, window=3, FUN=np.mean):\n",
    "  \"\"\"\n",
    "  Calculates a moving estimate for a signal\n",
    "\n",
    "  Args:\n",
    "      x (numpy.ndarray): a vector array of size N\n",
    "      window (int): size of the window, must be a positive integer\n",
    "      FUN (function): the function to apply to the samples in the window\n",
    "\n",
    "  Returns:\n",
    "      (numpy.ndarray): a vector array of size N, containing the moving\n",
    "      average of x, calculated with a window of size window\n",
    "\n",
    "  There are smarter and faster solutions (e.g. using convolution) but this\n",
    "  function shows what the output really means. This function skips NaNs, and\n",
    "  should not be susceptible to edge effects: it will simply use\n",
    "  all the  available samples, which means that close to the edges of the\n",
    "  signal or close to NaNs, the output will just be based on fewer samples. By\n",
    "  default, this function will apply a mean to the samples in the window, but\n",
    "  this can be changed to be a max/min/median or other function that returns a\n",
    "  single numeric value based on a sequence of values.\n",
    "  \"\"\"\n",
    "\n",
    "  # if data is a matrix, apply filter to each row:\n",
    "  if len(x.shape) == 2:\n",
    "    output = np.zeros(x.shape)\n",
    "    for rown in range(x.shape[0]):\n",
    "      output[rown, :] = my_moving_window(x[rown, :],\n",
    "                                         window=window,\n",
    "                                         FUN=FUN)\n",
    "    return output\n",
    "\n",
    "  # make output array of the same size as x:\n",
    "  output = np.zeros(x.size)\n",
    "\n",
    "  # loop through the signal in x\n",
    "  for samp_i in range(x.size):\n",
    "\n",
    "    values = []\n",
    "\n",
    "    # loop through the window:\n",
    "    for wind_i in range(int(1 - window), 1):\n",
    "\n",
    "      if ((samp_i + wind_i) < 0) or (samp_i + wind_i) > (x.size - 1):\n",
    "        # out of range\n",
    "        continue\n",
    "\n",
    "      # sample is in range and not nan, use it:\n",
    "      if not(np.isnan(x[samp_i + wind_i])):\n",
    "        values += [x[samp_i + wind_i]]\n",
    "\n",
    "    # calculate the mean in the window for this point in the output:\n",
    "    output[samp_i] = FUN(values)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def my_plot_percepts(datasets=None, plotconditions=False):\n",
    "\n",
    "  if isinstance(datasets, dict):\n",
    "    # try to plot the datasets\n",
    "    # they should be named...\n",
    "    # 'expectations', 'judgments', 'predictions'\n",
    "\n",
    "    plt.figure(figsize=(8, 8))  # set aspect ratio = 1? not really\n",
    "\n",
    "    plt.ylabel('perceived self motion [m/s]')\n",
    "    plt.xlabel('perceived world motion [m/s]')\n",
    "    plt.title('perceived velocities')\n",
    "\n",
    "    # loop through the entries in datasets\n",
    "    # plot them in the appropriate way\n",
    "    for k in datasets.keys():\n",
    "      if k == 'expectations':\n",
    "\n",
    "        expect = datasets[k]\n",
    "        plt.scatter(expect['world'], expect['self'], marker='*',\n",
    "                    color='xkcd:green', label='my expectations')\n",
    "\n",
    "      elif k == 'judgments':\n",
    "\n",
    "        judgments = datasets[k]\n",
    "\n",
    "        for condition in np.unique(judgments[:, 0]):\n",
    "          c_idx = np.where(judgments[:, 0] == condition)[0]\n",
    "          cond_self_motion = judgments[c_idx[0], 1]\n",
    "          cond_world_motion = judgments[c_idx[0], 2]\n",
    "          if cond_world_motion == -1 and cond_self_motion == 0:\n",
    "            c_label = 'world-motion condition judgments'\n",
    "          elif cond_world_motion == 0 and cond_self_motion == 1:\n",
    "            c_label = 'self-motion condition judgments'\n",
    "          else:\n",
    "            c_label = f\"condition {condition:d} judgments\"\n",
    "\n",
    "          plt.scatter(judgments[c_idx, 3], judgments[c_idx, 4],\n",
    "                      label=c_label, alpha=0.2)\n",
    "\n",
    "      elif k == 'predictions':\n",
    "\n",
    "        predictions = datasets[k]\n",
    "\n",
    "        for condition in np.unique(predictions[:, 0]):\n",
    "          c_idx = np.where(predictions[:, 0] == condition)[0]\n",
    "          cond_self_motion = predictions[c_idx[0], 1]\n",
    "          cond_world_motion = predictions[c_idx[0], 2]\n",
    "          if cond_world_motion == -1 and cond_self_motion == 0:\n",
    "            c_label = 'predicted world-motion condition'\n",
    "          elif cond_world_motion == 0 and cond_self_motion == 1:\n",
    "            c_label = 'predicted self-motion condition'\n",
    "          else:\n",
    "            c_label = f\"condition {condition:d} prediction\"\n",
    "\n",
    "          plt.scatter(predictions[c_idx, 4], predictions[c_idx, 3],\n",
    "                      marker='x', label=c_label)\n",
    "\n",
    "      else:\n",
    "        print(\"datasets keys should be 'hypothesis',\\\n",
    "        'judgments' and 'predictions'\")\n",
    "\n",
    "    if plotconditions:\n",
    "      # this code is simplified but only works for the dataset we have:\n",
    "      plt.scatter([1], [0], marker='<', facecolor='none',\n",
    "                  edgecolor='xkcd:black', linewidths=2,\n",
    "                  label='world-motion stimulus', s=80)\n",
    "      plt.scatter([0], [1], marker='>', facecolor='none',\n",
    "                  edgecolor='xkcd:black', linewidths=2,\n",
    "                  label='self-motion stimulus', s=80)\n",
    "\n",
    "    plt.legend(facecolor='xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "  else:\n",
    "    if datasets is not None:\n",
    "      print('datasets argument should be a dict')\n",
    "      raise TypeError\n",
    "\n",
    "\n",
    "def my_plot_stimuli(t, a, v):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(t, a, label='acceleration [$m/s^2$]')\n",
    "  plt.plot(t, v, label='velocity [$m/s$]')\n",
    "  plt.xlabel('time [s]')\n",
    "  plt.ylabel('[motion]')\n",
    "  plt.legend(facecolor='xkcd:white')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def my_plot_motion_signals():\n",
    "  dt = 1 / 10\n",
    "  a = gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
    "  t = np.arange(0, 10, dt)\n",
    "  v = np.cumsum(a * dt)\n",
    "\n",
    "  fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, sharex='col',\n",
    "                                 sharey='row', figsize=(14, 6))\n",
    "  fig.suptitle('Sensory ground truth')\n",
    "\n",
    "  ax1.set_title('world-motion condition')\n",
    "  ax1.plot(t, -v, label='visual [$m/s$]')\n",
    "  ax1.plot(t, np.zeros(a.size), label='vestibular [$m/s^2$]')\n",
    "  ax1.set_xlabel('time [s]')\n",
    "  ax1.set_ylabel('motion')\n",
    "  ax1.legend(facecolor='xkcd:white')\n",
    "\n",
    "  ax2.set_title('self-motion condition')\n",
    "  ax2.plot(t, -v, label='visual [$m/s$]')\n",
    "  ax2.plot(t, a, label='vestibular [$m/s^2$]')\n",
    "  ax2.set_xlabel('time [s]')\n",
    "  ax2.set_ylabel('motion')\n",
    "  ax2.legend(facecolor='xkcd:white')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def my_plot_sensorysignals(judgments, opticflow, vestibular, returnaxes=False,\n",
    "                           addaverages=False, integrateVestibular=False,\n",
    "                           addGroundTruth=False):\n",
    "\n",
    "  if addGroundTruth:\n",
    "    dt = 1 / 10\n",
    "    a = gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
    "    t = np.arange(0, 10, dt)\n",
    "    v = a\n",
    "\n",
    "  wm_idx = np.where(judgments[:, 0] == 0)\n",
    "  sm_idx = np.where(judgments[:, 0] == 1)\n",
    "\n",
    "  opticflow = opticflow.transpose()\n",
    "  wm_opticflow = np.squeeze(opticflow[:, wm_idx])\n",
    "  sm_opticflow = np.squeeze(opticflow[:, sm_idx])\n",
    "\n",
    "  if integrateVestibular:\n",
    "    vestibular = np.cumsum(vestibular * .1, axis=1)\n",
    "    if addGroundTruth:\n",
    "      v = np.cumsum(a * dt)\n",
    "\n",
    "  vestibular = vestibular.transpose()\n",
    "  wm_vestibular = np.squeeze(vestibular[:, wm_idx])\n",
    "  sm_vestibular = np.squeeze(vestibular[:, sm_idx])\n",
    "\n",
    "  X = np.arange(0, 10, .1)\n",
    "\n",
    "  fig, my_axes = plt.subplots(nrows=2, ncols=2, sharex='col', sharey='row',\n",
    "                              figsize=(15, 10))\n",
    "  fig.suptitle('Sensory signals')\n",
    "\n",
    "  my_axes[0][0].plot(X, wm_opticflow, color='xkcd:light red', alpha=0.1)\n",
    "  my_axes[0][0].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "  if addGroundTruth:\n",
    "      my_axes[0][0].plot(t, -v, color='xkcd:red')\n",
    "  if addaverages:\n",
    "      my_axes[0][0].plot(X, np.average(wm_opticflow, axis=1),\n",
    "                         color='xkcd:red', alpha=1)\n",
    "\n",
    "  my_axes[0][0].set_title('optic-flow in world-motion condition')\n",
    "  my_axes[0][0].set_ylabel('velocity signal [$m/s$]')\n",
    "\n",
    "  my_axes[0][1].plot(X, sm_opticflow, color='xkcd:azure', alpha=0.1)\n",
    "  my_axes[0][1].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "  if addGroundTruth:\n",
    "      my_axes[0][1].plot(t, -v, color='xkcd:blue')\n",
    "  if addaverages:\n",
    "      my_axes[0][1].plot(X, np.average(sm_opticflow, axis=1),\n",
    "                         color='xkcd:blue', alpha=1)\n",
    "\n",
    "  my_axes[0][1].set_title('optic-flow in self-motion condition')\n",
    "\n",
    "  my_axes[1][0].plot(X, wm_vestibular, color='xkcd:light red', alpha=0.1)\n",
    "  my_axes[1][0].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "  if addaverages:\n",
    "      my_axes[1][0].plot(X, np.average(wm_vestibular, axis=1),\n",
    "                         color='xkcd: red', alpha=1)\n",
    "\n",
    "  my_axes[1][0].set_title('vestibular signal in world-motion condition')\n",
    "  if addGroundTruth:\n",
    "      my_axes[1][0].plot(t, np.zeros(100), color='xkcd:red')\n",
    "  my_axes[1][0].set_xlabel('time [s]')\n",
    "  if integrateVestibular:\n",
    "      my_axes[1][0].set_ylabel('velocity signal [$m/s$]')\n",
    "  else:\n",
    "      my_axes[1][0].set_ylabel('acceleration signal [$m/s^2$]')\n",
    "\n",
    "  my_axes[1][1].plot(X, sm_vestibular, color='xkcd:azure', alpha=0.1)\n",
    "  my_axes[1][1].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "  if addGroundTruth:\n",
    "      my_axes[1][1].plot(t, v, color='xkcd:blue')\n",
    "  if addaverages:\n",
    "      my_axes[1][1].plot(X, np.average(sm_vestibular, axis=1),\n",
    "                         color='xkcd:blue', alpha=1)\n",
    "\n",
    "  my_axes[1][1].set_title('vestibular signal in self-motion condition')\n",
    "  my_axes[1][1].set_xlabel('time [s]')\n",
    "\n",
    "  if returnaxes:\n",
    "    return my_axes\n",
    "  else:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def my_threshold_solution(selfmotion_vel_est, threshold):\n",
    "  is_move = (selfmotion_vel_est > threshold)\n",
    "  return is_move\n",
    "\n",
    "\n",
    "def my_moving_threshold(selfmotion_vel_est, thresholds):\n",
    "\n",
    "  pselfmove_nomove = np.empty(thresholds.shape)\n",
    "  pselfmove_move = np.empty(thresholds.shape)\n",
    "  prop_correct = np.empty(thresholds.shape)\n",
    "  pselfmove_nomove[:] = np.NaN\n",
    "  pselfmove_move[:] = np.NaN\n",
    "  prop_correct[:] = np.NaN\n",
    "\n",
    "  for thr_i, threshold in enumerate(thresholds):\n",
    "\n",
    "    # run my_threshold that the students will write:\n",
    "    try:\n",
    "      is_move = my_threshold(selfmotion_vel_est, threshold)\n",
    "    except Exception:\n",
    "      is_move = my_threshold_solution(selfmotion_vel_est, threshold)\n",
    "\n",
    "    # store results:\n",
    "    pselfmove_nomove[thr_i] = np.mean(is_move[0:100])\n",
    "    pselfmove_move[thr_i] = np.mean(is_move[100:200])\n",
    "\n",
    "    # calculate the proportion\n",
    "    # classified correctly: (1 - pselfmove_nomove) + ()\n",
    "    # Correct rejections:\n",
    "    p_CR = (1 - pselfmove_nomove[thr_i])\n",
    "    # correct detections:\n",
    "    p_D = pselfmove_move[thr_i]\n",
    "\n",
    "    # this is corrected for proportion of trials in each condition:\n",
    "    prop_correct[thr_i] = (p_CR + p_D) / 2\n",
    "\n",
    "  return [pselfmove_nomove, pselfmove_move, prop_correct]\n",
    "\n",
    "\n",
    "def my_plot_thresholds(thresholds, world_prop, self_prop, prop_correct):\n",
    "\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.title('threshold effects')\n",
    "  plt.plot([min(thresholds), max(thresholds)], [0, 0], ':',\n",
    "           color='xkcd:black')\n",
    "  plt.plot([min(thresholds), max(thresholds)], [0.5, 0.5], ':',\n",
    "           color='xkcd:black')\n",
    "  plt.plot([min(thresholds), max(thresholds)], [1, 1], ':',\n",
    "           color='xkcd:black')\n",
    "  plt.plot(thresholds, world_prop, label='world motion condition')\n",
    "  plt.plot(thresholds, self_prop, label='self motion condition')\n",
    "  plt.plot(thresholds, prop_correct, color='xkcd:purple',\n",
    "           label='correct classification')\n",
    "  idx = np.argmax(prop_correct[::-1]) + 1\n",
    "  plt.plot([thresholds[-idx]]*2, [0, 1], '--', color='xkcd:purple',\n",
    "           label='best classification')\n",
    "  plt.text(0.7, 0.8,\n",
    "           f\"threshold:{thresholds[-idx]:0.2f}\\\n",
    "           \\ncorrect: {prop_correct[-idx]:0.2f}\")\n",
    "\n",
    "  plt.xlabel('threshold')\n",
    "  plt.ylabel('proportion correct or classified as self motion')\n",
    "  plt.legend(facecolor='xkcd:white')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def my_plot_predictions_data(judgments, predictions):\n",
    "\n",
    "  # conditions = np.concatenate((np.abs(judgments[:, 1]),\n",
    "  #                             np.abs(judgments[:, 2])))\n",
    "  # veljudgmnt = np.concatenate((judgments[:, 3], judgments[:, 4]))\n",
    "  # velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
    "\n",
    "  # self:\n",
    "  # conditions_self = np.abs(judgments[:, 1])\n",
    "  veljudgmnt_self = judgments[:, 3]\n",
    "  velpredict_self = predictions[:, 3]\n",
    "\n",
    "  # world:\n",
    "  # conditions_world = np.abs(judgments[:, 2])\n",
    "  veljudgmnt_world = judgments[:, 4]\n",
    "  velpredict_world = predictions[:, 4]\n",
    "\n",
    "  fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, sharey='row',\n",
    "                                 figsize=(12, 5))\n",
    "\n",
    "  ax1.scatter(veljudgmnt_self, velpredict_self, alpha=0.2)\n",
    "  ax1.plot([0, 1], [0, 1], ':', color='xkcd:black')\n",
    "  ax1.set_title('self-motion judgments')\n",
    "  ax1.set_xlabel('observed')\n",
    "  ax1.set_ylabel('predicted')\n",
    "\n",
    "  ax2.scatter(veljudgmnt_world, velpredict_world, alpha=0.2)\n",
    "  ax2.plot([0, 1], [0, 1], ':', color='xkcd:black')\n",
    "  ax2.set_title('world-motion judgments')\n",
    "  ax2.set_xlabel('observed')\n",
    "  ax2.set_ylabel('predicted')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "import os\n",
    "fname=\"W1D2_data.npz\"\n",
    "if not os.path.exists(fname):\n",
    "  !wget https://osf.io/c5xyf/download -O $fname\n",
    "\n",
    "filez = np.load(file=fname, allow_pickle=True)\n",
    "judgments = filez['judgments']\n",
    "opticflow = filez['opticflow']\n",
    "vestibular = filez['vestibular']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 1: Investigating the phenomenon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "bcd42ada-13f8-422b-8cba-904c56ff39cb"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Question\n",
    "video = YouTubeVideo(id='x4b2-hZoyiY', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal**: formulate a good question!\n",
    "\n",
    "**Background: The train illusion**\n",
    "\n",
    "In the video you have learnt about the train illusion. In the same situation, we sometimes perceive our own train to be moving and sometimes the other train. How come our perception is ambiguous?\n",
    "\n",
    "We will build a simple model with the goal _to learn about the process of model building_ (i.e.: not to explain train illusions or get a correct model). To keep this manageable, we use a _simulated_ data set. For the same reason, this tutorial contains both coding and thinking activities. Doing both are essential for success.\n",
    "\n",
    "Imagine we get data from an experimentalist who collected _judgments_ on self motion and world motion, in two conditions. One where there was only world motion, and one where there was only self motion. In either case, the velocity increased from 0 to 1 m/s across 10 seconds with the same (fairly low) acceleration. Each of these conditions was recorded 100 times:\n",
    "\n",
    "![illustration of the conditions](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig01.png)\n",
    "\n",
    "Participants sit very still during the trials and at the end of each 10 s trial they are given two sliders, one to indicate the self-motion velocity (in m/s) and another to indicate the world-motion velocity (in m/s) _at the end of the interval_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 1.1: Form expectations about the experiment, using the phenomena\n",
    "\n",
    "In the experiment we get the participants _judgments_ of the velocities they experienced. In the Python chunk below, you should retain the numbers that represent your expectations on the participants' judgments. Remember that in the train illusion people usually experience either self motion or world motion, but not both. \n",
    "\n",
    "From the lists, remove those pairs of responses you think are unlikely to be the participants' judgments. The first two pairs of coordinates (1 m/s, 0 m/s, and 0 m/s, 1 m/s) are the stimuli, so those reflect judgments without illusion. Those should stay, but how do you think participants judge velocities when they _do_ experience the illusion?\n",
    "\n",
    "**Create Expectations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "outputId": "58734d99-e019-4b19-e32b-0100f903ad9d"
   },
   "outputs": [],
   "source": [
    "# Create Expectations\n",
    "###################################################################\n",
    "# To complete the exercise, remove unlikely responses from the two\n",
    "# lists. The lists act as X and Y coordinates for a scatter plot,\n",
    "# so make sure the lists match in length.\n",
    "###################################################################\n",
    "\n",
    "world_vel_exp = [1, 0, 1, 0.5, 0.5, 0]\n",
    "self_vel_exp = [0, 1, 1, 0.5, 0, 0.5]\n",
    "\n",
    "# The code below creates a figure with your predictions:\n",
    "my_plot_percepts(datasets={'expectations': {'world': world_vel_exp,\n",
    "                                            'self': self_vel_exp}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## **TD 1.2**: Compare Expectations to Data\n",
    "The behavioral data from our experiment is in a 200 x 5 matrix called `judgments`, where each row indicates a trial.\n",
    "\n",
    "The first three columns in the `judgments` matrix represent the conditions in the experiment, and the last two columns list the velocity judgments.\n",
    "\n",
    "![illustration of the judgments matrix](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig02.png)\n",
    "\n",
    "The condition number can be 0 (world-motion condition, first 100 rows) or 1 (self-motion condition, last 100 rows). Columns 1 and 2 respectively list the true self- and world-motion velocities in the experiment. You will not have to use the first three columns.\n",
    "\n",
    "The motion judgements (columns 3 and 4) are the participants judgments of the self-motion velocity and world-motion velocity respectively, and should show the illusion. Let's plot the judgment data, along with the true motion of the stimuli in the experiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "outputId": "424e555d-e8ab-43c9-8092-14d8ee7e16e4"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Run to plot perceptual judgments\n",
    "\n",
    "my_plot_percepts(datasets={'judgments': judgments}, plotconditions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 1.3: Think about what the data is saying, by answering these questions:\n",
    "* How does it differ from your initial expectations? \n",
    "* Where are the clusters of data, roughly?\n",
    "* What does it mean that the some of the judgments from the world-motion condition are close to the self-motion stimulus and vice versa?\n",
    "* Why are there no data points in the middle?\n",
    "* What aspects of the data require explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 2: Understanding background \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "12d4a23c-711b-4a89-848a-7988ce926df5"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Background\n",
    "video = YouTubeVideo(id='DcJ91h5Ekis', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Goal:** Now that we have an interesting phenomenon, we gather background information which will refine our questions, and we lay the groundwork for developing scientific hypotheses. \n",
    "\n",
    "**Background: Motion Sensing**: \n",
    "\n",
    "Our self-motion percepts are based on our visual (optic flow) and vestibular (inner ear) sensing. Optic flow is the moving image on the retina caused by either self or world-motion. Vestibular signals are related to bodily self- movements only. The two signals can be understood as velocity in $m/s$ (optic flow) and acceleration in $m/s^2$ (vestibular signal). We'll first look at the ground truth which is stimulating the senses in our experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "outputId": "57d62b30-fb2f-4c4a-9935-36c003d904b8"
   },
   "outputs": [],
   "source": [
    "#@markdown **Run to plot motion stimuli**\n",
    "my_plot_motion_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 2.1: Examine the differences between the conditions:\n",
    "\n",
    "* how are the visual inputs (optic flow) different between the conditions?\n",
    "* how are the vestibular signals different between the conditions?\n",
    "* how might the brain use these signals to determine there is self motion?\n",
    "* how might the brain use these signals to determine there is world motion?\n",
    "\n",
    "We can see that, in theory, we have enough information to disambiguate self-motion from world-motion using these signals. Let's go over the logic together. The visual signal is ambiguous, it will be non-zero when there is either self-motion or world-motion. The vestibular signal is specific, itâ€™s only non-zero when there is self-motion. Combining these two signals should allow us to disambiguate the self-motion condition from the world-motion condition!\n",
    "\n",
    "* In the world-motion condition: The brain can simply compare the visual and vestibular signals. If there is visual motion AND NO vestibular motion, it must be that the world is moving but not the body/self = world-motion judgement.\n",
    "* In the self-motion condition: We can make a similar comparison. If there is both visual signals AND vestibular signals, it must be that the body/self is moving = self-motion judgement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Background: Integrating signals**: \n",
    "\n",
    "To understand how the vestibular _acceleration_ signal could underlie the perception of self-motion _velocity_, we assume the brain integrates the signal. This also allows comparing the vestibular signal to the visual signal, by getting them in the same units. Read more about integration on [Wikipedia](https://en.wikipedia.org/wiki/Integral).\n",
    "\n",
    "Below we will approximate the integral using `np.cumsum()`. The discrete integral would be:\n",
    "\n",
    "$$v_t = \\sum_{k=0}^t a_k\\cdot\\Delta t  + v_0$$\n",
    "\n",
    "* $a(t)$ is acceleration as a function of time\n",
    "* $v(t)$ is velocity as a function of time\n",
    "* $\\Delta t$ is equal to the sample interval of our recorded visual and vestibular signals (0.1 s).\n",
    "* $v_0$ is the _constant of integration_ which corresponds in the initial velocity at time $0$ (it would have to be known or remembered). Since that is always 0 in our experiment, we will leave it out from here on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Numerically Integrating a signal\n",
    "\n",
    "Below is a chunk of code which uses the `np.cumsum()` function to integrate the acceleration that was used in our (simulated) experiment: `a` over `dt` in order to get a velocity signal `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "outputId": "0cb7fe22-ae21-4083-b235-b9bf7837c0c5"
   },
   "outputs": [],
   "source": [
    "# Check out the code:\n",
    "dt = 1 / 10\n",
    "a = gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
    "t = np.arange(0, 10, dt)\n",
    "\n",
    "# This does the integration of acceleration into velocity:\n",
    "v = np.cumsum(a * dt)\n",
    "\n",
    "my_plot_stimuli(t, a, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Background: Sensory signals are noisy** \n",
    "\n",
    "In our experiment, we also recorded sensory signals in the participant. The data come in two 200 x 100 matrices:\n",
    "\n",
    "`opticflow` (with the visual signals)\n",
    "\n",
    "and\n",
    "\n",
    "`vestibular` (with the vestibular signals)\n",
    "\n",
    "In each of the signal matrices _rows_ (200) represent **trials**, in the same order as in the `judgments` matrix. _Columns_ (100) are **time samples**, representing 10 s collected with a 100 ms time bin. \n",
    "\n",
    "![illustration of the signal matrices](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig03.png)\n",
    "\n",
    "Here we plot the data representing our 'sensory signals':\n",
    "\n",
    "* plot optic flow signals for self-motion vs world-motion conditions (should be the same)\n",
    "* plot vestibular signals for self-motion vs world-motion conditions (should be different)\n",
    "\n",
    "The x-axis is time in seconds, but the y-axis can be one of several, depending on what you do with the signals: $m/s^2$ (acceleration) or $m/s$ (velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "colab_type": "code",
    "outputId": "e76bb3bd-c3dd-451b-e98a-c672b79de790"
   },
   "outputs": [],
   "source": [
    "#@markdown **Run to plot raw noisy sensory signals**\n",
    "# signals as they are:\n",
    "my_plot_sensorysignals(judgments, opticflow, vestibular,\n",
    "                       integrateVestibular=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 2.2: Understanding the problem of noisy sensory information\n",
    "\n",
    "**Answer the following questions:**\n",
    " * Is this what you expected?\n",
    " * In which of the two signals should we be able to see a difference between the conditions?\n",
    " * Can we use the data as it is to differentiate between the conditions? \n",
    " * Can we compare the the visual and vestibular motion signals when they're in different units?\n",
    " * What would the brain do differentiate the two conditions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now that we know how to integrate the vestibular signal to get it into the same unit as the optic flow, we can see if it shows the pattern it should: a flat line in the world-motion condition and the correct velocity profile in the self-motion condition. Run the chunk of Python below to plot the sensory data again, but now with the vestibular signal integrated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "colab_type": "code",
    "outputId": "55fe877f-1941-4b5a-b7c8-e352b1c7dc66"
   },
   "outputs": [],
   "source": [
    "#@markdown **Run to compare true signals to sensory data**\n",
    "my_plot_sensorysignals(judgments, opticflow, vestibular,\n",
    "                       integrateVestibular=True, returnaxes=False,\n",
    "                       addaverages=False, addGroundTruth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The thick lines are the ground truth: the actual velocities in each of the conditions. With some effort, we can make out that _on average_ the vestibular signal does show the expected pattern after all. But there is also a lot of noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Background  Summary**:  \n",
    "\n",
    "Now that we have examined the sensory signals, and understand how they relate to the ground truth. We see that there is enough information to _in principle_ disambiguate true self-motion from true world motion (there should be no illusion!). However, because the sensory information contains a lot of noise, i.e. it is unreliable, it could result in ambiguity.\n",
    "\n",
    "**_It is time to refine our research question:_**\n",
    "\n",
    "* Does the self-motion illusion occur due to unreliable sensory information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 3: Identifying ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "0ccedc12-ec5f-4776-b2e8-3554ab2c18c8"
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Ingredients\n",
    "video = YouTubeVideo(id='ZQRtysK4OCo', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 3.1: Understand the moving average function\n",
    "**Goal**: think about what ingredients we will need for our model\n",
    "\n",
    "We have access to sensory signals from the visual and vestibular systems that are used to estimate world motion and self motion.\n",
    "\n",
    "However, there are still two issues:\n",
    "\n",
    "1. _While sensory input can be noisy or unstable, perception is much more stable._\n",
    "2. _In the judgments there is either self motion or not._\n",
    "\n",
    "We will solve this by by using:\n",
    "\n",
    "1. _a moving average filter_ to stabilize our sensory signals\n",
    "2. _a threshold function_ to distinguish moving from non-moving\n",
    "\n",
    "\n",
    "One of the simplest models of noise reduction is a moving average (sometimes: moving mean or rolling mean) over the recent past. In a discrete signal we specify the number of samples to use for the average (including the current one), and this is often called the _window size_. For more information on the moving average, check [this Wikipedia page](https://en.wikipedia.org/wiki/Moving_average).\n",
    "\n",
    "In this tutorial there is a simple running average function available:\n",
    "\n",
    "`my_moving_window(s, w)`: takes a signal time series $s$ and a window size $w$ as input and returns the moving average for all samples in the signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "### Interactive Demo: Averaging window\n",
    "\n",
    "The code below picks one vestibular signal, integrates it to get a velocity estimate for self motion, and then filters. You can set the window size.\n",
    "Try different window sizes, then answer the following:\n",
    "\n",
    "* What is the maximum window size? The minimum?\n",
    "* Why does increasing the window size shift the curves? \n",
    "* How do the filtered estimates differ from the true signal?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494,
     "referenced_widgets": [
      "d4bf289a4e6048949c7eaae67f4734e7",
      "950e6f28168d426bbf69568ac18f6b34",
      "64fc28aab3674266b551bc075e262d16",
      "46c8eb92f15d4d4b8f21e1584d74ea54",
      "bd520ff695c04df3915cde97c93cf063",
      "b5ea876d1d6247ca8599fb89d24bf061",
      "5d37d283830e469084c074cc9ea78f28",
      "c1a0db22655c4b66b56ab7d445a27c7a",
      "e03c2eea6362471c8d8df9df8897ea7e",
      "ff4020b41fe14ecab1c9afebefb819ff"
     ]
    },
    "colab_type": "code",
    "outputId": "1242237e-958e-49f8-d4f6-555c480bc73b"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "t = np.arange(0, 10, .1)\n",
    "\n",
    "\n",
    "def refresh(trial_number=101, window=15):\n",
    "  # get the trial signal:\n",
    "  signal = vestibular[trial_number - 1, :]\n",
    "  # integrate it:\n",
    "  signal = np.cumsum(signal * .1)\n",
    "  # plot this signal\n",
    "  plt.plot(t, signal, label='integrated vestibular signal')\n",
    "  # filter:\n",
    "  signal = my_moving_window(signal, window=window, FUN=np.mean)\n",
    "  # plot filtered signal\n",
    "  plt.plot(t, signal, label=f'filtered with window: {window}')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, trial_number=(1, 200, 1), window=(5, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "_Note: the function `my_moving_window()` is defined in this notebook in the code block at at the top called \"Helper functions\". It should be the first function there, so feel free to check how it works._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 3.2: Thresholding the self-motion vestibular signal\n",
    "\n",
    "Comparing the integrated, filtered (accumulated) vestibular signals with a threshold should allow determining if there is self motion or not.\n",
    "\n",
    "To try this, we:\n",
    "1. Integrate the vestibular signal, apply a moving average filter, and take the last value of each trial's vestibular signal as an estimate of self-motion velocity. \n",
    "2. Transfer the estimates of self-motion velocity into binary (0,1) decisions by comparing them to a threshold. Remember the output of logical comparators (>=<) are logical (truth/1, false/0). 1 indicates we think there was self-motion and 0 indicates otherwise. YOUR CODE HERE.\n",
    "3. We sort these decisions separately for conditions of real world-motion vs. real self-motion to determine 'classification' accuracy.\n",
    "4. To understand how the threshold impacts classfication accuracy, we do 1-3  for a range of thresholds.\n",
    "\n",
    "There is one line fo code to complete, which will implement step 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Exercise 1: Threshold self-motion velocity into binary classifiction of self-motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def my_threshold(selfmotion_vel_est, threshold):\n",
    "  \"\"\"\n",
    "  This function should calculate proportion self motion\n",
    "  for both conditions and the overall proportion\n",
    "  correct classifications.\n",
    "\n",
    "  Args:\n",
    "      selfmotion_vel_est (numpy.ndarray): A sequence of floats\n",
    "      indicating the estimated self motion for all trials.\n",
    "\n",
    "      threshold (float): A threshold for the estimate of self motion when\n",
    "      the brain decides there really is self motion.\n",
    "\n",
    "  Returns:\n",
    "      (numpy.ndarray): self-motion: yes or no.\n",
    "  \"\"\"\n",
    "\n",
    "  ##############################################################\n",
    "  # Compare the self motion estimates to the threshold:\n",
    "  # Replace '...' with the proper code:\n",
    "  # Remove the next line to test your function\n",
    "  raise NotImplementedError(\"Modify my_threshold function\")\n",
    "  ##############################################################\n",
    "\n",
    "  # Compare the self motion estimates to the threshold\n",
    "  is_move = ...\n",
    "\n",
    "  return is_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D2_ModelingPractice/solutions/W1D2_Tutorial1_Solution_d278e3f8.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Interactive Demo: Threshold vs. averaging window\n",
    "\n",
    "Now we combine the classification steps 1-3 above, for a variable threshold. This will allow us to find the threshold that produces the most accurate classification of self-motion.\n",
    "We also add a 'widget' that controls the size of the moving average window. How does the optimal threshold vary with window size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606,
     "referenced_widgets": [
      "e22f208d5e604455874cbb01a6b2c8e6",
      "637b1830bc754b6aa7bc5610806e4877",
      "b7c5018a1d1c43b5a93eda6ce1ec2c58",
      "aca7f2b0d0734ace88a26dea8398fe0a",
      "e3779e060b0145f7bf4c5995004d5af9",
      "da720f1a62a74084b70c1e3ef44cb4fd",
      "ef8c24de96594594af9e38b3fa8baea8"
     ]
    },
    "colab_type": "code",
    "outputId": "1818767a-7b1c-45c5-a385-5a83cab579ed"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "thresholds = np.round_(np.arange(0, 1.01, .01), 2)\n",
    "v_ves = np.cumsum(vestibular * .1, axis=1)\n",
    "\n",
    "\n",
    "def refresh(window=50):\n",
    "\n",
    "  selfmotion_vel_est = my_moving_window(v_ves, window=window,\n",
    "                                        FUN=np.mean)[:, 99]\n",
    "\n",
    "  [pselfmove_nomove,\n",
    "    pselfmove_move,\n",
    "    pcorrect] = my_moving_threshold(selfmotion_vel_est, thresholds)\n",
    "\n",
    "  my_plot_thresholds(thresholds, pselfmove_nomove, pselfmove_move, pcorrect)\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, window=(1, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's unpack this: \n",
    "\n",
    "Ideally, in the self-motion condition (blue line) we should always detect self motion, and in never in the world-motion condition (red line). This doesn't happen, regardless of the settings we pick. However, we can pick a threshold that gives the highest proportion correctly classified trials, which depends on the window size, but is between 0.2 and 0.4. We'll pick the optimal threshold for a window size of 100 (the full signal) which is at 0.33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The ingredients we have collected for our model so far:\n",
    "* integration: get the vestibular signal in the same unit as the visual signal\n",
    "* running average: accumulate evidence over some time, so that perception is stable\n",
    "* decision if there was self motion (threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Since the velocity judgments are made at the end of the 10 second trials, it seems reasonable to use the sensory signals at the last sample to estimate what percept the participants said they had. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 4: Formulating hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "b19d04a8-6f02-4f90-e18b-6bb4053b1ba8"
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Hypotheses\n",
    "video = YouTubeVideo(id='wgOpbfUELqU', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal**: formulate reasonable hypotheses in mathematical language using the ingredients identified in step 3. Write hypotheses as a function that we evaluate the model against later.\n",
    "\n",
    "**Question:** _Why do we experience the illusion?_\n",
    "\n",
    "We know that there are two real motion signals, and that these drive two sensory signals:\n",
    "\n",
    "> $w_v$: world motion (velocity magnitude)\n",
    "> \n",
    "> $s_v$: self motion (velocity magnitude)\n",
    "> \n",
    "> $s_{visual}$: optic flow signal\n",
    "> \n",
    "> $s_{vestibular}$: vestibular signal\n",
    "\n",
    "Optic flow is ambiguous, as both world motion and self motion drive visual motion.\n",
    "\n",
    "$$s_{visual} = w_v - s_v + noise$$\n",
    "\n",
    "Notice that world motion and self motion might cancel out. For example, if the train you are on, and the train you are looking at, both move at exactly the same speed.\n",
    "\n",
    "Vestibular signals are driven only by self motion, but _can_ be ambiguous when they are noisy. \n",
    "\n",
    "$$s_{vestibular} = s_v + noise$$\n",
    "\n",
    "**Combining Relationships**\n",
    "\n",
    "Without the sensory noise, these two relations are two linear equations, with two unknowns!\n",
    "\n",
    "This suggests the brain could simply \"solve\" for $s_v$ and $w_v$.  \n",
    "\n",
    "However, given the noisy signals, sometimes these solutions will not be correct.  Perhaps that is enough to explain the illusion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 4.1: Write out Hypothesis\n",
    "\n",
    "Use the discussion and framing to write out your hypothesis in the form:\n",
    "> Illusory self-motion occurs when (give preconditions).  We hypothesize it occurs because (explain how our hypothesized relationships work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 4.2: Relate hypothesis to ingredients\n",
    "\n",
    "Now it's time to pull together the ingredients and relate them to our hypothesis.  \n",
    "\n",
    "**For each trial we have:**\n",
    "\n",
    "| variable | description |\n",
    "| ---- | ---- |\n",
    "| $\\hat{v_s}$ | **self motion judgment** (in m/s)|\n",
    "| $\\hat{v_w}$ | **world motion judgment** (in m/s)|\n",
    "|  $s_{ves}$ | **vestibular info** filtered and integrated vestibular information  |\n",
    "|  $s_{opt}$ | **optic flow info** filtered optic flow estibular information  |\n",
    "| $z_s$ | **Self-motion detection** boolean value (True/False) indicating whether the vestibular info was above threshold or not |\n",
    "\n",
    "Answer the following questions by replotting your data and ingredients: \n",
    "\n",
    "* which of the 5 variables do our hypothesis say should be related?\n",
    "* what do you expect these plots to look like?  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Run to calculate variables\n",
    "# these 5 lines calculate the main variables that we might use in the model\n",
    "s_ves = my_moving_window(np.cumsum(vestibular * .1, axis=1), window=100)[:, 99]\n",
    "s_opt = my_moving_window(opticflow, window=50)[:, 99]\n",
    "v_s = s_ves\n",
    "v_w = -s_opt - v_s\n",
    "z_s = (s_ves > 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the first chunk of code we plot histograms to compare the variability of the estimates of velocity we get from each of two sensory signals.\n",
    "\n",
    "**Plot histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "outputId": "e944d102-17aa-46fb-8cdd-0d2dd1e6c1e1"
   },
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(s_ves, label='vestibular', alpha=0.5)  # set the first argument here\n",
    "plt.hist(s_opt, label='visual', alpha=0.5)  # set the first argument here\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('velocity estimate')\n",
    "plt.legend(facecolor='xkcd:white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This matches that the vestibular signals are noisier than visual signals.\n",
    "\n",
    "Below is generic code to create scatter diagrams. Use it to see if the relationships between variables are the way you expect them. For example, what is the relationship between the estimates of self motion and world motion, as we calculate them here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Exercise 2: Build a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "outputId": "193b101b-9571-4abe-c946-ec0ba6b09200"
   },
   "outputs": [],
   "source": [
    "# this sets up a figure with some dotted lines on y=0 and x=0 for reference\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot([0, 0], [-0.5, 1.5], ':', color='xkcd:black')\n",
    "plt.plot([-0.5, 1.5], [0, 0], ':', color='xkcd:black')\n",
    "\n",
    "#############################################################################\n",
    "# uncomment below and fill in with your code\n",
    "#############################################################################\n",
    "\n",
    "# determine which variables you want to look at (variable on the abscissa / x-axis, variable on the ordinate / y-axis)\n",
    "# plt.scatter(...)\n",
    "\n",
    "plt.xlabel('world-motion velocity [m/s]')\n",
    "plt.ylabel('self-motion velocity [m/s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "text",
    "outputId": "f0d587d4-5cad-4707-fd63-74734689c45b"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D2_ModelingPractice/solutions/W1D2_Tutorial1_Solution_f533b89d.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=560 height=560 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/W1D2_Tutorial1_Solution_f533b89d_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Below is code that uses $z_s$ to split the trials in into two categories (i.e., $s_{ves}$ below or above threshold) and plot the mean in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Exercise 3: Split variable means bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Fill in source_var and uncomment\n",
    "####################################\n",
    "\n",
    "# source variable you want to check\n",
    "source_var = ...\n",
    "\n",
    "# below = np.mean(source_var[np.where(np.invert(z_s))[0]])\n",
    "# above = np.mean(source_var[np.where(z_s)[0]] )\n",
    "\n",
    "# plt.bar(x=[0, 1], height=[below, above])\n",
    "\n",
    "plt.xticks([0, 1], ['below', 'above'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W1D2_ModelingPractice/solutions/W1D2_Tutorial1_Solution_e66e09ba.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/W1D2_Tutorial1_Solution_e66e09ba_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 5: Toolkit selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Toolkit\n",
    "video = YouTubeVideo(id='rsmnayVfJyM', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Goal**: with the question, ingredients and hypotheses in mind, which toolkit (modeling approach) would be best to use?\n",
    "\n",
    "**Toolkits**\n",
    "\n",
    "The lecture covers the notion of toolkit.  Here we explain the toolkit we use in our simulation.\n",
    "\n",
    "**Simulation as a generic tool** \n",
    "\n",
    "Because our data provides per trial data of both inputs and outputs, designing a _simulation_ is a powerful method.  \n",
    "\n",
    "In general, simulation models have a typical structure:\n",
    "\n",
    "![Generic Simulation](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig04.png)\n",
    "\n",
    "\n",
    "Simulation models have:\n",
    "\n",
    "* **inputs** including input variables, _and_ the ingredients needed to build the simulation functions.\n",
    "* **simulation** which runs the model _many times_ on the inputs, over a range of scenarios (typically different parameter or model choices).  \n",
    "\n",
    " In addition, many simulations run _replications_, which are useful if components of your input or model are _stochastic_.\n",
    "* **outputs** the output variables.  these may have replicas if the simulations are repeated to account for stochastic elements, which are perfect for statistical analysis.\n",
    "\n",
    "\n",
    "_____\n",
    "The elements of a simulation model are quite generic and occur in most models.  The most important considerations in deciding a toolkit are: \n",
    "* What types of mechanisms (causes, processes, systems) will you need to consider in your model?\n",
    "* What aspects of the data and ingredients are _essential_ which are clearly _irrelevant_, and which can be abstracted over.\n",
    "\n",
    "These considerations are completely determined by your research question.  To better understand this, we will engage you in a TA-led discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 5.1: TA-guided discussion about how questions drive toolkit selection.  \n",
    "\n",
    "\n",
    "NOTES:   [See slides:](https://mfr.ca-1.osf.io/render?url=https://osf.io/v5emn/?direct%26mode=render%26action=download%26mode=render)\n",
    "\n",
    "1. **DISCUSS MECHANISMS/INGREDIENTS NEEDED FOR QUESTIONS**: what kinds of mechanisms and ingredients are needed for the discussion questions\n",
    "\n",
    "2. **DISCUSS WHAT INGREDIENTS DONT MATTER**: simulation / level of abstraction (depends on what is measured & is irrelevant to the question)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we worked through some steps of the process of modeling. \n",
    "\n",
    "- We defined a phenomenon and formulated a question (step 1)\n",
    "- We collected information the state-of-the-art about the topic (step 2)\n",
    "- We determined the basic ingredients (step 3), and used these to formulate a specific mathematically defined hypothesis (step 4), and\n",
    "- We chose the most appropriate modeling approach (i.e., toolkit) for our phenomenon/background information/hypothesis (step 5)\n",
    "\n",
    "In the next tutorial, we will continue with the steps 6-10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Reading\n",
    "\n",
    "Blohm G, Kording KP, Schrater PR (2020). _A How-to-Model Guide for Neuroscience_ eNeuro, 7(1) ENEURO.0352-19.2019. https://doi.org/10.1523/ENEURO.0352-19.2019 \n",
    "\n",
    "Dokka K, Park H, Jansen M, DeAngelis GC, Angelaki DE (2019). _Causal inference accounts for heading perception in the presence of object motion._ PNAS, 116(18):9060-9065. https://doi.org/10.1073/pnas.1820373116\n",
    "\n",
    "Drugowitsch J, DeAngelis GC, Klier EM, Angelaki DE, Pouget A (2014). _Optimal Multisensory Decision-Making in a Reaction-Time Task._ eLife, 3:e03005. https://doi.org/10.7554/eLife.03005\n",
    "\n",
    "Hartmann, M, Haller K, Moser I, Hossner E-J, Mast FW  (2014). _Direction detection thresholds of passive self-motion in artistic gymnasts._ Exp Brain Res, 232:1249â€“1258. https://doi.org/10.1007/s00221-014-3841-0\n",
    "\n",
    "Mensh B, Kording K (2017). _Ten simple rules for structuring papers._ PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619\n",
    "\n",
    "Seno T, Fukuda H (2012). _Stimulus Meanings Alter Illusory Self-Motion (Vection) - Experimental Examination of the Train Illusion._ Seeing Perceiving, 25(6):631-45. https://doi.org/10.1163/18784763-00002394\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
