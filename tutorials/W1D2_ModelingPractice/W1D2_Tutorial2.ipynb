{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D2_ModelingPractice/W1D2_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Neuromatch Academy: Week 1, Day 2, Tutorial 2\n",
    "# Modeling Practice: Model implementation and evaluation\n",
    "__Content creators:__ Marius 't Hart, Paul Schrater, Gunnar Blohm\n",
    "\n",
    "__Content reviewers:__ Norma Kuhn, Saeed Salehi, Madineh Sarvestani, Spiros Chavlis, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Tutorial objectives\n",
    "\n",
    "We are investigating a simple phenomena, working through the 10 steps of modeling ([Blohm et al., 2019](https://doi.org/10.1523/ENEURO.0352-19.2019)) in two notebooks: \n",
    "\n",
    "**Framing the question**\n",
    "\n",
    "1. finding a phenomenon and a question to ask about it\n",
    "2. understanding the state of the art\n",
    "3. determining the basic ingredients\n",
    "4. formulating specific, mathematically defined hypotheses\n",
    "\n",
    "**Implementing the model**\n",
    "\n",
    "5. selecting the toolkit\n",
    "6. planning the model\n",
    "7. implementing the model\n",
    "\n",
    "**Model testing**\n",
    "\n",
    "8. completing the model\n",
    "9. testing and evaluating the model\n",
    "\n",
    "**Publishing**\n",
    "\n",
    "10. publishing models\n",
    "\n",
    "We did steps 1-5 in Tutorial 1 and will cover steps 6-10 in Tutorial 2 (this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import gamma\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def my_moving_window(x, window=3, FUN=np.mean):\n",
    "    \"\"\"\n",
    "    Calculates a moving estimate for a signal\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): a vector array of size N\n",
    "        window (int): size of the window, must be a positive integer\n",
    "        FUN (function): the function to apply to the samples in the window\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): a vector array of size N, containing the moving\n",
    "        average of x, calculated with a window of size window\n",
    "\n",
    "    There are smarter and faster solutions (e.g. using convolution) but this\n",
    "    function shows what the output really means. This function skips NaNs, and\n",
    "    should not be susceptible to edge effects: it will simply use\n",
    "    all the  available samples, which means that close to the edges of the\n",
    "    signal or close to NaNs, the output will just be based on fewer samples. By\n",
    "    default, this function will apply a mean to the samples in the window, but\n",
    "    this can be changed to be a max/min/median or other function that returns a\n",
    "    single numeric value based on a sequence of values.\n",
    "    \"\"\"\n",
    "\n",
    "    # if data is a matrix, apply filter to each row:\n",
    "    if len(x.shape) == 2:\n",
    "        output = np.zeros(x.shape)\n",
    "        for rown in range(x.shape[0]):\n",
    "            output[rown, :] = my_moving_window(x[rown, :],\n",
    "                                               window=window, FUN=FUN)\n",
    "        return output\n",
    "\n",
    "    # make output array of the same size as x:\n",
    "    output = np.zeros(x.size)\n",
    "\n",
    "    # loop through the signal in x\n",
    "    for samp_i in range(x.size):\n",
    "\n",
    "        values = []\n",
    "\n",
    "        # loop through the window:\n",
    "        for wind_i in range(int(1 - window), 1):\n",
    "\n",
    "            if ((samp_i + wind_i) < 0) or (samp_i + wind_i) > (x.size - 1):\n",
    "                # out of range\n",
    "                continue\n",
    "\n",
    "            # sample is in range and not nan, use it:\n",
    "            if not(np.isnan(x[samp_i + wind_i])):\n",
    "                values += [x[samp_i + wind_i]]\n",
    "\n",
    "        # calculate the mean in the window for this point in the output:\n",
    "        output[samp_i] = FUN(values)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def my_plot_percepts(datasets=None, plotconditions=False):\n",
    "\n",
    "    if isinstance(datasets, dict):\n",
    "        # try to plot the datasets\n",
    "        # they should be named...\n",
    "        # 'expectations', 'judgments', 'predictions'\n",
    "\n",
    "        plt.figure(figsize=(8, 8))  # set aspect ratio = 1? not really\n",
    "\n",
    "        plt.ylabel('perceived self motion [m/s]')\n",
    "        plt.xlabel('perceived world motion [m/s]')\n",
    "        plt.title('perceived velocities')\n",
    "\n",
    "        # loop through the entries in datasets\n",
    "        # plot them in the appropriate way\n",
    "        for k in datasets.keys():\n",
    "            if k == 'expectations':\n",
    "\n",
    "                expect = datasets[k]\n",
    "                plt.scatter(expect['world'], expect['self'], marker='*',\n",
    "                            color='xkcd:green', label='my expectations')\n",
    "\n",
    "            elif k == 'judgments':\n",
    "\n",
    "                judgments = datasets[k]\n",
    "\n",
    "                for condition in np.unique(judgments[:, 0]):\n",
    "                    c_idx = np.where(judgments[:, 0] == condition)[0]\n",
    "                    cond_self_motion = judgments[c_idx[0], 1]\n",
    "                    cond_world_motion = judgments[c_idx[0], 2]\n",
    "                    if cond_world_motion == -1 and cond_self_motion == 0:\n",
    "                        c_label = 'world-motion condition judgments'\n",
    "                    elif cond_world_motion == 0 and cond_self_motion == 1:\n",
    "                        c_label = 'self-motion condition judgments'\n",
    "                    else:\n",
    "                        c_label = f\"condition [{condition:d}] judgments\"\n",
    "\n",
    "                    plt.scatter(judgments[c_idx, 3], judgments[c_idx, 4],\n",
    "                                label=c_label, alpha=0.2)\n",
    "\n",
    "            elif k == 'predictions':\n",
    "\n",
    "                predictions = datasets[k]\n",
    "\n",
    "                for condition in np.unique(predictions[:, 0]):\n",
    "                    c_idx = np.where(predictions[:, 0] == condition)[0]\n",
    "                    cond_self_motion = predictions[c_idx[0], 1]\n",
    "                    cond_world_motion = predictions[c_idx[0], 2]\n",
    "                    if cond_world_motion == -1 and cond_self_motion == 0:\n",
    "                        c_label = 'predicted world-motion condition'\n",
    "                    elif cond_world_motion == 0 and cond_self_motion == 1:\n",
    "                        c_label = 'predicted self-motion condition'\n",
    "                    else:\n",
    "                        c_label = f\"condition [{condition:d}] prediction\"\n",
    "\n",
    "                    plt.scatter(predictions[c_idx, 4], predictions[c_idx, 3],\n",
    "                                marker='x', label=c_label)\n",
    "\n",
    "            else:\n",
    "                print(\"datasets keys should be 'hypothesis', \\\n",
    "                'judgments' and 'predictions'\")\n",
    "\n",
    "        if plotconditions:\n",
    "            # this code is simplified but only works for the dataset we have:\n",
    "            plt.scatter([1], [0], marker='<', facecolor='none',\n",
    "                        edgecolor='xkcd:black', linewidths=2,\n",
    "                        label='world-motion stimulus', s=80)\n",
    "            plt.scatter([0], [1], marker='>', facecolor='none',\n",
    "                        edgecolor='xkcd:black', linewidths=2,\n",
    "                        label='self-motion stimulus', s=80)\n",
    "\n",
    "        plt.legend(facecolor='xkcd:white')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        if datasets is not None:\n",
    "            print('datasets argument should be a dict')\n",
    "            raise TypeError\n",
    "\n",
    "\n",
    "def my_plot_stimuli(t, a, v):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t, a, label='acceleration [$m/s^2$]')\n",
    "    plt.plot(t, v, label='velocity [$m/s$]')\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.ylabel('[motion]')\n",
    "    plt.legend(facecolor='xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def my_plot_motion_signals():\n",
    "    dt = 1 / 10\n",
    "    a = gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
    "    t = np.arange(0, 10, dt)\n",
    "    v = np.cumsum(a * dt)\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, sharex='col',\n",
    "                                   sharey='row', figsize=(14, 6))\n",
    "    fig.suptitle('Sensory ground truth')\n",
    "\n",
    "    ax1.set_title('world-motion condition')\n",
    "    ax1.plot(t, -v, label='visual [$m/s$]')\n",
    "    ax1.plot(t, np.zeros(a.size), label='vestibular [$m/s^2$]')\n",
    "    ax1.set_xlabel('time [s]')\n",
    "    ax1.set_ylabel('motion')\n",
    "    ax1.legend(facecolor='xkcd:white')\n",
    "\n",
    "    ax2.set_title('self-motion condition')\n",
    "    ax2.plot(t, -v, label='visual [$m/s$]')\n",
    "    ax2.plot(t, a, label='vestibular [$m/s^2$]')\n",
    "    ax2.set_xlabel('time [s]')\n",
    "    ax2.set_ylabel('motion')\n",
    "    ax2.legend(facecolor='xkcd:white')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def my_plot_sensorysignals(judgments, opticflow, vestibular, returnaxes=False,\n",
    "                           addaverages=False, integrateVestibular=False,\n",
    "                           addGroundTruth=False):\n",
    "\n",
    "    if addGroundTruth:\n",
    "        dt = 1 / 10\n",
    "        a = gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
    "        t = np.arange(0, 10, dt)\n",
    "        v = a\n",
    "\n",
    "    wm_idx = np.where(judgments[:, 0] == 0)\n",
    "    sm_idx = np.where(judgments[:, 0] == 1)\n",
    "\n",
    "    opticflow = opticflow.transpose()\n",
    "    wm_opticflow = np.squeeze(opticflow[:, wm_idx])\n",
    "    sm_opticflow = np.squeeze(opticflow[:, sm_idx])\n",
    "\n",
    "    if integrateVestibular:\n",
    "        vestibular = np.cumsum(vestibular * .1, axis=1)\n",
    "        if addGroundTruth:\n",
    "            v = np.cumsum(a * dt)\n",
    "\n",
    "    vestibular = vestibular.transpose()\n",
    "    wm_vestibular = np.squeeze(vestibular[:, wm_idx])\n",
    "    sm_vestibular = np.squeeze(vestibular[:, sm_idx])\n",
    "\n",
    "    X = np.arange(0, 10, .1)\n",
    "\n",
    "    fig, my_axes = plt.subplots(nrows=2, ncols=2, sharex='col',\n",
    "                                sharey='row', figsize=(15, 10))\n",
    "    fig.suptitle('Sensory signals')\n",
    "\n",
    "    my_axes[0][0].plot(X, wm_opticflow, color='xkcd:light red', alpha=0.1)\n",
    "    my_axes[0][0].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "    if addGroundTruth:\n",
    "        my_axes[0][0].plot(t, -v, color='xkcd:red')\n",
    "    if addaverages:\n",
    "        my_axes[0][0].plot(X, np.average(wm_opticflow, axis=1),\n",
    "                           color='xkcd:red', alpha=1)\n",
    "    my_axes[0][0].set_title('optic-flow in world-motion condition')\n",
    "    my_axes[0][0].set_ylabel('velocity signal [$m/s$]')\n",
    "\n",
    "    my_axes[0][1].plot(X, sm_opticflow, color='xkcd:azure', alpha=0.1)\n",
    "    my_axes[0][1].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "    if addGroundTruth:\n",
    "        my_axes[0][1].plot(t, -v, color='xkcd:blue')\n",
    "    if addaverages:\n",
    "        my_axes[0][1].plot(X, np.average(sm_opticflow, axis=1),\n",
    "                           color='xkcd:blue', alpha=1)\n",
    "    my_axes[0][1].set_title('optic-flow in self-motion condition')\n",
    "\n",
    "    my_axes[1][0].plot(X, wm_vestibular, color='xkcd:light red', alpha=0.1)\n",
    "    my_axes[1][0].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "    if addaverages:\n",
    "        my_axes[1][0].plot(X, np.average(wm_vestibular, axis=1),\n",
    "                           color='xkcd:red', alpha=1)\n",
    "    my_axes[1][0].set_title('vestibular signal in world-motion condition')\n",
    "    if addGroundTruth:\n",
    "        my_axes[1][0].plot(t, np.zeros(100), color='xkcd:red')\n",
    "    my_axes[1][0].set_xlabel('time [s]')\n",
    "    if integrateVestibular:\n",
    "        my_axes[1][0].set_ylabel('velocity signal [$m/s$]')\n",
    "    else:\n",
    "        my_axes[1][0].set_ylabel('acceleration signal [$m/s^2$]')\n",
    "\n",
    "    my_axes[1][1].plot(X, sm_vestibular, color='xkcd:azure', alpha=0.1)\n",
    "    my_axes[1][1].plot([0, 10], [0, 0], ':', color='xkcd:black')\n",
    "    if addGroundTruth:\n",
    "        my_axes[1][1].plot(t, v, color='xkcd:blue')\n",
    "    if addaverages:\n",
    "        my_axes[1][1].plot(X, np.average(sm_vestibular, axis=1),\n",
    "                           color='xkcd:blue', alpha=1)\n",
    "    my_axes[1][1].set_title('vestibular signal in self-motion condition')\n",
    "    my_axes[1][1].set_xlabel('time [s]')\n",
    "\n",
    "    if returnaxes:\n",
    "        return my_axes\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def my_threshold_solution(selfmotion_vel_est, threshold):\n",
    "    is_move = (selfmotion_vel_est > threshold)\n",
    "    return is_move\n",
    "\n",
    "\n",
    "def my_moving_threshold(selfmotion_vel_est, thresholds):\n",
    "\n",
    "    pselfmove_nomove = np.empty(thresholds.shape)\n",
    "    pselfmove_move = np.empty(thresholds.shape)\n",
    "    prop_correct = np.empty(thresholds.shape)\n",
    "    pselfmove_nomove[:] = np.NaN\n",
    "    pselfmove_move[:] = np.NaN\n",
    "    prop_correct[:] = np.NaN\n",
    "\n",
    "    for thr_i, threshold in enumerate(thresholds):\n",
    "\n",
    "        # run my_threshold that the students will write:\n",
    "        try:\n",
    "            is_move = my_threshold(selfmotion_vel_est, threshold)\n",
    "        except Exception:\n",
    "            is_move = my_threshold_solution(selfmotion_vel_est, threshold)\n",
    "\n",
    "        # store results:\n",
    "        pselfmove_nomove[thr_i] = np.mean(is_move[0:100])\n",
    "        pselfmove_move[thr_i] = np.mean(is_move[100:200])\n",
    "\n",
    "        # calculate the proportion classified correctly:\n",
    "        # (1-pselfmove_nomove) + ()\n",
    "        # Correct rejections:\n",
    "        p_CR = (1 - pselfmove_nomove[thr_i])\n",
    "        # correct detections:\n",
    "        p_D = pselfmove_move[thr_i]\n",
    "\n",
    "        # this is corrected for proportion of trials in each condition:\n",
    "        prop_correct[thr_i] = (p_CR + p_D) / 2\n",
    "\n",
    "    return [pselfmove_nomove, pselfmove_move, prop_correct]\n",
    "\n",
    "\n",
    "def my_plot_thresholds(thresholds, world_prop, self_prop, prop_correct):\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('threshold effects')\n",
    "    plt.plot([min(thresholds), max(thresholds)], [0, 0], ':',\n",
    "             color='xkcd:black')\n",
    "    plt.plot([min(thresholds), max(thresholds)], [0.5, 0.5], ':',\n",
    "             color='xkcd:black')\n",
    "    plt.plot([min(thresholds), max(thresholds)], [1, 1], ':',\n",
    "             color='xkcd:black')\n",
    "    plt.plot(thresholds, world_prop, label='world motion condition')\n",
    "    plt.plot(thresholds, self_prop, label='self motion condition')\n",
    "    plt.plot(thresholds, prop_correct, color='xkcd:purple',\n",
    "             label='correct classification')\n",
    "    plt.xlabel('threshold')\n",
    "    plt.ylabel('proportion correct or classified as self motion')\n",
    "    plt.legend(facecolor='xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def my_plot_predictions_data(judgments, predictions):\n",
    "\n",
    "    # conditions = np.concatenate((np.abs(judgments[:, 1]),\n",
    "    #                              np.abs(judgments[:, 2])))\n",
    "    # veljudgmnt = np.concatenate((judgments[:, 3], judgments[:, 4]))\n",
    "    # velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
    "\n",
    "    # self:\n",
    "    # conditions_self = np.abs(judgments[:, 1])\n",
    "    veljudgmnt_self = judgments[:, 3]\n",
    "    velpredict_self = predictions[:, 3]\n",
    "\n",
    "    # world:\n",
    "    # conditions_world = np.abs(judgments[:, 2])\n",
    "    veljudgmnt_world = judgments[:, 4]\n",
    "    velpredict_world = predictions[:, 4]\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, sharey='row',\n",
    "                                   figsize=(12, 5))\n",
    "\n",
    "    ax1.scatter(veljudgmnt_self, velpredict_self, alpha=0.2)\n",
    "    ax1.plot([0, 1], [0, 1], ':', color='xkcd:black')\n",
    "    ax1.set_title('self-motion judgments')\n",
    "    ax1.set_xlabel('observed')\n",
    "    ax1.set_ylabel('predicted')\n",
    "\n",
    "    ax2.scatter(veljudgmnt_world, velpredict_world, alpha=0.2)\n",
    "    ax2.plot([0, 1], [0, 1], ':', color='xkcd:black')\n",
    "    ax2.set_title('world-motion judgments')\n",
    "    ax2.set_xlabel('observed')\n",
    "    ax2.set_ylabel('predicted')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "import os\n",
    "fname=\"W1D2_data.npz\"\n",
    "if not os.path.exists(fname):\n",
    "  !wget https://osf.io/c5xyf/download -O $fname\n",
    "\n",
    "filez = np.load(file=fname, allow_pickle=True)\n",
    "judgments = filez['judgments']\n",
    "opticflow = filez['opticflow']\n",
    "vestibular = filez['vestibular']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 6: Model planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "5e31c5d8-dbb3-42ab-8e9e-08b4a9d7df87"
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Planning\n",
    "video = YouTubeVideo(id='dRTOFFigxa0', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal:** Identify the key components of the model and how they work together.\n",
    "\n",
    "Our goal all along has been to model our perceptual estimates of sensory data.\n",
    "Now that we have some idea of what we want to do, we need to line up the components of the model: what are the input and output? Which computations are done and in what order? \n",
    "\n",
    "Our model will have:\n",
    "* **inputs**: the values the system has available - this can be broken down in _data:_ the sensory signals, _parameters:_ the threshold and the window sizes for filtering\n",
    "* **outputs**: these are the predictions our model will make - for this tutorial these are the perceptual judgments on each trial in m/s, just like the judgments participants made.\n",
    "* **model functions**: A set of functions that perform the hypothesized computations.\n",
    "\n",
    "We will define a set of functions that take our data and some parameters as input, can run our model, and output a prediction for the judgment data.\n",
    "\n",
    "**Recap of what we've accomplished so far:**\n",
    "\n",
    "To model perceptual estimates from our sensory data, we need to \n",
    "1. _integrate:_ to ensure sensory information are in appropriate units\n",
    "2. _filter:_ to reduce noise and set timescale\n",
    "3. _threshold:_ to model detection\n",
    "\n",
    "This will be done with these operations:\n",
    "1. _integrate:_ `np.cumsum()`\n",
    "2. _filter:_ `my_moving_window()`\n",
    "3. _threshold:_ `if` with a comparison (`>` or `<`) and `else`\n",
    "\n",
    "**_Planning our model:_**\n",
    "\n",
    "We will now start putting all the pieces together. Normally you would sketch this yourself, but here is an overview of how the functions comprising the model are going to work:\n",
    "\n",
    "![model functions purpose](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig05.png)\n",
    "\n",
    "Below is the main function with a detailed explanation of what the function is supposed to do, exactly what input is expected, and what output will be generated. \n",
    "\n",
    "The model is not complete, so it only returns nans (**n**ot-**a**-**n**umber) for now. However, this outlines how most model code works: it gets some measured data (the sensory signals) and a set of parameters as input, and as output returns a prediction on other measured data (the velocity judgments). \n",
    "\n",
    "The goal of this function is to define the top level of a simulation model which:\n",
    "* receives all input\n",
    "* loops through the cases\n",
    "* calls functions that computes predicted values for each case\n",
    "* outputs the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Main model function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "outputId": "6d75a04d-5e34-4d82-ed20-5da912ac15c2"
   },
   "outputs": [],
   "source": [
    "def my_train_illusion_model(sensorydata, params):\n",
    "  \"\"\"\n",
    "  Generate output predictions of perceived self-motion and perceived\n",
    "  world-motion velocity based on input visual and vestibular signals.\n",
    "\n",
    "  Args:\n",
    "\n",
    "    sensorydata: (dict) dictionary with two named entries:\n",
    "        opticflow: (numpy.ndarray of float) NxM array with N trials on rows\n",
    "                   and M visual signal samples in columns\n",
    "\n",
    "        vestibular: (numpy.ndarray of float) NxM array with N trials on rows\n",
    "                    and M vestibular signal samples in columns\n",
    "\n",
    "    params: (dict) dictionary with named entries:\n",
    "        threshold: (float) vestibular threshold for credit assignment\n",
    "\n",
    "        filterwindow: (list of int) determines the strength of filtering for\n",
    "                      the visual and vestibular signals, respectively\n",
    "\n",
    "        integrate (bool): whether to integrate the vestibular signals, will\n",
    "                          be set to True if absent\n",
    "\n",
    "        FUN (function): function used in the filter, will be set to\n",
    "                        np.mean if absent\n",
    "\n",
    "        samplingrate (float): the number of samples per second in the\n",
    "                              sensory data, will be set to 10 if absent\n",
    "\n",
    "  Returns:\n",
    "\n",
    "    dict with two entries:\n",
    "\n",
    "      selfmotion: (numpy.ndarray) vector array of length N, with predictions\n",
    "                  of perceived self motion\n",
    "\n",
    "      worldmotion: (numpy.ndarray) vector array of length N, with predictions\n",
    "                   of perceived world motion\n",
    "  \"\"\"\n",
    "\n",
    "  # sanitize input a little\n",
    "  if not('FUN' in params.keys()):\n",
    "      params['FUN'] = np.mean\n",
    "  if not('integrate' in params.keys()):\n",
    "      params['integrate'] = True\n",
    "  if not('samplingrate' in params.keys()):\n",
    "      params['samplingrate'] = 10\n",
    "\n",
    "  # number of trials:\n",
    "  ntrials = sensorydata['opticflow'].shape[0]\n",
    "\n",
    "  # set up variables to collect output\n",
    "  selfmotion = np.empty(ntrials)\n",
    "  worldmotion = np.empty(ntrials)\n",
    "\n",
    "  # loop through trials?\n",
    "  for trialN in range(ntrials):\n",
    "\n",
    "      # these are our sensory variables (inputs)\n",
    "      vis = sensorydata['opticflow'][trialN, :]\n",
    "      ves = sensorydata['vestibular'][trialN, :]\n",
    "\n",
    "      # generate output predicted perception:\n",
    "      selfmotion[trialN],\\\n",
    "        worldmotion[trialN] = my_perceived_motion(vis=vis, ves=ves,\n",
    "                                                  params=params)\n",
    "\n",
    "  return {'selfmotion': selfmotion, 'worldmotion': worldmotion}\n",
    "\n",
    "\n",
    "# here is a mock version of my_perceived motion.\n",
    "# so you can test my_train_illusion_model()\n",
    "def my_perceived_motion(*args, **kwargs):\n",
    "  return [np.nan, np.nan]\n",
    "\n",
    "\n",
    "# let's look at the preditions we generated for two sample trials (0,100)\n",
    "# we should get a 1x2 vector of self-motion prediction and another\n",
    "# for world-motion\n",
    "\n",
    "sensorydata={'opticflow': opticflow[[0, 100], :0],\n",
    "             'vestibular': vestibular[[0, 100], :0]}\n",
    "params={'threshold': 0.33, 'filterwindows': [100, 50]}\n",
    "my_train_illusion_model(sensorydata=sensorydata, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We've also completed the `my_perceived_motion()` function for you below. Follow this example to complete the template for `my_selfmotion()` and `my_worldmotion()`. Write out the inputs and outputs, and the steps required to calculate the outputs from the inputs.\n",
    "\n",
    "**Perceived motion function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Full perceived motion function\n",
    "\n",
    "\n",
    "def my_perceived_motion(vis, ves, params):\n",
    "  \"\"\"\n",
    "  Takes sensory data and parameters and returns predicted percepts\n",
    "\n",
    "  Args:\n",
    "      vis (numpy.ndarray) : 1xM array of optic flow velocity data\n",
    "      ves (numpy.ndarray) : 1xM array of vestibular acceleration data\n",
    "      params              : (dict) dictionary with named entries:\n",
    "                            see my_train_illusion_model() for details\n",
    "\n",
    "  Returns:\n",
    "      [list of floats]    : prediction for perceived self-motion based on\n",
    "                            vestibular data, and prediction for perceived\n",
    "                            world-motion based on perceived self-motion and\n",
    "                            visual data\n",
    "  \"\"\"\n",
    "\n",
    "  # estimate self motion based on only the vestibular data\n",
    "  # pass on the parameters\n",
    "  selfmotion = my_selfmotion(ves=ves, params=params)\n",
    "\n",
    "  # estimate the world motion, based on the selfmotion and visual data\n",
    "  # pass on the parameters as well\n",
    "  worldmotion = my_worldmotion(vis=vis, selfmotion=selfmotion, params=params)\n",
    "\n",
    "  return [selfmotion, worldmotion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 6.1: Formulate purpose of the self motion function\n",
    "\n",
    "Now we plan out the purpose of one of the remaining functions. **Only name input arguments, write help text and comments, _no code_.** The goal of this exercise is to make writing the code (in Micro-tutorial 7) much easier. Based on our work before the break, you should now be able to answer these questions for each function:\n",
    "\n",
    "* what (sensory) data is necessary? \n",
    "* what parameters does the function need, if any?\n",
    "* which operations will be performed on the input?\n",
    "* what is the output?\n",
    "\n",
    "The number of arguments is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Template calculate self motion**\n",
    "\n",
    "Name the _input arguments_, complete the _help text_, and add _comments_ in the function below to describe the inputs, the outputs, and operations using elements from the recap at the top of this notebook (or from micro-tutorials 3 and 4 in part 1), in order to plan out the function. Do not write any code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def my_selfmotion(arg1, arg2):\n",
    "    \"\"\"\n",
    "    Short description of the function\n",
    "\n",
    "    Args:\n",
    "        argument 1: explain the format and content of the first argument\n",
    "        argument 2: explain the format and content of the second argument\n",
    "\n",
    "    Returns:\n",
    "        what output does the function generate?\n",
    "\n",
    "    Any further description?\n",
    "    \"\"\"\n",
    "\n",
    "    ##################################################\n",
    "    # what operations do we perform on the input?\n",
    "    # use the elements from micro-tutorials 3, 4, and 5\n",
    "    # 1.\n",
    "    # 2.\n",
    "    # 3.\n",
    "    # 4.\n",
    "\n",
    "    # what output should this function produce?\n",
    "    ##################################################\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "def my_selfmotion(ves, params):\n",
    "    \"\"\"\n",
    "    Estimates self motion for one vestibular signal\n",
    "\n",
    "    Args:\n",
    "        ves (numpy.ndarray): 1xM array with a vestibular signal\n",
    "        params (dict): dictionary with named entries:\n",
    "            see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float): an estimate of self motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    ##################################################\n",
    "    # 1. integrate vestibular signal\n",
    "    # 2. running window function\n",
    "    # 3. take final value\n",
    "    # 4. compare to threshold\n",
    "\n",
    "    # if higher than threshold: return value\n",
    "    # if lower than threshold: return 0\n",
    "    ##################################################\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Template calculate world motion**\n",
    "\n",
    "We have drafted the help text and written comments in the function below that describe the inputs, the outputs, and operations we use to estimate world motion, based on the recap above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# World motion function\n",
    "\n",
    "\n",
    "def my_worldmotion(vis, selfmotion, params):\n",
    "    \"\"\"\n",
    "    Estimates world motion based on the visual signal, the estimate of\n",
    "\n",
    "    Args:\n",
    "        vis (numpy.ndarray): 1xM array with the optic flow signal\n",
    "        selfmotion (float): estimate of self motion\n",
    "        params (dict): dictionary with named entries:\n",
    "            see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float): an estimate of world motion in m/s\n",
    "    \"\"\"\n",
    "    ##################################################\n",
    "    # 1. running window function\n",
    "    # 2. take final value\n",
    "    # 3. subtract selfmotion from value\n",
    "\n",
    "    # return final value\n",
    "    ##################################################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 7: Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "322900c0-f76e-4272-9fd0-66b6e26dbcda"
   },
   "outputs": [],
   "source": [
    "# @title Video 7: Implementation\n",
    "video = YouTubeVideo(id='DMSIt7t-LO8', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal:** We write the components of the model in actual code.\n",
    "\n",
    "For the operations we picked, there function ready to use:\n",
    "* integration: `np.cumsum(data, axis=1)` (axis=1: per trial and over samples)\n",
    "* filtering: `my_moving_window(data, window)` (window: int, default 3)\n",
    "* take last `selfmotion` value as our estimate\n",
    "* threshold: if (value > thr): <operation 1> else: <operation 2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 7.1: Write code to estimate self motion\n",
    "\n",
    "Use the operations to finish writing the function that will calculate an estimate of self motion. Fill in the descriptive list of items with actual operations. Use the function for estimating world-motion below, which we've filled for you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Exercise 1: finish self motion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def my_selfmotion(ves, params):\n",
    "    \"\"\"\n",
    "    Estimates self motion for one vestibular signal\n",
    "\n",
    "    Args:\n",
    "        ves (numpy.ndarray): 1xM array with a vestibular signal\n",
    "        params (dict)      : dictionary with named entries:\n",
    "                             see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float)            : an estimate of self motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    ##################################################\n",
    "    ## TODO for students: fill in ... in code below\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Student exercise: estimate my_selfmotion\")\n",
    "    ##################################################\n",
    "\n",
    "    # 1. integrate vestibular signal:\n",
    "    ves = np.cumsum(ves * (1 / params['samplingrate']))\n",
    "\n",
    "    # 2. running window function to accumulate evidence:\n",
    "    selfmotion = ...\n",
    "\n",
    "    # 3. take final value of self-motion vector as our estimate\n",
    "    selfmotion = ...\n",
    "\n",
    "    # 4. compare to threshold. Hint the threshodl is stored in\n",
    "    # params['threshold']\n",
    "    # if selfmotion is higher than threshold: return value\n",
    "    # if it's lower than threshold: return 0\n",
    "\n",
    "    if ...:\n",
    "      selfmotion = ...\n",
    "\n",
    "    return selfmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def my_selfmotion(ves, params):\n",
    "    \"\"\"\n",
    "    Estimates self motion for one vestibular signal\n",
    "\n",
    "    Args:\n",
    "        ves (numpy.ndarray): 1xM array with a vestibular signal\n",
    "        params (dict)      : dictionary with named entries:\n",
    "                             see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float)            : an estimate of self motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. integrate vestibular signal:\n",
    "    ves = np.cumsum(ves * (1 / params['samplingrate']))\n",
    "\n",
    "    # 2. running window function to accumulate evidence:\n",
    "    selfmotion = my_moving_window(ves, window=params['filterwindows'][0])\n",
    "\n",
    "    # 3. take final value of self-motion vector as our estimate\n",
    "    selfmotion = selfmotion[-1]\n",
    "\n",
    "    # 4. compare to threshold. Hint the threshodl is stored in\n",
    "    # params['threshold']\n",
    "    # if selfmotion is higher than threshold: return value\n",
    "    # if it's lower than threshold: return 0\n",
    "\n",
    "    if selfmotion < params['threshold']:\n",
    "        selfmotion = 0\n",
    "\n",
    "    return selfmotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Interactive Demo: Unit testing\n",
    "\n",
    "Testing if the functions you wrote do what they are supposed to do is important, and known as 'unit testing'. Here we will simplify this for the `my_selfmotion()` function, by allowing varying the threshold and window size with a slider, and seeing what the distribution of self-motion estimates looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494,
     "referenced_widgets": [
      "c9ec91d228024471a0eea443c9f8424c",
      "0184c697952542cfbc6f49b9e4850314",
      "d45a739ee7c442eeaa85d84e50e794be",
      "3e06aed75813468081186eb16c1e1dd8",
      "d05ef16f706f429fae2492377f37a790",
      "43a3edc8d565448cbe21de2409f05b4d",
      "04b8249a6c0b4520b6ae75ab3e3e330d",
      "e0ef96e74a0c45229e6289d3311fa507",
      "975db5ba0e154e979fe833c2434b72e0",
      "fac47f8ab2e449efbe7a6653142f64bc"
     ]
    },
    "colab_type": "code",
    "outputId": "44b30e73-8b86-4f89-da58-124ed73ecfd5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "def refresh(threshold=0, windowsize=100):\n",
    "\n",
    "    params = {'samplingrate': 10, 'FUN': np.mean}\n",
    "    params['filterwindows'] = [windowsize, 50]\n",
    "    params['threshold'] = threshold\n",
    "\n",
    "    selfmotion_estimates = np.empty(200)\n",
    "\n",
    "    # get the estimates for each trial:\n",
    "    for trial_number in range(200):\n",
    "        ves = vestibular[trial_number, :]\n",
    "        selfmotion_estimates[trial_number] = my_selfmotion(ves, params)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(selfmotion_estimates, bins=20)\n",
    "    plt.xlabel('self-motion estimate')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, threshold=(-1, 2, .01), windowsize=(1, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Estimate world motion**\n",
    "\n",
    "We have completed the `my_worldmotion()` function for you below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# World motion function\n",
    "def my_worldmotion(vis, selfmotion, params):\n",
    "    \"\"\"\n",
    "    Short description of the function\n",
    "\n",
    "    Args:\n",
    "        vis (numpy.ndarray): 1xM array with the optic flow signal\n",
    "        selfmotion (float): estimate of self motion\n",
    "        params (dict): dictionary with named entries:\n",
    "            see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float): an estimate of world motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    # running average to smooth/accumulate sensory evidence\n",
    "    visualmotion = my_moving_window(vis, window=params['filterwindows'][1],\n",
    "                                    FUN=np.mean)\n",
    "\n",
    "    # take final value\n",
    "    visualmotion = visualmotion[-1]\n",
    "\n",
    "    # subtract selfmotion from value\n",
    "    worldmotion = visualmotion + selfmotion\n",
    "\n",
    "    # return final value\n",
    "    return worldmotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 8: Model completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "14d71c79-c3eb-46b7-b87a-4df3310669c4"
   },
   "outputs": [],
   "source": [
    "# @title Video 8: Completion\n",
    "video = YouTubeVideo(id='EM-G8YYdrDg', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal:** Make sure the model can speak to the hypothesis. Eliminate all the parameters that do not speak to the hypothesis.\n",
    "\n",
    "Now that we have a working model, we can keep improving it, but at some point we need to decide that it is finished. Once we have a model that displays the properties of a system we are interested in, it should be possible to say something about our hypothesis and question. Keeping the model simple makes it easier to understand the phenomenon and answer the research question. Here that means that our model should have illusory perception, and perhaps make similar judgments to those of the participants, but not much more.\n",
    "\n",
    "To test this, we will run the model, store the output and plot the models' perceived self motion over perceived world motion, like we did with the actual perceptual judgments (it even uses the same plotting function).\n",
    "\n",
    "## TD 8.1: See if the model produces illusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "outputId": "48907f74-f6aa-4f69-d9b0-5e479a5a5797"
   },
   "outputs": [],
   "source": [
    "# @markdown Run to plot model predictions of motion estimates\n",
    "# prepare to run the model again:\n",
    "data = {'opticflow': opticflow, 'vestibular': vestibular}\n",
    "params = {'threshold': 0.6, 'filterwindows': [100, 50], 'FUN': np.mean}\n",
    "modelpredictions = my_train_illusion_model(sensorydata=data, params=params)\n",
    "\n",
    "# process the data to allow plotting...\n",
    "predictions = np.zeros(judgments.shape)\n",
    "predictions[:, 0:3] = judgments[:, 0:3]\n",
    "predictions[:, 3] = modelpredictions['selfmotion']\n",
    "predictions[:, 4] = modelpredictions['worldmotion'] * -1\n",
    "my_plot_percepts(datasets={'predictions': predictions}, plotconditions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "* How does the distribution of data points compare to the plot in TD 1.2 or in TD 7.1?\n",
    "* Did you expect to see this?\n",
    "* Where do the model's predicted judgments for each of the two conditions fall?\n",
    "* How does this compare to the behavioral data?\n",
    "\n",
    "However, the main observation should be that **there are illusions**: the blue and red data points are mixed in each of the two clusters of data points. This mean the model can help us understand the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 9: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "c9b877df-6024-467e-dc40-0be3d6f2fc4d"
   },
   "outputs": [],
   "source": [
    "# @title Video 9: Evaluation\n",
    "video = YouTubeVideo(id='bWLFyobm4Rk', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal:** Once we have finished the model, we need a description of how good it is. The question and goals we set in micro-tutorial 1 and 4 help here. There are multiple ways to evaluate a model. Aside from the obvious fact that we want to get insight into the phenomenon that is not directly accessible without the model, we always want to quantify how well the model agrees with the data.\n",
    "\n",
    "**Quantify model quality with $R^2$**\n",
    "\n",
    "Let's look at how well our model matches the actual judgment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "outputId": "128b7bad-dba1-4a54-8014-498e88b9e2bd"
   },
   "outputs": [],
   "source": [
    "# @markdown Run to plot predictions over data\n",
    "my_plot_predictions_data(judgments, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "When model predictions are correct, the red points in the figure above should lie along the identity line (a dotted black line here). Points off the identity line represent model prediction errors. While in each plot we see two clusters of dots that are fairly close to the identity line, there are also two clusters that are not. For the trials that those points represent, the model has an illusion while the participants don't or vice versa.\n",
    "\n",
    "We will use a straightforward, quantitative measure of how good the model is: $R^2$ (pronounced: \"R-squared\"), which can take values between 0 and 1, and expresses how much variance is explained by the relationship between two variables (here the model's predictions and the actual judgments). It is also called [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), and is calculated here as the square of the correlation coefficient (r or $\\rho$). Just run the chunk below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "outputId": "f9e63c44-03fa-4105-af1a-456151bb2e38"
   },
   "outputs": [],
   "source": [
    "# @markdown Run to calculate R^2\n",
    "conditions = np.concatenate((np.abs(judgments[:, 1]), np.abs(judgments[:, 2])))\n",
    "veljudgmnt = np.concatenate((judgments[:, 3], judgments[:, 4]))\n",
    "velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
    "\n",
    "slope, intercept, r_value,\\\n",
    "  p_value, std_err = stats.linregress(conditions, veljudgmnt)\n",
    "print(f\"conditions -> judgments R^2: {r_value ** 2:0.3f}\")\n",
    "\n",
    "slope, intercept, r_value,\\\n",
    "  p_value, std_err = stats.linregress(veljudgmnt, velpredict)\n",
    "print(f\"predictions -> judgments R^2: {r_value ** 2:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "These $R^2$s express how well the experimental conditions explain the participants judgments and how well the models predicted judgments explain the participants judgments.\n",
    "\n",
    "You will learn much more about model fitting, quantitative model evaluation and model comparison tomorrow!\n",
    "\n",
    "Perhaps the $R^2$ values don't seem very impressive, but the judgments produced by the participants are explained by the model's predictions better than by the actual conditions. In other words: in a certain percentage of cases the model tends to have the same illusions as the participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 9.1 Varying the threshold parameter to improve the model\n",
    "\n",
    "In the code below, see if you can find a better value for the threshold parameter, to reduce errors in the models' predictions.\n",
    "\n",
    "**Testing thresholds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "### Interactive Demo: optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440,
     "referenced_widgets": [
      "3c14de7b128445bea89761710719eda4",
      "1f3a9bd779c64323b33fd74907723cb1",
      "abf45616594a491cbe93657020611e99",
      "9b399040c1bf419eb0e5a8e2c2c23ec0",
      "d01824eecc5d4b16940ffb0fbc0b1277",
      "08373764d1b345c6b590313cff9b8ad5",
      "d5c33092cbec41fb9b05074a4d246090",
      "28ea68dab9c54f8eae1a7e5854cf86eb",
      "750f646481294afc8578554b5a983aad",
      "0759da942603446fb5f09df8bcbe0526"
     ]
    },
    "colab_type": "code",
    "outputId": "352cf63f-ff0a-46f8-fce2-a9becfd658a9"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "data = {'opticflow': opticflow, 'vestibular': vestibular}\n",
    "\n",
    "\n",
    "def refresh(threshold=0, windowsize=100):\n",
    "\n",
    "    # set parameters according to sliders:\n",
    "    params = {'samplingrate': 10, 'FUN': np.mean}\n",
    "    params['filterwindows'] = [windowsize, 50]\n",
    "    params['threshold'] = threshold\n",
    "\n",
    "    modelpredictions = my_train_illusion_model(sensorydata=data, params=params)\n",
    "\n",
    "    predictions = np.zeros(judgments.shape)\n",
    "    predictions[:, 0:3] = judgments[:, 0:3]\n",
    "    predictions[:, 3] = modelpredictions['selfmotion']\n",
    "    predictions[:, 4] = modelpredictions['worldmotion'] * -1\n",
    "\n",
    "    # plot the predictions:\n",
    "    my_plot_predictions_data(judgments, predictions)\n",
    "\n",
    "    # calculate R2\n",
    "    veljudgmnt = np.concatenate((judgments[:, 3], judgments[:, 4]))\n",
    "    velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
    "    slope, intercept, r_value,\\\n",
    "      p_value, std_err = stats.linregress(veljudgmnt, velpredict)\n",
    "\n",
    "    print(f\"predictions -> judgments R^2: {r_value ** 2:0.3f}\")\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, threshold=(-1, 2, .01), windowsize=(1, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Varying the parameters this way, allows you to increase the models' performance in predicting the actual data as measured by $R^2$. This is called model fitting, and will be done better in the coming weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 9.2: Credit assigmnent of self motion\n",
    "\n",
    "When we look at the figure in **TD 8.1**, we can see a cluster does seem very close to (1,0), just like in the actual data. The cluster of points at (1,0) are from the case where we conclude there is no self motion, and then set the self motion to 0. That value of 0 removes a lot of noise from the world-motion estimates, and all noise from the self-motion estimate. In the other case, where there is self motion, we still have a lot of noise (see also micro-tutorial 4).\n",
    "\n",
    "Let's change our `my_selfmotion()` function to return a self motion of 1 when the vestibular signal indicates we are above threshold, and 0 when we are below threshold. Edit the function here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Exercise 2: function for credit assigment of self motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def my_selfmotion(ves, params):\n",
    "    \"\"\"\n",
    "    Estimates self motion for one vestibular signal\n",
    "\n",
    "    Args:\n",
    "        ves (numpy.ndarray): 1xM array with a vestibular signal\n",
    "        params (dict): dictionary with named entries:\n",
    "            see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float): an estimate of self motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    # integrate signal:\n",
    "    ves = np.cumsum(ves * (1 / params['samplingrate']))\n",
    "\n",
    "    # use running window to accumulate evidence:\n",
    "    selfmotion = my_moving_window(ves, window=params['filterwindows'][0],\n",
    "                                  FUN=params['FUN'])\n",
    "\n",
    "    # take the final value as our estimate:\n",
    "    selfmotion = selfmotion[-1]\n",
    "\n",
    "    ###########################################################################\n",
    "    # Exercise: Complete credit assignment. Remove the next line to test your function\n",
    "    raise NotImplementedError(\"Modify with credit assignment\")\n",
    "    ###########################################################################\n",
    "\n",
    "    # compare to threshold, set to 0 if lower\n",
    "    if selfmotion < params['threshold']:\n",
    "        selfmotion = 0\n",
    "    else:\n",
    "        selfmotion = ...\n",
    "\n",
    "    return selfmotion\n",
    "\n",
    "# Use the updated function to run the model and plot the data\n",
    "# Uncomment below to test your function\n",
    "data = {'opticflow': opticflow, 'vestibular': vestibular}\n",
    "params = {'threshold': 0.33, 'filterwindows': [100, 50], 'FUN': np.mean}\n",
    "# modelpredictions = my_train_illusion_model(sensorydata=data, params=params)\n",
    "\n",
    "predictions = np.zeros(judgments.shape)\n",
    "predictions[:, 0:3] = judgments[:, 0:3]\n",
    "predictions[:, 3] = modelpredictions['selfmotion']\n",
    "predictions[:, 4] = modelpredictions['worldmotion'] * -1\n",
    "# my_plot_percepts(datasets={'predictions': predictions}, plotconditions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "outputId": "8b2fd1fd-d015-45b6-ed89-d72a9602af2d"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def my_selfmotion(ves, params):\n",
    "    \"\"\"\n",
    "    Estimates self motion for one vestibular signal\n",
    "\n",
    "    Args:\n",
    "        ves (numpy.ndarray): 1xM array with a vestibular signal\n",
    "        params (dict): dictionary with named entries:\n",
    "            see my_train_illusion_model() for details\n",
    "\n",
    "    Returns:\n",
    "        (float): an estimate of self motion in m/s\n",
    "    \"\"\"\n",
    "\n",
    "    # integrate signal:\n",
    "    ves = np.cumsum(ves * (1 / params['samplingrate']))\n",
    "\n",
    "    # use running window to accumulate evidence:\n",
    "    selfmotion = my_moving_window(ves, window=params['filterwindows'][0],\n",
    "                                  FUN=params['FUN'])\n",
    "\n",
    "    # take the final value as our estimate:\n",
    "    selfmotion = selfmotion[-1]\n",
    "\n",
    "    # compare to threshold, set to 0 if lower\n",
    "    if selfmotion < params['threshold']:\n",
    "        selfmotion = 0\n",
    "    else:\n",
    "        selfmotion = 1\n",
    "\n",
    "    return selfmotion\n",
    "\n",
    "# Use the updated function to run the model and plot the data\n",
    "# Uncomment below to test your function\n",
    "data = {'opticflow': opticflow, 'vestibular': vestibular}\n",
    "params = {'threshold': 0.33, 'filterwindows': [100, 50], 'FUN': np.mean}\n",
    "modelpredictions = my_train_illusion_model(sensorydata=data, params=params)\n",
    "\n",
    "predictions = np.zeros(judgments.shape)\n",
    "predictions[:, 0:3] = judgments[:, 0:3]\n",
    "predictions[:, 3] = modelpredictions['selfmotion']\n",
    "predictions[:, 4] = modelpredictions['worldmotion'] * -1\n",
    "with plt.xkcd():\n",
    "  my_plot_percepts(datasets={'predictions': predictions}, plotconditions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That looks much better, and closer to the actual data. Let's see if the $R^2$ values have improved. Use the optimal values for the threshold and window size that you found previously.\n",
    "\n",
    "### Interactive Demo: evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440,
     "referenced_widgets": [
      "ef4e67ec60974bd1a8c723dc93fb25ba",
      "d9c9723010e5449b942a754b297a6ece",
      "71d527f055e34e9e88ad302e24f05865",
      "a58e09847f354c12a65acb7f458857ee",
      "c8b7ecb991644d91b28229d33c657f57",
      "feb5639c7da44107aa3a160837075947",
      "64967eab8e3b43cda312cf6418add113",
      "28cd95200033432b997ab74403e95e73",
      "c444b1b30f4541a5b7e5c2f83afc7245",
      "0dcba53b6d1249ec876ae9adb4fc8ece"
     ]
    },
    "colab_type": "code",
    "outputId": "b629b765-e6cc-419e-9e6d-08d2fa780ec7"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "data = {'opticflow': opticflow, 'vestibular': vestibular}\n",
    "\n",
    "\n",
    "def refresh(threshold=0, windowsize=100):\n",
    "\n",
    "    # set parameters according to sliders:\n",
    "    params = {'samplingrate': 10, 'FUN': np.mean}\n",
    "    params['filterwindows'] = [windowsize, 50]\n",
    "    params['threshold'] = threshold\n",
    "\n",
    "    modelpredictions = my_train_illusion_model(sensorydata=data, params=params)\n",
    "\n",
    "    predictions = np.zeros(judgments.shape)\n",
    "    predictions[:, 0:3] = judgments[:, 0:3]\n",
    "    predictions[:, 3] = modelpredictions['selfmotion']\n",
    "    predictions[:, 4] = modelpredictions['worldmotion'] * -1\n",
    "\n",
    "    # plot the predictions:\n",
    "    my_plot_predictions_data(judgments, predictions)\n",
    "\n",
    "    # calculate R2\n",
    "    veljudgmnt = np.concatenate((judgments[:, 3], judgments[:, 4]))\n",
    "    velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
    "    slope, intercept, r_value,\\\n",
    "      p_value, std_err = stats.linregress(veljudgmnt, velpredict)\n",
    "\n",
    "    print(f\"predictions -> judgments R2: {r_value ** 2:0.3f}\")\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, threshold=(-1, 2, .01), windowsize=(1, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "While the model still predicts velocity judgments better than the conditions (i.e. the model predicts illusions in somewhat similar cases), the $R^2$ values are a little worse than those of the simpler model. What's really going on is that the same set of points that were model prediction errors in the previous model are also errors here. All we have done is reduce the spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Interpret the model's meaning**\n",
    "\n",
    "Here's what you should have learned from model the train illusion: \n",
    "\n",
    "1. A noisy, vestibular, acceleration signal can give rise to illusory motion.\n",
    "2. However, disambiguating the optic flow by adding the vestibular signal simply adds a lot of noise. This is not a plausible thing for the brain to do.\n",
    "3. Our other hypothesis - credit assignment - is more qualitatively correct, but our simulations were not able to match the frequency of the illusion on a trial-by-trial basis.\n",
    "\n",
    "We decided that for now we have learned enough, so it's time to write it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Section 10: Model publication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "outputId": "140cb6fe-1e35-4edd-ad2a-cecb1896bec1"
   },
   "outputs": [],
   "source": [
    "# @title Video 10: Publication\n",
    "video = YouTubeVideo(id='zm8x7oegN6Q', width=854, height=480, fs=1)\n",
    "print(f\"Video available at https://youtube.com/watch?v={video.id}\")\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "\n",
    "**Goal:** In order for our model to impact the field, it needs to be accepted by our peers, and order for that to happen it matters how the model is published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## TD 10.1: Write a summary of the project\n",
    "\n",
    "Here we will write up our model, by answering the following questions:\n",
    "* **What is the phenomena**?  Here summarize the part of the phenomena which your model addresses.\n",
    "* **What is the key scientific question?**:  Clearly articulate the question which your model tries to answer.\n",
    "* **What was our hypothesis?**:  Explain the key relationships which we relied on to simulate the phenomena.\n",
    "* **How did your model work?** Give an overview of the model, it's main components, and how the model works.  ''Here we ... ''\n",
    "* **What did we find? Did the model work?** Explain the key outcomes of your model evaluation. \n",
    "* **What can we conclude?** Conclude as much as you can _with reference to the hypothesis_, within the limits of the model.  \n",
    "* **What did you learn? What is left to be learned?** Briefly argue the plausibility of the approach and what you think is _essential_ that may have been left out.\n",
    "\n",
    "### Guidance for the future\n",
    "There are good guidelines for structuring and writing an effective paper (e.g., [Mensh & Kording, 2017](https://doi.org/10.1371/journal.pcbi.1005619)), all of which apply to papers about models. There are some extra considerations when publishing a model. In general, you should explain each of the steps in the paper:\n",
    "\n",
    "**Introduction:** Steps 1 & 2 (maybe 3)\n",
    "\n",
    "**Methods:** Steps 3-7, 9\n",
    "\n",
    "**Results:** Steps 8 & 9, going back to 1, 2 & 4\n",
    "\n",
    "In addition, you should provide a visualization of the model, and upload the code implementing the model and the data it was trained and tested on to a repository (e.g. GitHub and OSF).\n",
    "\n",
    "The audience for all of this should be experimentalists, as they are the ones who can test predictions made by your your model and collect new data. This way your models can impact future experiments, and that future data can then be modeled (see modeling process schematic below). Remember your audience - it is _always_ hard to clearly convey the main points of your work to others, especially if your audience doesn't necessarily create computational models themselves.\n",
    "\n",
    "![how-to-model process from Blohm et al 2019](https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W1D2_ModelingPractice/static/NMA-W1D2-fig06.png)\n",
    "\n",
    "### Suggestion\n",
    "\n",
    "For every modeling project, a very good exercise in this is to _**first**_ write a short, 100-word abstract of the project plan and expected impact, like the summary you wrote. This forces focussing on the main points: describing the relevance, question, model, answer and what it all means very succinctly. This allows you to decide to do this project or not **before you commit time writing code for no good purpose**. Notice that this is really what we've walked you through carefully in this tutorial! :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "Confatulations! You have finished Day2 of NMA! In this tutorial, we worked through the rest steps of the process of modeling.\n",
    "\n",
    "- We identified the key components of the model, and examined how they work together (step 6)\n",
    "- We implemented the model (step 7), and completed it (step 8)\n",
    "- We tested and evaluated our model (step 9), and finally\n",
    "- We learn how to publish our model in order to increase its visibility amongts our peers\n",
    "\n",
    "## Post-script\n",
    "\n",
    "Note that the model we built here was extremely simple and used artificial data on purpose. It allowed us to go through all the steps of building a model, and hopefully you noticed that it is not always a linear process, you will go back to different steps if you hit a roadblock somewhere.\n",
    "\n",
    "However, if you're interested in how to actually approach modeling a similar phenomenon in a probabilistic way, we encourage you to read the paper by [Dokka et. al., 2019](https://doi.org/10.1073/pnas.1820373116), where the authors model how judgments of heading direction are influenced by objects that are also moving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "# Reading\n",
    "\n",
    "Blohm G, Kording KP, Schrater PR (2020). _A How-to-Model Guide for Neuroscience_ eNeuro, 7(1). https://doi.org/10.1523/ENEURO.0352-19.2019 \n",
    "\n",
    "Dokka K, Park H, Jansen M, DeAngelis GC, Angelaki DE (2019). _Causal inference accounts for heading perception in the presence of object motion._ PNAS, 116(18):9060-9065. https://doi.org/10.1073/pnas.1820373116\n",
    "\n",
    "Drugowitsch J, DeAngelis GC, Klier EM, Angelaki DE, Pouget A (2014). _Optimal Multisensory Decision-Making in a Reaction-Time Task._ eLife, 3:e03005. https://doi.org/10.7554/eLife.03005\n",
    "\n",
    "Hartmann, M, Haller K, Moser I, Hossner E-J, Mast FW  (2014). _Direction detection thresholds of passive self-motion in artistic gymnasts._ Exp Brain Res, 232:12491258. https://doi.org/10.1007/s00221-014-3841-0\n",
    "\n",
    "Mensh B, Kording K (2017). _Ten simple rules for structuring papers._ PLOS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619\n",
    "\n",
    "Seno T, Fukuda H (2012). _Stimulus Meanings Alter Illusory Self-Motion (Vection) - Experimental Examination of the Train Illusion._ Seeing Perceiving, 25(6):631-45. https://doi.org/10.1163/18784763-00002394\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
