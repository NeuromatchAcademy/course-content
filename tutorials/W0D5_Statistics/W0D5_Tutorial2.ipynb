{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W0D5_Statistics/W0D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Statistical Inference\n",
    "**Week 0, Day 5: Probability & Statistics**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Ulrik Beierholm\n",
    "\n",
    "\n",
    "__Content reviewers:__ Natalie Schaworonkow, Keith van Antwerp, Anoop Kulkarni, Pooya Pakarian, Hyosub Kim\n",
    "\n",
    "__Production editors:__ Ethan Cheng, Ella Batty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "This tutorial builds on Tutorial 1 by explaining how to do inference through inverting the generative process.\n",
    "\n",
    "By completing the exercises in this tutorial, you should:\n",
    "* understand what the likelihood function is, and have some intuition of why it is important\n",
    "* know how to summarise the Gaussian distribution using mean and variance \n",
    "* know how to maximise a likelihood function\n",
    "* be able to do simple inference in both classical and Bayesian ways\n",
    "* (Optional) understand how Bayes Net can be used to model causal relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:36.242730Z",
     "iopub.status.busy": "2021-07-02T12:04:36.242145Z",
     "iopub.status.idle": "2021-07-02T12:04:37.507768Z",
     "shell.execute_reply": "2021-07-02T12:04:37.506883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from numpy.random import default_rng   # a default random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.516616Z",
     "iopub.status.busy": "2021-07-02T12:04:37.515520Z",
     "iopub.status.idle": "2021-07-02T12:04:37.686743Z",
     "shell.execute_reply": "2021-07-02T12:04:37.686293Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label, interact_manual\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.703097Z",
     "iopub.status.busy": "2021-07-02T12:04:37.694646Z",
     "iopub.status.idle": "2021-07-02T12:04:37.706575Z",
     "shell.execute_reply": "2021-07-02T12:04:37.706988Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Plotting & Helper functions\n",
    "\n",
    "def plot_hist(data, xlabel, figtitle = None, num_bins = None):\n",
    "  \"\"\" Plot the given data as a histogram.\n",
    "\n",
    "    Args:\n",
    "      data (ndarray): array with data to plot as histogram\n",
    "      xlabel (str): label of x-axis\n",
    "      figtitle (str): title of histogram plot (default is no title)\n",
    "      num_bins (int): number of bins for histogram (default is 10)\n",
    "\n",
    "    Returns:\n",
    "      count (ndarray): number of samples in each histogram bin\n",
    "      bins (ndarray): center of each histogram bin\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel('Count')\n",
    "  if num_bins is not None:\n",
    "    count, bins, _ = plt.hist(data, max(data), bins = num_bins)\n",
    "  else:\n",
    "    count, bins, _ = plt.hist(data, max(data))  # 10 bins default\n",
    "  if figtitle is not None:\n",
    "    fig.suptitle(figtitle, size=16)\n",
    "  plt.show()\n",
    "  return count, bins\n",
    "\n",
    "def plot_gaussian_samples_true(samples, xspace, mu, sigma, xlabel, ylabel):\n",
    "  \"\"\" Plot a histogram of the data samples on the same plot as the gaussian\n",
    "  distribution specified by the give mu and sigma values.\n",
    "\n",
    "    Args:\n",
    "      samples (ndarray): data samples for gaussian distribution\n",
    "      xspace (ndarray): x values to sample from normal distribution\n",
    "      mu (scalar): mean parameter of normal distribution\n",
    "      sigma (scalar): variance parameter of normal distribution\n",
    "      xlabel (str): the label of the x-axis of the histogram\n",
    "      ylabel (str): the label of the y-axis of the histogram\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  # num_samples = samples.shape[0]\n",
    "\n",
    "  count, bins, _ = plt.hist(samples, density=True) # probability density function\n",
    "\n",
    "  plt.plot(xspace, norm.pdf(xspace, mu, sigma),'r-')\n",
    "  plt.show()\n",
    "\n",
    "def plot_likelihoods(likelihoods, mean_vals, variance_vals):\n",
    "  \"\"\" Plot the likelihood values on a heatmap plot where the x and y axes match\n",
    "  the mean and variance parameter values the likelihoods were computed for.\n",
    "\n",
    "    Args:\n",
    "      likelihoods (ndarray): array of computed likelihood values\n",
    "      mean_vals (ndarray): array of mean parameter values for which the\n",
    "                            likelihood was computed\n",
    "      variance_vals (ndarray): array of variance parameter values for which the\n",
    "                            likelihood was computed\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(likelihoods)\n",
    "\n",
    "  cbar = ax.figure.colorbar(im, ax=ax)\n",
    "  cbar.ax.set_ylabel('log likelihood', rotation=-90, va=\"bottom\")\n",
    "\n",
    "  ax.set_xticks(np.arange(len(mean_vals)))\n",
    "  ax.set_yticks(np.arange(len(variance_vals)))\n",
    "  ax.set_xticklabels(mean_vals)\n",
    "  ax.set_yticklabels(variance_vals)\n",
    "  ax.set_xlabel('Mean')\n",
    "  ax.set_ylabel('Variance')\n",
    "\n",
    "def posterior_plot(x, likelihood=None, prior=None, posterior_pointwise=None, ax=None):\n",
    "  \"\"\"\n",
    "  Plots normalized Gaussian distributions and posterior.\n",
    "\n",
    "    Args:\n",
    "        x (numpy array of floats):         points at which the likelihood has been evaluated\n",
    "        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`\n",
    "        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`\n",
    "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
    "        ax: Axis in which to plot. If None, create new axis.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "  \"\"\"\n",
    "  if likelihood is None:\n",
    "      likelihood = np.zeros_like(x)\n",
    "\n",
    "  if prior is None:\n",
    "      prior = np.zeros_like(x)\n",
    "\n",
    "  if posterior_pointwise is None:\n",
    "      posterior_pointwise = np.zeros_like(x)\n",
    "\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(x, likelihood, '-C1', LineWidth=2, label='Auditory')\n",
    "  ax.plot(x, prior, '-C0', LineWidth=2, label='Visual')\n",
    "  ax.plot(x, posterior_pointwise, '-C2', LineWidth=2, label='Posterior')\n",
    "  ax.legend()\n",
    "  ax.set_ylabel('Probability')\n",
    "  ax.set_xlabel('Orientation (Degrees)')\n",
    "  plt.show()\n",
    "\n",
    "  return ax\n",
    "\n",
    "def plot_classical_vs_bayesian_normal(num_points, mu_classic, var_classic,\n",
    "                                      mu_bayes, var_bayes):\n",
    "  \"\"\" Helper function to plot optimal normal distribution parameters for varying\n",
    "  observed sample sizes using both classic and Bayesian inference methods.\n",
    "\n",
    "    Args:\n",
    "      num_points (int): max observed sample size to perform inference with\n",
    "      mu_classic (ndarray): estimated mean parameter for each observed sample size\n",
    "                                using classic inference method\n",
    "      var_classic (ndarray): estimated variance parameter for each observed sample size\n",
    "                                using classic inference method\n",
    "      mu_bayes (ndarray): estimated mean parameter for each observed sample size\n",
    "                                using Bayesian inference method\n",
    "      var_bayes (ndarray): estimated variance parameter for each observed sample size\n",
    "                                using Bayesian inference method\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  xspace = np.linspace(0, num_points, num_points)\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('n data points')\n",
    "  ax.set_ylabel('mu')\n",
    "  plt.plot(xspace, mu_classic,'r-', label = \"Classical\")\n",
    "  plt.plot(xspace, mu_bayes,'b-', label = \"Bayes\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('n data points')\n",
    "  ax.set_ylabel('sigma^2')\n",
    "  plt.plot(xspace, var_classic,'r-', label = \"Classical\")\n",
    "  plt.plot(xspace, var_bayes,'b-', label = \"Bayes\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "# Section 1: Basic probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Basic probability theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.717253Z",
     "iopub.status.busy": "2021-07-02T12:04:37.714562Z",
     "iopub.status.idle": "2021-07-02T12:04:37.794892Z",
     "shell.execute_reply": "2021-07-02T12:04:37.793051Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Basic Probability\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1bw411o7HR\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"SL0_6rw8zrM\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This video covers basic probability theory, including complementary probability, conditional probability, joint probability, and marginalisation.\n",
    "\n",
    "<details>\n",
    "<summary> <font color='blue'>Click here for text recap of video </font></summary>\n",
    "\n",
    "Previously we were only looking at sampling or properties of a single variables, but as we will now move on to statistical inference, it is useful to go over basic probability theory.\n",
    "\n",
    "\n",
    "As a reminder, probability has to be in the range 0 to 1\n",
    "$P(A) \\in  [0,1] $\n",
    "\n",
    "and the complementary can always be defined as\n",
    "\n",
    "$P(\\neg A) = 1-P(A)$\n",
    "\n",
    "\n",
    "When we have two variables, the *conditional probability* of $A$ given $B$ is \n",
    "\n",
    "$P (A|B) = P (A \\cap B)/P (B)=P (A, B)/P (B)$\n",
    "\n",
    "while the *joint probability* of $A$ and $B$ is \n",
    "\n",
    "$P(A \\cap B)=P(A,B) = P(B|A)P(A) = P(A|B)P(B) $\n",
    "\n",
    "We can then also define the process of *marginalisation* (for discrete variables) as \n",
    "\n",
    "$P(A)=\\sum P(A,B)=\\sum P(A|B)P(B)$ \n",
    "\n",
    "where the summation is over the possible values of $B$.\n",
    "\n",
    "As an example if $B$ is a binary variable that can take values $B+$ or $B0$ then \n",
    "$P(A)=\\sum P(A,B)=P(A|B+)P(B+)+ P(A|B0)P(B0) $.\n",
    "\n",
    "For continuous variables marginalization is given as \n",
    "$P(A)=\\int P(A,B) dB=\\int P(A|B)P(B) dB$ \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Exercise 1.1: Probability example\n",
    "\n",
    "To remind ourselves of how to use basic probability theory we will do a short exercise (no coding needed!), based on measurement of binary probabilistic neural responses.\n",
    "As shown by Hubel and Wiesel in 1959 there are neurons in primary visual cortex that respond to different orientations of visual stimuli, with different neurons being sensitive to different orientations. The numbers in the following are however purely fictional.\n",
    "\n",
    "Imagine that your collaborator tells you that they have recorded the activity of visual neurons while presenting either a horizontal or vertical grid as a visual stimulus. The activity of the neurons is measured as binary: they are either active or inactive in response to the stimulus.\n",
    "\n",
    "After recording from a large number of neurons they find that when presenting a horizontal grid, on average 40% of neurons are active, while 30% respond to vertical grids.\n",
    "\n",
    "We will use the following notation to indicate the probability that a randomly chosen neuron responds to horizontal grids\n",
    "\n",
    "$P(h+)=0.4$\n",
    "\n",
    "and this to show the probability it responds to vertical:\n",
    "\n",
    "$P(v+)=0.3$\n",
    "\n",
    "We can find the complementary event, that the neuron does not respond to the horizontal grid, using the fact that these events must add up to 1. We see that the probability the neuron does not respond to the horizontal grid ($h0$) is \n",
    "\n",
    "$P(h0)=1-P(h+)=0.6$ \n",
    "\n",
    "and that the probability to not respond to vertical is \n",
    "\n",
    "$P(v0)=1-P(v+)=0.7$ \n",
    "\n",
    "We will practice computing various probabilities in this framework.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Product\n",
    "\n",
    "Assuming that the horizontal and vertical orientation selectivity are independent, what is the probability that a randomly chosen neuron is sensitive to both horizontal and vertical orientations? \n",
    "\n",
    "Hint: Two events are independent if the outcome of one does not affect the outcome of the other.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.803623Z",
     "iopub.status.busy": "2021-07-02T12:04:37.803134Z",
     "iopub.status.idle": "2021-07-02T12:04:37.805708Z",
     "shell.execute_reply": "2021-07-02T12:04:37.806090Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Independent here means that  ùëÉ(‚Ñé+,ùë£+) = ùëÉ(‚Ñé+)ùëÉ(ùë£+)\n",
    "\n",
    "P(h+,v+) = P(h+) p(v+)=0.4*0.3=0.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Joint probability generally\n",
    "A collaborator informs you that actually these are not independent. Of those neurons that respond to vertical, only 10 percent also respond to horizontal, i.e. the probability of responding to horizonal *given* that it responds to vertical is $P(h+|v+)=0.1$\n",
    "\n",
    "Given this new information, what is now the probability that a randomly chosen neuron is sensitive to both horizontal and vertical orientations?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.810407Z",
     "iopub.status.busy": "2021-07-02T12:04:37.809918Z",
     "iopub.status.idle": "2021-07-02T12:04:37.812218Z",
     "shell.execute_reply": "2021-07-02T12:04:37.812623Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Remember that joint probability can generally be expressed as  ùëÉ(ùëé,ùëè)=ùëÉ(ùëé|ùëè)ùëÉ(ùëè)\n",
    "\n",
    "ùëÉ(‚Ñé+,ùë£+)=ùëÉ(‚Ñé+|ùë£+)ùëÉ(ùë£+)=0.1‚àó0.3=0.03\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Conditional probability\n",
    "\n",
    "You start measuring from a neuron and find that it responds to horizontal orientations. What is now the probability that it also responds to vertical ($ùëÉ(v+|h+)$)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.816910Z",
     "iopub.status.busy": "2021-07-02T12:04:37.816424Z",
     "iopub.status.idle": "2021-07-02T12:04:37.818686Z",
     "shell.execute_reply": "2021-07-02T12:04:37.819066Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "The conditional probability is given by  ùëÉ(ùëé|ùëè)=ùëÉ(ùëé,ùëè)/ùëÉ(ùëè)\n",
    "\n",
    "ùëÉ(ùë£+|‚Ñé+)=ùëÉ(ùë£+,‚Ñé+)/ùëÉ(‚Ñé+)=ùëÉ(‚Ñé+|ùë£+)ùëÉ(ùë£+)/ùëÉ(‚Ñé+)=0.1‚àó0.3/0.4=0.075\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Marginal probability\n",
    "\n",
    "Lastly, let's check that everything has been done correctly. Given our knowledge about the conditional probabilities, we should be able to use marginalisation to recover the marginal probability of a random neuron responding to vertical orientations ($P(v+)$). We know from above that this should equal 0.3.\n",
    "Calculate $P(v+)$  based on the conditional probabilities for $P(v+|h+)$ and $P(v+|h0)$ (the latter which you will need to calculate).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.822836Z",
     "iopub.status.busy": "2021-07-02T12:04:37.822351Z",
     "iopub.status.idle": "2021-07-02T12:04:37.824507Z",
     "shell.execute_reply": "2021-07-02T12:04:37.824868Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "The first step is to calculute:\n",
    "ùëÉ(ùë£+|‚Ñé0)=ùëÉ(‚Ñé0|ùë£+)ùëÉ(ùë£+)/ùëÉ(‚Ñé0)=(1‚àí0.1)‚àó0.3/(1‚àí0.4)=0.45\n",
    "\n",
    "Then use the property of marginalisation (discrete version)\n",
    "\n",
    "ùëÉ(ùëé)=‚àëùëñùëÉ(ùëé|ùëè=ùëñ)ùëÉ(ùëè=ùëñ)\n",
    "\n",
    "ùëÉ(ùë£+)=ùëÉ(ùë£+|‚Ñé+)ùëÉ(‚Ñé+)+ùëÉ(ùë£+|‚Ñé0)ùëÉ(‚Ñé0)=0.075‚àó0.4+0.45‚àó(1‚àí0.4)=0.3\n",
    "\n",
    "Phew, we recovered the correct value!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Markov chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.835291Z",
     "iopub.status.busy": "2021-07-02T12:04:37.834814Z",
     "iopub.status.idle": "2021-07-02T12:04:37.889017Z",
     "shell.execute_reply": "2021-07-02T12:04:37.887012Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Markov Chains\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1Rh41187ZC\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"XjQF13xMpss\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Coding exercise 1.2 Markov chains\n",
    "\n",
    "\n",
    "We will practice more probability theory by looking at **Markov chains**. The Markov property specifies that you can fully encapsulate the important properties of a system based on its *current* state at the current time, any previous history does not matter. It is memoryless.\n",
    "\n",
    "As an example imagine that a rat is able to move freely between 3 areas: a dark rest area\n",
    "($state=1$), a nesting area ($state=2$) and a bright area for collecting food ($state=3$). Every 5 minutes (timepoint $i$) we record the rat's location. We can use a **categorical distribution** to look at the probability that the rat moves to one state from another.\n",
    "\n",
    "The table below shows the probability of the rat transitioning from one area to another between timepoints ($state_i$ to $state_{i+1}$).\n",
    "\n",
    "\\begin{array}{|l |  l | l | l |} \\hline\n",
    "state_{i} &P(state_{i+1}=1|state_i=*) &P(state_{i+1}=2|state_i=*) & P(state_{i+1}=3|state=_i*) \\\\ \\hline\n",
    "state_{i}=1& 0.2  &0.6 &0.2\\\\\n",
    "state_{i}=2& .6 &0.3& 0.1\\\\\n",
    "state_{i}=3& 0.8 &0.2 &0\\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "We are modeling this as a Markov chain, so the animal is only in one of the states at a time and can transition between the states.\n",
    "\n",
    "We want to get the probability of each state at time $i+1$. We know from Section 1.1 that we can use marginalisation:\n",
    "\n",
    "$$P_(state_{i+1} = 1) = P(state_{i+1}=1|state_i=1)P(state_i = 1) +  P(state_{i+1}=1|state_i=2)P(state_i = 2) +  P(state_{i+1}=1|state_i=3)P(state_i = 3)  $$\n",
    "\n",
    "Let's say we had a row vector (a vector defined as a row, not a column so matrix multiplication will work out) of the probabilities of the current state:\n",
    "\n",
    "$$P_i = [P(state_i = 1), P(state_i = 2), P(state_i = 3) ] $$\n",
    "\n",
    "If we actually know where the rat is at the current time point, this would be deterministic (e.g. $P_i = [0, 1, 0]$ if the rat is in state 2). Otherwise, this could be probabilistic (e.g. $P_i = [0.1, 0.7, 0.2]$).\n",
    "\n",
    "To compute the vector of probabilities of the state at the time $i+1$, we can use linear algebra and multiply our vector of the probabilities of the current state with the transition matrix.  Recall your matrix multiplication skills from W0D3 to check this! \n",
    "\n",
    "$$P_{i+1} = P_{i} T$$\n",
    "where $T$ is our transition matrix.\n",
    "\n",
    "\n",
    "This is the same formula for every step, which allows us to get the probabilities for a time more than 1 step in advance easily. If we started at $i=0$ and wanted to look at the probabilities at step $i=2$, we could do:\n",
    "\n",
    "\\begin{align*}\n",
    "P_{1} &= P_{0}T\\\\\n",
    "P_{2} &= P_{1}T = P_{0}TT = P_{0}T^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So, every time we take a further step we can just multiply with the transition matrix again. So, the probability vector of states at j timepoints after the current state at timepoint i is equal to the probability vector at timepoint i times the transition matrix raised to the jth power.\n",
    "$$P_{i + j} = P_{i}T^j $$\n",
    "\n",
    "If the animal starts in area 2, what is the probability the animal will again be in area 2 when we check on it 20 minutes (4 transitions) later?\n",
    "\n",
    "Fill in the transition matrix in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.899195Z",
     "iopub.status.busy": "2021-07-02T12:04:37.898706Z",
     "iopub.status.idle": "2021-07-02T12:04:37.985153Z",
     "shell.execute_reply": "2021-07-02T12:04:37.984577Z"
    }
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## TODO for student\n",
    "## Fill out the following then remove\n",
    "raise NotImplementedError(\"Student exercise: compute state probabilities after 4 transitions\")\n",
    "###################################################################\n",
    "\n",
    "# Transition matrix\n",
    "transition_matrix = np.array([[ 0.2, 0.6, 0.2],[ .6, 0.3, 0.1], [0.8, 0.2, 0]])\n",
    "\n",
    "# Initial state, p0\n",
    "p0 = np.array([0, 1, 0])\n",
    "\n",
    "# Compute the probabilities 4 transitions later (use np.linalg.matrix_power to raise a matrix a power)\n",
    "p4 = ...\n",
    "\n",
    "# The second area is indexed as 1 (Python starts indexing at 0)\n",
    "print(\"The probability the rat will be in area 2 after 4 transitions is: \" + str(p4[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.991084Z",
     "iopub.status.busy": "2021-07-02T12:04:37.990037Z",
     "iopub.status.idle": "2021-07-02T12:04:37.992656Z",
     "shell.execute_reply": "2021-07-02T12:04:37.992224Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# Transition matrix\n",
    "transition_matrix = np.array([[ 0.2, 0.6, 0.2],[ .6, 0.3, 0.1], [0.8, 0.2, 0]])\n",
    "\n",
    "# Initial state, p0\n",
    "p0 = np.array([0, 1, 0])\n",
    "\n",
    "# Compute the probabilities 4 transitions later (use np.linalg.matrix_power to raise a matrix a power)\n",
    "p4 = p0 @ np.linalg.matrix_power(transition_matrix, 4)\n",
    "\n",
    "# The second area is indexed as 1 (Python starts indexing at 0)\n",
    "print(\"The probability the rat will be in area 2 after 4 transitions is: \" + str(p4[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a probability of 0.4311, i.e. there is a 43.11% chance that you will find the rat in area 2 in 20 minutes.\n",
    "\n",
    "What is the average amount of time spent by the rat in each of the states? \n",
    "\n",
    "Implicit in the question is the idea that we can start off with a random initial state and then measure how much relative time is spent in each area. If we make a few assumptions (e.g. ergodic or 'randomly mixing' system), we can instead start with an initial random distribution and see how the final probabilities of each state after many time steps (100) to estimate the time spent in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:37.999578Z",
     "iopub.status.busy": "2021-07-02T12:04:37.999090Z",
     "iopub.status.idle": "2021-07-02T12:04:38.001387Z",
     "shell.execute_reply": "2021-07-02T12:04:38.000903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize random initial distribution\n",
    "p_random = np.ones((1,3))/3\n",
    "\n",
    "###################################################################\n",
    "## TODO for student: Fill compute the state matrix after 100 transitions\n",
    "raise NotImplementedError(\"Student exercise: need to complete computation below\")\n",
    "###################################################################\n",
    "\n",
    "# Fill in the missing line to get the state matrix after 100 transitions, like above\n",
    "p_average_time_spent = ...\n",
    "print(\"The proportion of time spend by the rat in each of the three states is: \"\n",
    "            + str(p_average_time_spent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.006875Z",
     "iopub.status.busy": "2021-07-02T12:04:38.005779Z",
     "iopub.status.idle": "2021-07-02T12:04:38.008432Z",
     "shell.execute_reply": "2021-07-02T12:04:38.008010Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# Initialize random initial distribution\n",
    "p_random = np.ones((1,3))/3\n",
    "\n",
    "# Fill in the missing line to get the state matrix after 100 transitions, like above\n",
    "p_average_time_spent = p_random @ np.linalg.matrix_power(transition_matrix, 100)\n",
    "print(\"The proportion of time spend by the rat in each of the three states is: \"\n",
    "            + str(p_average_time_spent[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of time spend in each of the three areas are 0.4473, 0.4211, and 0.1316, respectively.\n",
    "\n",
    "Imagine now that if the animal is satiated and tired the transitions change to:\n",
    "\n",
    "\\begin{array}{|l |  l | l | l |} \\hline\n",
    "state_{i} &P(state_{i+1}=1|state_i=*) &P(state_{i+1}=2|state_i=*) &P(state_{i+1}=3|state_i=*) \\\\ \\hline\n",
    "state_{i}=1& 0.2  &0.7 &0.1\\\\\n",
    "state_{i}=2& .3 &0.7& 0.\\\\\n",
    "state_{i}=3& 0.8 &0.2 &0\\\\ \\hline\n",
    "\\end{array}\n",
    "\n",
    "Try repeating the questions above for this table of transitions by changing the transition matrix. Based on the probability values, what would you predict? Check how much time the rat spends on average in each area and see if it matches your predictions.\n",
    "\n",
    "**Main course preview:** The Markov property is extremely important for many models, particularly Hidden Markov Models, discussed on day W3D2, and for methods such as Markov Chain Monte Carlo sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Statistical inference and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.018333Z",
     "iopub.status.busy": "2021-07-02T12:04:38.015446Z",
     "iopub.status.idle": "2021-07-02T12:04:38.068932Z",
     "shell.execute_reply": "2021-07-02T12:04:38.065793Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Statistical inference and likelihood\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1LM4y1g7wT\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"7aiKvKlYwR0\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correction to video**: The variance estimate that maximizes the likelihood is $\\bar{\\sigma}^2=\\frac{1}{n} \\sum_i (x_i-\\bar{x})^2 $. This is a biased estimate. Shown in the video is the sample variance, which is an unbiased estimate for variance: $\\bar{\\sigma}^2=\\frac{1}{n-1} \\sum_i (x_i-\\bar{x})^2 $. See section 2.2.3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='blue'>Click here for text recap of video </font></summary>\n",
    "\n",
    "A generative model (such as the Gaussian distribution from the previous tutorial) allows us to make predictions about outcomes. \n",
    "\n",
    "However, after we observe $n$ data points, we can also evaluate our model (and any of its associated parameters) by calculating the **likelihood** of our model having generated each of those data points $x_i$.\n",
    "\n",
    "$$P(x_i|\\mu,\\sigma)=\\mathcal{N}(x_i,\\mu,\\sigma)$$\n",
    "\n",
    "For all data points $\\mathbf{x}=(x_1, x_2, x_3, ...x_n) $ we can then calculate the likelihood for the whole dataset by computing the product of the likelihood for each single data point.\n",
    "\n",
    "$$P(\\mathbf{x}|\\mu,\\sigma)=\\prod_{i=1}^n \\mathcal{N}(x_i,\\mu,\\sigma)$$\n",
    "\n",
    "</details>\n",
    "\n",
    "While the likelihood may be written as a conditional probability ($P(x|\\mu,\\sigma)$), we refer to it as the **likelihood function**, $L(\\mu,\\sigma)$.  This slight switch in notation is to emphasize our focus: we use likelihood functions when the data points $\\mathbf{x}$ are fixed and we are focused on the parameters. \n",
    "Our new notation makes clear that the likelihood $L(\\mu,\\sigma)$ is a function of $\\mu$ and $\\sigma$, not of $\\mathbf{x}$.\n",
    "\n",
    "In the last tutorial we reviewed how the data was generated given the selected parameters of the generative process. If we do not know the parameters $\\mu$, $\\sigma$ that generated the data, we can try to **infer** which parameter values (given our model) gives the best (highest) likelihood. This is what we call statistical inference: trying to infer what parameters make our observed data the most likely or probable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 2.1: Computing likelihood\n",
    "\n",
    "Let's start with computing the likelihood of some set of data points being drawn from a Gaussian distribution with a mean and variance we choose. \n",
    "\n",
    "\n",
    "\n",
    "As multiplying small probabilities together can lead to very small numbers, it is often convenient to report the *logarithm* of the likelihood. This is just a convenient transformation and as logarithm is a monotonically increasing function this does not change what parameters maximise the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.083464Z",
     "iopub.status.busy": "2021-07-02T12:04:38.082960Z",
     "iopub.status.idle": "2021-07-02T12:04:38.085276Z",
     "shell.execute_reply": "2021-07-02T12:04:38.085696Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
    "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
    "  potential mean and variance values for a normal distribution\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): 1-D array with all the observed data\n",
    "      mean_val (scalar): value of mean for which to compute likelihood\n",
    "      standard_dev_val (scalar): value of variance for which to compute likelihood\n",
    "\n",
    "    Returns:\n",
    "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
    "  \"\"\"\n",
    "\n",
    "  ###################################################################\n",
    "  ## TODO for student\n",
    "  raise NotImplementedError(\"Student exercise: compute likelihood\")\n",
    "  ###################################################################\n",
    "\n",
    "  # Get probability of each data point (use norm.pdf from scipy stats)\n",
    "  p_data = ...\n",
    "\n",
    "  # Compute likelihood (sum over the log of the probabilities)\n",
    "  likelihood = ...\n",
    "\n",
    "  return likelihood\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Compute likelihood for a guessed mean/standard dev\n",
    "guess_mean = 4\n",
    "guess_standard_dev = .1\n",
    "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.091681Z",
     "iopub.status.busy": "2021-07-02T12:04:38.091206Z",
     "iopub.status.idle": "2021-07-02T12:04:38.093402Z",
     "shell.execute_reply": "2021-07-02T12:04:38.093759Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
    "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
    "  potential mean and variance values for a normal distribution\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): 1-D array with all the observed data\n",
    "      mean_val (scalar): value of mean for which to compute likelihood\n",
    "      standard_dev_val (scalar): value of variance for which to compute likelihood\n",
    "\n",
    "    Returns:\n",
    "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
    "  \"\"\"\n",
    "\n",
    "  # Get probability of each data point (use norm.pdf from scipy stats)\n",
    "  p_data = norm.pdf(x, mean_val, standard_dev_val)\n",
    "\n",
    "  # Compute likelihood (sum over the log of the probabilities)\n",
    "  likelihood = np.sum(np.log(p_data))\n",
    "\n",
    "  return likelihood\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Compute likelihood for a guessed mean/standard dev\n",
    "guess_mean = 4\n",
    "guess_standard_dev = .1\n",
    "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a likelihood of -92904.81. This is somewhat meaningless to us! For it to be useful, we need to compare it to the likelihoods computing using other guesses of the mean or standard deviation. The visualization below shows us the likelihood for various values of the mean and the standard deviation. Essentially, we are performing a rough grid-search over means and standard deviations.  What would you guess as the true mean and standard deviation based on this visualization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.100697Z",
     "iopub.status.busy": "2021-07-02T12:04:38.100199Z",
     "iopub.status.idle": "2021-07-02T12:04:38.515711Z",
     "shell.execute_reply": "2021-07-02T12:04:38.514927Z"
    }
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to visualize likelihoods\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "\n",
    "# Compute likelihood for different mean/variance values\n",
    "mean_vals = np.linspace(1, 10, 10) # potential mean values to ry\n",
    "standard_dev_vals = np.array([0.7, 0.8, 0.9, 1, 1.2, 1.5, 2, 3, 4, 5]) # potential variance values to try\n",
    "\n",
    "# Initialise likelihood collection array\n",
    "likelihood = np.zeros((mean_vals.shape[0], standard_dev_vals.shape[0]))\n",
    "\n",
    "# Compute the likelihood for observing the gvien data x assuming\n",
    "# each combination of mean and variance values\n",
    "for idxMean in range(mean_vals.shape[0]):\n",
    "  for idxVar in range(standard_dev_vals .shape[0]):\n",
    "    likelihood[idxVar,idxMean]= sum(np.log(norm.pdf(x, mean_vals[idxMean],\n",
    "                                              standard_dev_vals[idxVar])))\n",
    "\n",
    "# Uncomment once you've generated the samples and compute likelihoods\n",
    "xspace = np.linspace(0, 10, 100)\n",
    "plot_likelihoods(likelihood, mean_vals, standard_dev_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.526794Z",
     "iopub.status.busy": "2021-07-02T12:04:38.526314Z",
     "iopub.status.idle": "2021-07-02T12:04:38.585702Z",
     "shell.execute_reply": "2021-07-02T12:04:38.585279Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Maximum likelihood\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1Lo4y1C7xy\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"Fuwx_V64nEU\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implicitly, by looking for the parameters that give the highest likelihood in the last section, we have been searching for the **maximum likelihood** estimate.\n",
    "$$(\\hat{\\mu},\\hat{\\sigma})=argmax_{\\mu,\\sigma}L(\\mu,\\sigma)=argmax_{\\mu,\\sigma} \\prod_{i=1}^n \\mathcal{N}(x_i,\\mu,\\sigma)$$.\n",
    "\n",
    "\n",
    "\n",
    "In next sections, we will look at other ways of inferring such parameter variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.1: Searching for best parameters\n",
    "\n",
    "We want to do inference on this data set, i.e. we want to infer the parameters that most likely gave rise to the data given our model. Intuitively that means that we want as good as possible a fit between the observed data and the probability distribution function with the best inferred parameters. We can search for the best parameters manually by trying out a bunch of possible values of the parameters, computing the likelihoods, and picking the parameters that resulted in the highest likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Demo 2.2: Maximum likelihood inference\n",
    "\n",
    "Try to see how well you can fit the probability distribution to the data by using the demo sliders to control the mean and standard deviation parameters of the distribution. We will visualize the histogram of data points (in blue) and the Gaussian density curve with that mean and standard deviation (in red). Below, we print the log-likelihood.\n",
    "\n",
    "- What (approximate) values of mu and sigma result in the best fit?\n",
    "- How does the value below the plot (the log-likelihood) change with the quality of fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.620236Z",
     "iopub.status.busy": "2021-07-02T12:04:38.603745Z",
     "iopub.status.idle": "2021-07-02T12:04:38.853403Z",
     "shell.execute_reply": "2021-07-02T12:04:38.853805Z"
    }
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget and fit by hand!\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "vals = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "def plotFnc(mu,sigma):\n",
    "  loglikelihood= sum(np.log(norm.pdf(vals,mu,sigma)))\n",
    "  #calculate histogram\n",
    "\n",
    "  #prepare to plot\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_ylabel('probability')\n",
    "\n",
    "  #plot histogram\n",
    "  count, bins, ignored = plt.hist(vals,density=True)\n",
    "  x = np.linspace(0,10,100)\n",
    "\n",
    "  #plot pdf\n",
    "  plt.plot(x, norm.pdf(x,mu,sigma),'r-')\n",
    "  plt.show()\n",
    "  print(\"The log-likelihood for the selected parameters is: \" + str(loglikelihood))\n",
    "\n",
    "#interact(plotFnc, mu=5.0, sigma=2.1);\n",
    "#interact(plotFnc, mu=widgets.IntSlider(min=0.0, max=10.0, step=1, value=4.0),sigma=widgets.IntSlider(min=0.1, max=10.0, step=1, value=4.0));\n",
    "interact(plotFnc, mu=(0.0, 15.0, 0.1),sigma=(0.1, 5.0, 0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.860487Z",
     "iopub.status.busy": "2021-07-02T12:04:38.859984Z",
     "iopub.status.idle": "2021-07-02T12:04:38.862623Z",
     "shell.execute_reply": "2021-07-02T12:04:38.862219Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "- The log-likelihood should be greatest when  ùúá  = 5 and  ùúé  = 1.\n",
    "- The summed log-liklihood increases (becomes less negative) as the fit improves\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this was similar to the grid searched image from Section 2.1. Really, we want to see if we can do inference on observed data in a bit more principled way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.2: Optimization to find parameters\n",
    "\n",
    "Let's again assume that we have a data set, $\\mathbf{x}$, assumed to be generated by a normal distribution (we actually generate it ourselves in line 1, so we know how it was generated!).\n",
    "We want to maximise the likelihood of the parameters $\\mu$ and $\\sigma^2$. We can do so using a couple of tricks:\n",
    "\n",
    "*   Using a log transform will not change the maximum of the function, but will allow us to work with very small numbers that could lead to problems with machine precision.\n",
    "*   Maximising a function is the same as minimising the negative of a function, allowing us to use the minimize optimisation provided by scipy.\n",
    "\n",
    "The optimisation will be done using `sp.optimize.minimize`, which does a version of gradient descent (there are hundreds of ways to do numerical optimisation, we will not cover these here!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coding Exercise 2.2: Maximum Likelihood Estimation\n",
    "\n",
    "\n",
    "In the code below, insert the missing line (see the `compute_likelihood_normal` function from previous exercise), with the mean as theta[0] and variance as theta[1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.870905Z",
     "iopub.status.busy": "2021-07-02T12:04:38.870425Z",
     "iopub.status.idle": "2021-07-02T12:04:38.887087Z",
     "shell.execute_reply": "2021-07-02T12:04:38.886514Z"
    }
   },
   "outputs": [],
   "source": [
    "# We define the function to optimise, the negative log likelihood\n",
    "def negLogLike(theta, x):\n",
    "  \"\"\" Function for computing the negative log-likelihood given the observed data\n",
    "  and given parameter values stored in theta.\n",
    "\n",
    "    Args:\n",
    "      theta (ndarray): normal distribution parameters (mean is theta[0],\n",
    "                            variance is theta[1])\n",
    "       x (ndarray): array with observed data points\n",
    "\n",
    "    Returns:\n",
    "      Calculated negative Log Likelihood value!\n",
    "  \"\"\"\n",
    "  ###################################################################\n",
    "  ## TODO for students: Compute the negative log-likelihood value for the\n",
    "  ## given observed data values and parameters (theta)\n",
    "  # Fill out the following then remove\n",
    "  raise NotImplementedError(\"Student exercise: need to compute the negative \\\n",
    "                                log-likelihood value\")\n",
    "  ###################################################################\n",
    "  return ...\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Define bounds, var has to be positive\n",
    "bnds = ((None, None), (0, None))\n",
    "\n",
    "# Optimize with scipy!\n",
    "optimal_parameters = sp.optimize.minimize(negLogLike, (2, 2), args = x, bounds = bnds)\n",
    "print(\"The optimal mean estimate is: \" + str(optimal_parameters.x[0]))\n",
    "print(\"The optimal variance estimate is: \" + str(optimal_parameters.x[1]))\n",
    "\n",
    "# optimal_parameters contains a lot of information about the optimization,\n",
    "# but we mostly want the mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.893753Z",
     "iopub.status.busy": "2021-07-02T12:04:38.893178Z",
     "iopub.status.idle": "2021-07-02T12:04:38.906162Z",
     "shell.execute_reply": "2021-07-02T12:04:38.905773Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# We define the function to optimise, the negative log likelihood\n",
    "def negLogLike(theta, x):\n",
    "  \"\"\" Function for computing the negative log-likelihood given the observed data\n",
    "  and given parameter values stored in theta.\n",
    "\n",
    "    Args:\n",
    "      theta (ndarray): normal distribution parameters (mean is theta[0],\n",
    "                            variance is theta[1])\n",
    "      x (ndarray): array with observed data points\n",
    "\n",
    "    Returns:\n",
    "      Calculated negative Log Likelihood value!\n",
    "  \"\"\"\n",
    "  return -sum(np.log(norm.pdf(x, theta[0], theta[1])))\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Define bounds, var has to be positive\n",
    "bnds = ((None, None), (0, None))\n",
    "\n",
    "# Optimize with scipy!\n",
    "optimal_parameters = sp.optimize.minimize(negLogLike, (2, 2), args = x, bounds = bnds)\n",
    "print(\"The optimal mean estimate is: \" + str(optimal_parameters.x[0]))\n",
    "print(\"The optimal variance estimate is: \" + str(optimal_parameters.x[1]))\n",
    "\n",
    "# optimal_parameters contains a lot of information about the optimization,\n",
    "# but we mostly want the mean and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the approximations of the parameters that maximise the likelihood ($\\mu$ ~ 5.280 and $\\sigma$ ~ 1.148). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.3: Analytical solution\n",
    "\n",
    "Sometimes, things work out well and we can come up with formulas for the maximum likelihood estimates of parameters. We won't get into this further but basically we could set the derivative of the likelihood to 0 (to find a maximum) and solve for the parameters. This won't always work but for the Gaussian distribution, it does.\n",
    "\n",
    "Specifically , the special thing about the Gaussian is that mean and standard deviation of the random sample can effectively approximate the two parameters of a Gaussian, $\\mu, \\sigma$.\n",
    "\n",
    "\n",
    "Hence using the  mean, $\\bar{x}=\\frac{1}{n}\\sum_i x_i$, and variance, $\\bar{\\sigma}^2=\\frac{1}{n} \\sum_i (x_i-\\bar{x})^2 $ of the sample should give us the best/maximum likelihood, $L(\\bar{x},\\bar{\\sigma}^2)$. \n",
    "\n",
    "Let's compare these values to those we've been finding using manual search and optimization, and the true values (which we only know because we generated the numbers!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.912004Z",
     "iopub.status.busy": "2021-07-02T12:04:38.911011Z",
     "iopub.status.idle": "2021-07-02T12:04:38.913602Z",
     "shell.execute_reply": "2021-07-02T12:04:38.913205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Compute and print sample means and standard deviations\n",
    "print(\"This is the sample mean as estimated by numpy: \" + str(np.mean(x)))\n",
    "print(\"This is the sample standard deviation as estimated by numpy: \" + str(np.std(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.917785Z",
     "iopub.status.busy": "2021-07-02T12:04:38.917304Z",
     "iopub.status.idle": "2021-07-02T12:04:38.919578Z",
     "shell.execute_reply": "2021-07-02T12:04:38.919964Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\" You should notice that the parameters estimated by maximum likelihood\n",
    "estimation/inference are very close to the true parameters (mu = 5, sigma = 1),\n",
    "as well as the parameters visualized to be best after Coding Exercise 2.1,\n",
    " where all likelihood values were calculated explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try out different values of the mean and standard deviation in all the previous exercises, you should see that changing the mean and\n",
    "sigma parameter values (and generating new data from a distribution with theseparameters) makes no difference as MLE methods can still recover these parameters. \n",
    "\n",
    "There is a slight problem: it turns out that the maximum likelihood estimate for the variance is actually a biased one! This means that the estimators expected value (mean value) and the true value of the parameter are different.  An unbiased estimator for the variance is $\\bar{\\sigma}^2=\\frac{1}{n-1} \\sum_i (x_i-\\bar{x})^2 $, this is called the sample variance. For more details, see [the wiki page on bias of estimators](https://en.wikipedia.org/wiki/Bias_of_an_estimator). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:38.930696Z",
     "iopub.status.busy": "2021-07-02T12:04:38.930220Z",
     "iopub.status.idle": "2021-07-02T12:04:38.988408Z",
     "shell.execute_reply": "2021-07-02T12:04:38.987826Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Bayesian inference with Gaussian distribution\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV11K4y1u7vH\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"1Q3VqcpfvBk\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start to introduce Bayesian inference here to contrast with our maximum likelihood methods, but you will also revisit Bayesian inference in great detail on W3D1 of the course so we won't dive into all details.\n",
    "\n",
    "For Bayesian inference we do not focus on the likelihood function $L(y)=P(x|y)$, but instead focus on the posterior distribution: \n",
    "\n",
    "$$P(y|x)=\\frac{P(x|y)P(y)}{P(x)}$$\n",
    "\n",
    "which is composed of the **likelihood** function $P(x|y)$, the **prior** $P(y)$ and a normalising term $P(x)$ (which we will ignore for now).\n",
    "\n",
    "While there are other advantages to using Bayesian inference (such as the ability to derive Bayesian Nets, see optional bonus task below), we will start by focusing on the role of the prior in inference. Does including prior information allow us to infer parameters in a better way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think! 3.1: Bayesian inference with Gaussian distribution\n",
    "\n",
    "In the above sections we performed inference using maximum likelihood, i.e. finding the parameters that maximised the likelihood of a set of parameters, given the model and data.\n",
    "\n",
    "We will now repeat the inference process, but with an added Bayesian prior, and compare it to the \"classical\" inference (maximum likelihood) process we did before (Section 2). When using conjugate priors (more on this below) we can just update the parameter values of the distributions (here Gaussian distributions). \n",
    "\n",
    "\n",
    "For the prior we start by guessing a mean of 5 (mean of previously observed data points 4 and 6) and variance of 1 (variance of 4 and 6). We use a trick (not detailed here) that is a simplified way of applying a prior, that allows us to just add these 2 values (pseudo-data) to the real data.\n",
    "\n",
    "See the visualization below that shows the mean and standard deviation inferred by our classical maximum likelihood approach and the Bayesian approach for different numbers of data points.\n",
    "\n",
    "Remembering that our true values are $\\mu = 5$, and $\\sigma^2 = 1$, how do the Bayesian inference and classical inference compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.049499Z",
     "iopub.status.busy": "2021-07-02T12:04:39.025055Z",
     "iopub.status.idle": "2021-07-02T12:04:39.484122Z",
     "shell.execute_reply": "2021-07-02T12:04:39.483386Z"
    }
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to visualize inference\n",
    "\n",
    "def classic_vs_bayesian_normal(mu, sigma, num_points, prior):\n",
    "  \"\"\" Compute both classical and Bayesian inference processes over the range of\n",
    "  data sample sizes (num_points) for a normal distribution with parameters\n",
    "  mu,sigma for comparison.\n",
    "\n",
    "  Args:\n",
    "    mu (scalar): the mean parameter of the normal distribution\n",
    "    sigma (scalar): the standard deviation parameter of the normal distribution\n",
    "    num_points (int): max number of points to use for inference\n",
    "    prior (ndarray): prior data points for Bayesian inference\n",
    "\n",
    "  Returns:\n",
    "    mean_classic (ndarray): estimate mean parameter via classic inference\n",
    "    var_classic (ndarray): estimate variance parameter via classic inference\n",
    "    mean_bayes (ndarray): estimate mean parameter via Bayesian inference\n",
    "    var_bayes (ndarray): estimate variance parameter via Bayesian inference\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize the classical and Bayesian inference arrays that will estimate\n",
    "  # the normal parameters given a certain number of randomly sampled data points\n",
    "  mean_classic = np.zeros(num_points)\n",
    "  var_classic = np.zeros(num_points)\n",
    "\n",
    "  mean_bayes = np.zeros(num_points)\n",
    "  var_bayes = np.zeros(num_points)\n",
    "\n",
    "  for nData in range(num_points):\n",
    "\n",
    "    random_num_generator = default_rng(0)\n",
    "    x = random_num_generator.normal(mu, sigma, nData + 1)\n",
    "\n",
    "    # Compute the mean of those points and set the corresponding array entry to this value\n",
    "    mean_classic[nData] = np.mean(x)\n",
    "\n",
    "    # Compute the variance of those points and set the corresponding array entry to this value\n",
    "    var_classic[nData] = np.var(x)\n",
    "\n",
    "    # Bayesian inference with the given prior is performed below for you\n",
    "    xsupp = np.hstack((x, prior))\n",
    "    mean_bayes[nData] = np.mean(xsupp)\n",
    "    var_bayes[nData] = np.var(xsupp)\n",
    "\n",
    "  return mean_classic, var_classic, mean_bayes, var_bayes\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set normal distribution parameters, mu and sigma\n",
    "mu = 5\n",
    "sigma = 1\n",
    "\n",
    "# Set the prior to be two new data points, 4 and 6, and print the mean and variance\n",
    "prior = np.array((4, 6))\n",
    "print(\"The mean of the data comprising the prior is: \" + str(np.mean(prior)))\n",
    "print(\"The variance of the data comprising the prior is: \" + str(np.var(prior)))\n",
    "\n",
    "mean_classic, var_classic, mean_bayes, var_bayes = classic_vs_bayesian_normal(mu, sigma, 60, prior)\n",
    "plot_classical_vs_bayesian_normal(60, mean_classic, var_classic, mean_bayes, var_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.488823Z",
     "iopub.status.busy": "2021-07-02T12:04:39.488333Z",
     "iopub.status.idle": "2021-07-02T12:04:39.493660Z",
     "shell.execute_reply": "2021-07-02T12:04:39.493053Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Hopefully you can see that the blue line stays a little closer to the true values ($\\mu=5$, $\\sigma^2=1$).\n",
    "\n",
    " Having a simple prior in the Bayesian inference process (blue) helps to regularise\n",
    " the inference of the mean and variance parameters when you have very little data,\n",
    " but has little effect with large data sets. You can see that as the number of data points\n",
    " (x-axis) increases, both inference processes (blue and red lines) get closer and closer\n",
    " together, i.e. their estimates for the true parameters converge as sample size increases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that the prior is only beneficial when it is close to the true value, i.e. 'a good guess' (or at least not a bad guess). As we will see in the next exercise, if you have a prior/bias that is very wrong, your inference will start off very wrong!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Conjugate priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.504635Z",
     "iopub.status.busy": "2021-07-02T12:04:39.495789Z",
     "iopub.status.idle": "2021-07-02T12:04:39.563759Z",
     "shell.execute_reply": "2021-07-02T12:04:39.563349Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Conjugate priors\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1Hg41137Zr\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"mDEyZHaG5aY\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demo 3.2: Conjugate priors\n",
    "Let's return to our example from Tutorial 1 using the binomial distribution - rat in a T-maze.\n",
    "\n",
    "Bayesian inference can be used for any likelihood distribution, but it is a lot more convenient to work with **conjugate** priors, where multiplying the prior with the likelihood just provides another instance of the prior distribution with updated values. \n",
    "\n",
    "For the binomial likelihood it is convenient to use the **beta** distribution as a prior\n",
    "\n",
    "\\begin{aligned}f(p;\\alpha ,\\beta )={\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}p^{\\alpha -1}(1-p)^{\\beta -1}\\end{aligned}\n",
    "where $B$ is the beta function, $\\alpha$ and $\\beta$ are parameters, and $p$ is the probability of the rat turning left or right. The beta distribution is thus a distribution over a probability.\n",
    "\n",
    "Given a series of Left and Right moves of the rat, we can now estimate the probability that the animal will turn left. Using Bayesian Inference, we use a beta distribution *prior*, which is then multiplied with the *likelihood* to create a *posterior* that is also a beta distribution, but with updated parameters (we will not cover the math here). \n",
    "\n",
    "Activate the widget below to explore the variables, and follow the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.615035Z",
     "iopub.status.busy": "2021-07-02T12:04:39.614557Z",
     "iopub.status.idle": "2021-07-02T12:04:39.898518Z",
     "shell.execute_reply": "2021-07-02T12:04:39.899193Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "#@markdown Make sure you execute this cell to enable the widget\n",
    "\n",
    "#beta distribution\n",
    "#and binomial\n",
    "def plotFnc(p,n,priorL,priorR):\n",
    "  # Set random seed\n",
    "  np.random.seed(1)\n",
    "  #sample from binomial\n",
    "  numL = np.random.binomial(n, p, 1)\n",
    "  numR = n - numL\n",
    "  stepSize=0.001\n",
    "  x = np.arange(0, 1, stepSize)\n",
    "  betaPdf=sp.stats.beta.pdf(x,numL+priorL,numR+priorR)\n",
    "  betaPrior=sp.stats.beta.pdf(x,priorL,priorR)\n",
    "  print(\"number of left \"+str(numL))\n",
    "  print(\"number of right \"+str(numR))\n",
    "  print(\" \")\n",
    "  print(\"max likelihood \"+str(numL/(numL+numR)))\n",
    "  print(\" \")\n",
    "  print(\"max posterior \" + str(x[np.argmax(betaPdf)]))\n",
    "  print(\"mean posterior \" + str(np.mean(betaPdf*x)))\n",
    "\n",
    "\n",
    "  print(\" \")\n",
    "\n",
    "  with plt.xkcd():\n",
    "    #rng.beta()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "    ax.set_xlabel('p')\n",
    "    ax.set_ylabel('probability density')\n",
    "    plt.plot(x,betaPdf, label = \"Posterior\")\n",
    "    plt.plot(x,betaPrior, label = \"Prior\")\n",
    "    #print(int(len(betaPdf)/2))\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "interact(plotFnc, p=(0, 1, 0.01),n=(1, 50, 1), priorL=(1, 10, 1),priorR=(1, 10, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows you the prior distribution (i.e. before any data) and the posterior distribution (after data), with a summary of the data (number of left and right moves) and the maximum likelihood, maximum posterior and mean of the posterior. Dependent on the purpose either the mean or the max of the posterior can be useful as a 'single-number' summary of the posterior.\n",
    "Once you are familiar with the sliders and what they represent, go through these instructions.\n",
    "\n",
    "**For $p=0.5$**\n",
    "\n",
    "- Set $p=0.5$ and start off with a \"flat\" prior (`priorL=0`, `priorR=0`). Note that the prior distribution (orange) is flat, also known as uniformative. In this case the maximum likelihood and maximum posterior will get you almost identical results as you vary the number of datapoints ($n$) and the probability of the rat going left. However the posterior is a full distribution and not just a single point estimate.\n",
    "\n",
    "- As $n$ gets large you will also notice that the estimate (max likelihood or max posterior) changes less for each change in $n$, i.e. the estimation stabilises.\n",
    "\n",
    "- How many data points do you need think is needed for the probability estimate to stabilise? Note that this depends on how large fluctuations you are willing to accept.\n",
    "\n",
    "- Try increasing the strength of the prior, `priorL=10` and `priorR=10`. You will see that the prior distribution becomes more 'peaky'. In short this prior means that small or large values of $p$ are conidered very unlikely. Try playing with the number of data points $n$, you should find that the prior stabilises/regularises the maximum posterior estimate so that it does not move as much. \n",
    "\n",
    "**For $p=0.2$**\n",
    "\n",
    "Try the same as you just did, now with $p=0.2$,\n",
    "do you notice any differences? Note that the prior (assumeing equal chance Left and Right) is now badly matched to the data. Do the maximum likelihood and maximum posterior still give similar results, for a weak prior? For a strong prior? Does the prior still have a stabilising effect on the estimate?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take-away message:** \n",
    "Bayesian inference gives you a full distribution over the variables that you are inferring, can help regularise inference when you have limited data, and allows you to build more complex models that better reflects true causality (see bonus below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think! 3.2: Bayesian Brains\n",
    "Bayesian inference can help you when doing data analysis, especially when you only have little data. But consider whether the brain might be able to benefit from this too. If the brain needs to make inferences about the world, would it be useful to do regularisation on the input? Maybe there are times where having a full probability distribution could be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.902099Z",
     "iopub.status.busy": "2021-07-02T12:04:39.901639Z",
     "iopub.status.idle": "2021-07-02T12:04:39.906126Z",
     "shell.execute_reply": "2021-07-02T12:04:39.905736Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\" You will learn more about \"Bayesian brains\" and the theory surrounding\n",
    "these ideas once the course begins. Here is a brief explanation: it may\n",
    "be ideal for human brains to implement Bayesian inference by integrating \"prior\"\n",
    "information the brain has about the world (memories, prior knowledge, etc.) with\n",
    "new evidence that updates its \"beliefs\"/prior. This process seems to parallel\n",
    "the brain's method of learning about its environment, making it a compelling\n",
    "theory for many neuroscience researchers. One of Bonus exercises below examines a possible\n",
    "real world model for Bayesian inference: sound localization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.916369Z",
     "iopub.status.busy": "2021-07-02T12:04:39.913129Z",
     "iopub.status.idle": "2021-07-02T12:04:39.967008Z",
     "shell.execute_reply": "2021-07-02T12:04:39.964015Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 7: Summary\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "      def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "          self.id=id\n",
    "          src = 'https://player.bilibili.com/player.html?bvid={0}&page={1}'.format(id, page)\n",
    "          super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=\"BV1qB4y1K7WZ\", width=854, height=480, fs=1)\n",
    "  print('Video available at https://www.bilibili.com/video/{0}'.format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=\"OJN7ri3_FCA\", width=854, height=480, fs=1, rel=0)\n",
    "  print('Video available at https://youtube.com/watch?v=' + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Having done the different exercises you should now:\n",
    "* understand what the likelihood function is, and have some intuition of why it is important\n",
    "* know how to summarise the Gaussian distribution using mean and variance \n",
    "* know how to maximise a likelihood function\n",
    "* be able to do simple inference in both classical and Bayesian ways\n",
    "\n",
    "For more resources see \n",
    "https://github.com/NeuromatchAcademy/precourse/blob/master/resources.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Coding Exercise 1: Finding the posterior computationally   \n",
    "\n",
    "Imagine an experiment where participants estimate the location of a noise-emitting object. To estimate its position, the participants can use two sources of information: \n",
    "  1. new noisy auditory information (the likelihood)\n",
    "  2. prior visual expectations of where the stimulus is likely to come from (visual prior). \n",
    "\n",
    "The auditory and visual information are both noisy, so participants will combine these sources of information to better estimate the position of the object.\n",
    "\n",
    "We will use Gaussian distributions to represent the auditory likelihood (in red), and a Gaussian visual prior (expectations - in blue). Using Bayes rule, you will combine them into a posterior distribution that summarizes the probability that the object is in each possible location. \n",
    "\n",
    "We have provided you with a ready-to-use plotting function, and a code skeleton.\n",
    "\n",
    "* You can use `my_gaussian` from Tutorial 1 (also included below), to generate an auditory likelihood with parameters $\\mu$ = 3 and $\\sigma$ = 1.5\n",
    "* Generate a visual prior with parameters $\\mu$ = -1 and $\\sigma$ = 1.5\n",
    "* Calculate the posterior using pointwise multiplication of the likelihood and prior. Don't forget to normalize so the posterior adds up to 1\n",
    "* Plot the likelihood, prior and posterior using the predefined function `posterior_plot`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:39.978912Z",
     "iopub.status.busy": "2021-07-02T12:04:39.977846Z",
     "iopub.status.idle": "2021-07-02T12:04:39.979458Z",
     "shell.execute_reply": "2021-07-02T12:04:39.979840Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_gaussian(x_points, mu, sigma):\n",
    "  \"\"\" Returns normalized Gaussian estimated at points `x_points`, with parameters:\n",
    "  mean `mu` and standard deviation `sigma`\n",
    "\n",
    "  Args:\n",
    "      x_points (ndarray of floats): points at which the gaussian is evaluated\n",
    "      mu (scalar): mean of the Gaussian\n",
    "      sigma (scalar): standard deviation of the gaussian\n",
    "\n",
    "  Returns:\n",
    "      (numpy array of floats) : normalized Gaussian evaluated at `x`\n",
    "  \"\"\"\n",
    "  px = 1/(2*np.pi*sigma**2)**1/2 *np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
    "\n",
    "  # as we are doing numerical integration we may have to remember to normalise\n",
    "  # taking into account the stepsize (0.1)\n",
    "  px = px/(0.1*sum(px))\n",
    "  return px\n",
    "\n",
    "def compute_posterior_pointwise(prior, likelihood):\n",
    "  \"\"\" Compute the posterior probability distribution point-by-point using Bayes\n",
    "  Rule.\n",
    "\n",
    "    Args:\n",
    "      prior (ndarray): probability distribution of prior\n",
    "      likelihood (ndarray): probability distribution of likelihood\n",
    "\n",
    "    Returns:\n",
    "      posterior (ndarray): probability distribution of posterior\n",
    "  \"\"\"\n",
    "  ##############################################################################\n",
    "  # TODO for students: Write code to compute the posterior from the prior and\n",
    "  # likelihood via pointwise multiplication. (You may assume both are defined\n",
    "  # over the same x-axis)\n",
    "  #\n",
    "  # Comment out the line below to test your solution\n",
    "  raise NotImplementedError(\"Finish the simulation code first\")\n",
    "  ##############################################################################\n",
    "\n",
    "  posterior = ...\n",
    "\n",
    "  return posterior\n",
    "\n",
    "def localization_simulation(mu_auditory = 3.0, sigma_auditory = 1.5,\n",
    "                            mu_visual = -1.0, sigma_visual = 1.5):\n",
    "  \"\"\" Perform a sound localization simulation with an auditory prior.\n",
    "\n",
    "    Args:\n",
    "      mu_auditory (float): mean parameter value for auditory prior\n",
    "      sigma_auditory (float): standard deviation parameter value for auditory\n",
    "                                prior\n",
    "      mu_visual (float): mean parameter value for visual likelihood distribution\n",
    "      sigma_visual (float): standard deviation parameter value for visual\n",
    "                                likelihood distribution\n",
    "\n",
    "    Returns:\n",
    "      x (ndarray): range of values for which to compute probabilities\n",
    "      auditory (ndarray): probability distribution of the auditory prior\n",
    "      visual (ndarray): probability distribution of the visual likelihood\n",
    "      posterior_pointwise (ndarray): posterior probability distribution\n",
    "  \"\"\"\n",
    "  ##############################################################################\n",
    "  ## Using the x variable below,\n",
    "  ##      create a gaussian called 'auditory' with mean 3, and std 1.5\n",
    "  ##      create a gaussian called 'visual' with mean -1, and std 1.5\n",
    "  #\n",
    "  #\n",
    "  ## Comment out the line below to test your solution\n",
    "  raise NotImplementedError(\"Finish the simulation code first\")\n",
    "  ###############################################################################\n",
    "  x = np.arange(-8, 9, 0.1)\n",
    "\n",
    "  auditory = ...\n",
    "  visual = ...\n",
    "  posterior = compute_posterior_pointwise(auditory, visual)\n",
    "\n",
    "  return x, auditory, visual, posterior\n",
    "\n",
    "\n",
    "# Uncomment the lines below to plot the results\n",
    "# x, auditory, visual, posterior_pointwise = localization_simulation()\n",
    "# _ = posterior_plot(x, auditory, visual, posterior_pointwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:40.000161Z",
     "iopub.status.busy": "2021-07-02T12:04:39.998230Z",
     "iopub.status.idle": "2021-07-02T12:04:40.279555Z",
     "shell.execute_reply": "2021-07-02T12:04:40.279096Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def my_gaussian(x_points, mu, sigma):\n",
    "  \"\"\" Returns normalized Gaussian estimated at points `x_points`, with parameters:\n",
    "  mean `mu` and standard deviation `sigma`\n",
    "\n",
    "  Args:\n",
    "      x_points (ndarray of floats): points at which the gaussian is evaluated\n",
    "      mu (scalar): mean of the Gaussian\n",
    "      sigma (scalar): standard deviation of the gaussian\n",
    "\n",
    "  Returns:\n",
    "      (numpy array of floats) : normalized Gaussian evaluated at `x`\n",
    "  \"\"\"\n",
    "  px = 1/(2*np.pi*sigma**2)**1/2 *np.exp(-(x_points-mu)**2/(2*sigma**2))\n",
    "\n",
    "  # as we are doing numerical integration we may have to remember to normalise\n",
    "  # taking into account the stepsize (0.1)\n",
    "  px = px/(0.1*sum(px))\n",
    "  return px\n",
    "\n",
    "def compute_posterior_pointwise(prior, likelihood):\n",
    "  \"\"\" Compute the posterior probability distribution point-by-point using Bayes\n",
    "  Rule.\n",
    "\n",
    "    Args:\n",
    "      prior (ndarray): probability distribution of prior\n",
    "      likelihood (ndarray): probability distribution of likelihood\n",
    "\n",
    "    Returns:\n",
    "      posterior (ndarray): probability distribution of posterior\n",
    "  \"\"\"\n",
    "\n",
    "  posterior = likelihood * prior\n",
    "  posterior =posterior/ (0.1*posterior.sum())\n",
    "\n",
    "  return posterior\n",
    "\n",
    "def localization_simulation(mu_auditory = 3.0, sigma_auditory = 1.5,\n",
    "                            mu_visual = -1.0, sigma_visual = 1.5):\n",
    "  \"\"\" Perform a sound localization simulation with an auditory prior.\n",
    "\n",
    "    Args:\n",
    "      mu_auditory (float): mean parameter value for auditory prior\n",
    "      sigma_auditory (float): standard deviation parameter value for auditory\n",
    "                                prior\n",
    "      mu_visual (float): mean parameter value for visual likelihood distribution\n",
    "      sigma_visual (float): standard deviation parameter value for visual\n",
    "                                likelihood distribution\n",
    "\n",
    "    Returns:\n",
    "      x (ndarray): range of values for which to compute probabilities\n",
    "      auditory (ndarray): probability distribution of the auditory prior\n",
    "      visual (ndarray): probability distribution of the visual likelihood\n",
    "      posterior_pointwise (ndarray): posterior probability distribution\n",
    "  \"\"\"\n",
    "\n",
    "  x = np.arange(-8, 9, 0.1)\n",
    "\n",
    "  auditory = my_gaussian(x, mu_auditory, sigma_auditory)\n",
    "  visual = my_gaussian(x, mu_visual, mu_visual)\n",
    "  posterior = compute_posterior_pointwise(auditory, visual)\n",
    "\n",
    "  return x, auditory, visual, posterior\n",
    "\n",
    "# Uncomment the lines below to plot the results\n",
    "x, auditory, visual, posterior_pointwise = localization_simulation()\n",
    "with plt.xkcd():\n",
    "  _ = posterior_plot(x, auditory, visual, posterior_pointwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the the visual and auditory information could help the brain get a better estimate of the location of an audio-visual object, with lower variance. \n",
    "\n",
    "**Main course preview:** On Week 3 Day 1 (W3D1) there will be a whole day devoted to examining whether the brain uses Bayesian inference. Is the brain Bayesian?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Coding Exercise 2: Bayes Net\n",
    "If you have the time, here is another extra exercise.\n",
    "\n",
    "Bayes Net, or Bayesian Belief Networks, provide a way to make inferences about multiple levels of information, which would be very difficult to do in a classical frequentist paradigm.\n",
    "\n",
    "We can encapsulate our knowledge about causal relationships and use this to make inferences about hidden properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try a simple example of a Bayesian Net (aka belief network). Imagine that you have a house with an unreliable sprinkler system installed for watering the grass. This is set to water the grass independently of whether it has rained that day. We have three variables, rain ($r$), sprinklers ($s$) and wet grass ($w$). Each of these can be true (1) or false (0). See the graphical model representing the relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a table below describing all the relationships between $w, r$, and s$.\n",
    "\n",
    "Obviously the grass is more likely to be wet if either the sprinklers were on or it was raining. On any given day the sprinklers have probability 0.25 of being on, $P(s = 1) = 0.25$, while there is a probability 0.1 of rain, $P (r = 1) = 0.1$. The table then lists the conditional probabilities for the given being wet, given a rain and sprinkler condition for that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{|l |  l || ll |} \\hline\n",
    "r &s&P(w=0|r,s) &P(w=1|r,s)$\\\\ \\hline\n",
    "0& 0  &0.999 &0.001\\\\\n",
    "0& 1 &0.1& 0.9\\\\\n",
    "1& 0 &0.01 &0.99\\\\\n",
    "1& 1& 0.001 &0.999\\\\ \\hline\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You come home and find that the the grass is wet, what is the probability the sprinklers were on today (you do not know if it was raining)?\n",
    "\n",
    "We can start by writing out the joint probability:\n",
    "$P(r,w,s)=P(w|r,s)P(r)P(s)$\n",
    "\n",
    "The conditional probability is then:\n",
    "\n",
    "$\n",
    "P(s|w)=\\frac{\\sum_{r} P(w|s,r)P(s)  P(r)}{P(w)}=\\frac{P(s) \\sum_{r} P(w|s,r) P(r)}{P(w)}\n",
    "$\n",
    "\n",
    "Note that we are summing over all possible conditions for $r$ as we do not know if it was raining. Specifically, we want to know the probability of sprinklers having been on given the wet grass, $P(s=1|w=1)$:\n",
    "\n",
    "$\n",
    "P(s=1|w=1)=\\frac{P(s = 1)( P(w = 1|s = 1, r = 1) P(r = 1)+ P(w = 1|s = 1,r = 0)  P(r = 0))}{P(w = 1)} \n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(w=1)=P(s=1)( P(w=1|s=1,r=1 ) P(r=1) &+ P(w=1|s=1,r=0)  P(r=0))\\\\\n",
    "+P(s=0)( P(w=1|s=0,r=1 )  P(r=1) &+ P(w=1|s=0,r=0)  P(r=0))\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "This code has been written out below, you just need to insert the right numbers from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:40.286183Z",
     "iopub.status.busy": "2021-07-02T12:04:40.285699Z",
     "iopub.status.idle": "2021-07-02T12:04:40.291165Z",
     "shell.execute_reply": "2021-07-02T12:04:40.290670Z"
    }
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO for student: Write code to insert the correct conditional probabilities\n",
    "# from the table; see the comments to match variable with table entry.\n",
    "# Comment out the line below to test your solution\n",
    "raise NotImplementedError(\"Finish the simulation code first\")\n",
    "##############################################################################\n",
    "\n",
    "Pw1r1s1 = ...  # the probability of wet grass given rain and sprinklers on\n",
    "Pw1r1s0 = ...  # the probability of wet grass given rain and sprinklers off\n",
    "Pw1r0s1 = ...  # the probability of wet grass given no rain and sprinklers on\n",
    "Pw1r0s0 = ...  # the probability of wet grass given no rain and sprinklers off\n",
    "Ps = ... # the probability of the sprinkler being on\n",
    "Pr = ... # the probability of rain that day\n",
    "\n",
    "\n",
    "# Uncomment once variables are assigned above\n",
    "# A= Ps * (Pw1r1s1 * Pr + (Pw1r0s1) * (1 - Pr))\n",
    "# B= (1 - Ps) * (Pw1r1s0 *Pr + (Pw1r0s0) * (1 - Pr))\n",
    "# print(\"Given that the grass is wet, the probability the sprinkler was on is: \" +\n",
    "#       str(A/(A + B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T12:04:40.297312Z",
     "iopub.status.busy": "2021-07-02T12:04:40.296230Z",
     "iopub.status.idle": "2021-07-02T12:04:40.298857Z",
     "shell.execute_reply": "2021-07-02T12:04:40.298457Z"
    }
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "Pw1r1s1 = 0.999  # the probability of wet grass given rain and sprinklers on\n",
    "Pw1r1s0 = 0.99   # the probability of wet grass given rain and sprinklers off\n",
    "Pw1r0s1 = 0.9    # the probability of wet grass given no rain and sprinklers on\n",
    "Pw1r0s0 = 0.001  # the probability of wet grass given no rain and sprinklers off\n",
    "Ps = 0.25  # the probability of the sprinkler being on\n",
    "Pr = 0.1   # the probability of rain that day\n",
    "\n",
    "# Uncomment once variables are assigned above\n",
    "A= Ps * (Pw1r1s1 * Pr + (Pw1r0s1) * (1 - Pr))\n",
    "B= (1 - Ps) * (Pw1r1s0 *Pr + (Pw1r0s0) * (1 - Pr))\n",
    "print(\"Given that the grass is wet, the probability the sprinkler was on is: \" +\n",
    "      str(A/(A + B)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability you should get is about 0.7522.\n",
    "\n",
    "Your neighbour now tells you that it was indeed \n",
    "raining today, $P (r = 1) = 1$, so what is now the probability the sprinklers were on? Try changing the numbers above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Think!: Causality in the Brain\n",
    "\n",
    "In a causal stucture this is the correct way to calculate the probabilities. Do you think this is how the brain solves such problems? Would it be different for task involving novel stimuli (e.g. for someone with no previous exposure to sprinklers), as opposed to common stimuli?\n",
    "\n",
    "**Main course preview:** On W3D5 we will discuss causality further!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W0D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
