
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neuromatch Academy: Week 1, Day 4, Tutorial 2 &#8212; Neuromatch Computational Neuroscience</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="W1D5 - Dimensionality Reduction" href="../W1D5_DimensionalityReduction/README.html" />
    <link rel="prev" title="Neuromatch Academy: Week 1, Day 4, Tutorial 1" href="W1D4_Tutorial1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   NMA 2021
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preliminary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W0D1_PythonWorkshop1/README.html">
   Python Workshop 1
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W0D1_PythonWorkshop1/W0D1_Tutorial1.html">
     Neuromatch Academy: Week 0, Day 1, Tutorial 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W0D2_PythonWorkshop2/README.html">
   Python Workshop 2
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W0D2_PythonWorkshop2/W0D2_Tutorial1.html">
     Neuromatch Academy: Week 0, Day 2, Tutorial 1
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Week 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D1_ModelTypes/README.html">
   Model Types
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D2_ModelingPractice/README.html">
   Modeling Practice
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D2_ModelingPractice/W1D2_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 2, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D2_ModelingPractice/W1D2_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 2, Tutorial 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D3_ModelFitting/README.html">
   Model Fitting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial4.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial5.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D3_ModelFitting/W1D3_Tutorial6.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 6
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="README.html">
   Machine Learning
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="W1D4_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 4, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Neuromatch Academy: Week 1, Day 4, Tutorial 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D5_DimensionalityReduction/README.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial4.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 4
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Week 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D2_LinearSystems/README.html">
   Linear Systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial1.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial2.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial3.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial4.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D3_DecisionMaking/README.html">
   Decision Making
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D4_OptimalControl/README.html">
   Optimal Control
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/tutorials/W1D4_MachineLearning/W1D4_Tutorial2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/NeuromatchAcademy/course_content/blob/master/book/tutorials/W1D4_MachineLearning/W1D4_Tutorial2.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neuromatch Academy: Week 1, Day 4, Tutorial 2
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-classifiers-and-regularizers">
   Machine Learning: Classifiers and regularizers
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-1-the-logistic-regression-model">
     Section 1.1: The logistic regression model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-1-implement-the-sigmoid-function">
       Exercise 1: implement the sigmoid function
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-1-2-using-scikit-learn">
     Section 1.2: Using scikit-learn
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-1-setting-up-the-data">
     Section 2.1: Setting up the data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-format">
       Data format
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-2-fitting-the-model">
     Section 2.2: Fitting the model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-3-classifying-the-training-data">
     Section 2.3: Classifying the training data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-4-evaluating-the-model">
     Section 2.4: Evaluating the model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-2-classifier-accuracy">
       Exercise 2: classifier accuracy
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-5-cross-validating-the-classifer">
     Section 2.5: Cross-validating the classifer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cross-validating-using-scikit-learn-helper-functions">
       Cross-validating using
       <code class="docutils literal notranslate">
        <span class="pre">
         scikit-learn
        </span>
       </code>
       helper functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-more-features-than-samples-leads-to-overfitting">
       Why more features than samples leads to overfitting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#what-we-can-do-about-it">
       What we can do about it
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-3-1-l-2-regularization">
     Section 3.1:
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     regularization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interactive-demo-the-effect-of-varying-c-on-parameter-size">
       Interactive Demo: The effect of varying C on parameter size
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-3-2-l-1-regularization">
     Section 3.2:
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-4-the-key-difference-between-l-1-and-l-2-regularization-sparsity">
   Section 4: The key difference between
   <span class="math notranslate nohighlight">
    \(L_1\)
   </span>
   and
   <span class="math notranslate nohighlight">
    \(L_2\)
   </span>
   regularization: sparsity
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3-the-effect-of-l-1-regularization-on-parameter-sparsity">
     Exercise 3: The effect of
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     regularization on parameter sparsity
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-4-1-choosing-the-regularization-penalty">
     Section 4.1: Choosing the regularization penalty
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-4-model-selection">
       Exercise 4: Model selection
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-the-logistic-regression-model-in-full">
   Appendix: The Logistic Regression model in full
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-logistic-link-function">
     The logistic link function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-bernoulli-likelihood">
     The Bernoulli likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-more-detail-about-model-selection">
   Appendix: More detail about model selection
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D4_MachineLearning/W1D4_Tutorial2.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="neuromatch-academy-week-1-day-4-tutorial-2">
<h1>Neuromatch Academy: Week 1, Day 4, Tutorial 2<a class="headerlink" href="#neuromatch-academy-week-1-day-4-tutorial-2" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="machine-learning-classifiers-and-regularizers">
<h1>Machine Learning: Classifiers and regularizers<a class="headerlink" href="#machine-learning-classifiers-and-regularizers" title="Permalink to this headline">¶</a></h1>
<p><strong>Content creators:</strong> Pierre-Etienne H. Fiquet, Ari Benjamin, Jakob Macke</p>
<p><strong>Content reviewers:</strong> Davide Valeriani, Alish Dipani, Michael Waskom</p>
<p>This is part 2 of a 2-part series about Generalized Linear Models (GLMs), which are a fundamental framework for supervised learning. In part 1, we learned about and implemented GLMs. In this tutorial, we’ll implement logistic regression, a special case of GLMs used to model binary outcomes.
Oftentimes the variable you would like to predict takes only one of two possible values. Left or right? Awake or asleep? Car or bus? In this tutorial, we will decode a mouse’s left/right decisions from spike train data. Our objectives are to:</p>
<ol class="simple">
<li><p>Learn about logistic regression, how it is derived within the GLM theory, and how it is implemented in scikit-learn</p></li>
<li><p>Apply logistic regression to decode choies from neural responses</p></li>
<li><p>Learn about regularization, including the different approaches and the influence of hyperparameters</p></li>
</ol>
<hr class="docutils" />
<p>We would like to acknowledge <a class="reference external" href="https://www.nature.com/articles/s41586-019-1787-x">Steinmetz <em>et al.</em> (2019)</a> for sharing their data, a subset of which is used here.</p>
</div>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper functions</span>

<span class="k">def</span> <span class="nf">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Draw a stem plot of weights for each model in models dict.&quot;&quot;&quot;</span>
  <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
  <span class="n">axs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">sharey</span><span class="p">)</span>
  <span class="n">axs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=.</span><span class="mi">02</span><span class="p">)</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_marker</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s2">&quot;.2&quot;</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_linewidths</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s2">&quot;.2&quot;</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C3&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Weight&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Neuron (a.k.a. feature)&quot;</span><span class="p">)</span>
  <span class="n">f</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate f() on linear space between points and plot.</span>

<span class="sd">    Args:</span>
<span class="sd">      f (callable): function that maps scalar -&gt; scalar</span>
<span class="sd">      name (string): Function name for axis labels</span>
<span class="sd">      var (string): Variable name for axis labels.</span>
<span class="sd">      points (tuple): Args for np.linspace to create eval grid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">points</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(</span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s1">)$&#39;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_model_selection</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plot the accuracy curve over log-spaced C values.&quot;&quot;&quot;</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
  <span class="n">best_C</span> <span class="o">=</span> <span class="n">C_values</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)]</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
      <span class="n">xticks</span><span class="o">=</span><span class="n">C_values</span><span class="p">,</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;$C$&quot;</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Cross-validated accuracy&quot;</span><span class="p">,</span>
      <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best C: </span><span class="si">{</span><span class="n">best_C</span><span class="si">:</span><span class="s2">1g</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
  <span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_non_zero_coefs</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">n_voxels</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plot the accuracy curve over log-spaced C values.&quot;&quot;&quot;</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xticks</span><span class="o">=</span><span class="n">C_values</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;$C$&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Number of non-zero coefficients&quot;</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">n_voxels</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;.1&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Total</span><span class="se">\n</span><span class="s2"># Neurons&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">C_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_voxels</span> <span class="o">*</span> <span class="o">.</span><span class="mi">98</span><span class="p">),</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;top&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Data retrieval and loading</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">hashlib</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://osf.io/r9gh8/download&quot;</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;W1D4_steinmetz_data.npz&quot;</span>
<span class="n">expected_md5</span> <span class="o">=</span> <span class="s2">&quot;d19716354fed0981267456b80db07ea8&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;!!! Failed to download data !!!&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;!!! Failed to download data !!!&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;!!! Data download appears corrupted !!!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
        <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_steinmetz_data</span><span class="p">(</span><span class="n">data_fname</span><span class="o">=</span><span class="n">fname</span><span class="p">):</span>

  <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">dobj</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">dobj</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p>#Section 1: Logistic regression</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 1: Logistic regression</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;qfXFrUnLU0o&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtube.com/watch?v=&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>Logistic Regression is a binary classification model. It is a GLM with a <em>logistic</em> link function and a <em>Bernoulli</em> (i.e. coinflip) noise model.</p>
<p>Like in the last notebook, logistic regression invokes a standard procedure:</p>
<ol class="simple">
<li><p>Define a <em>model</em> of how inputs relate to outputs.</p></li>
<li><p>Adjust the parameters to maximize (log) probability of your data given your model</p></li>
</ol>
<div class="section" id="section-1-1-the-logistic-regression-model">
<h2>Section 1.1: The logistic regression model<a class="headerlink" href="#section-1-1-the-logistic-regression-model" title="Permalink to this headline">¶</a></h2>
<p>The fundamental input/output equation of logistic regression is:</p>
<div class="math notranslate nohighlight">
\[ \hat{y} \equiv p(y=1|x,\theta) = \sigma(\theta^Tx)\]</div>
<p>Note that we interpret the output of logistic regression, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, as the <strong>probability that y = 1</strong> given inputs <span class="math notranslate nohighlight">\(x\)</span> and parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Here <span class="math notranslate nohighlight">\(\sigma()\)</span> is a “squashing” function called the <strong>sigmoid function</strong> or <strong>logistic function</strong>. Its output is in the range <span class="math notranslate nohighlight">\(0 \leq y \leq 1\)</span>. It looks like this:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + \textrm{exp}(-z)}\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(z = \theta^T x\)</span>. The parameters decide whether <span class="math notranslate nohighlight">\(\theta^T x\)</span> will be very negative, in which case <span class="math notranslate nohighlight">\(\sigma(\theta^T x)\approx 0\)</span>, or very positive, meaning  <span class="math notranslate nohighlight">\(\sigma(\theta^T x)\approx 1\)</span>.</p>
<div class="section" id="exercise-1-implement-the-sigmoid-function">
<h3>Exercise 1: implement the sigmoid function<a class="headerlink" href="#exercise-1-implement-the-sigmoid-function" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the logistic transform of z.&quot;&quot;&quot;</span>
  <span class="c1">##############################################################################</span>
  <span class="c1"># TODO for students: Fill in the missing code (...) and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Student excercise: implement the sigmoid function&quot;</span><span class="p">)</span>
  <span class="c1">##############################################################################</span>
  <span class="k">return</span> <span class="o">...</span>

<span class="c1"># Uncomment to test your sigmoid function</span>
<span class="c1"># plot_function(sigmoid, &quot;\sigma&quot;, &quot;z&quot;, (-10, 10))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return the logistic transform of z.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_function</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="s2">&quot;\sigma&quot;</span><span class="p">,</span> <span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-2-using-scikit-learn">
<h2>Section 1.2: Using scikit-learn<a class="headerlink" href="#section-1-2-using-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>Unlike the previous notebook, we’re not going to write the code that implements all of the Logistic Regression model itself. Instead, we’re going to use the implementation in <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a>, a very popular library for Machine Learning.</p>
<p>The goal of this next section is to introduce <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> classifiers and understand how to apply it to real neural data.</p>
<p>#Section 2: Decoding neural data with logistic regression</p>
</div>
<div class="section" id="section-2-1-setting-up-the-data">
<h2>Section 2.1: Setting up the data<a class="headerlink" href="#section-2-1-setting-up-the-data" title="Permalink to this headline">¶</a></h2>
<p>In this notebook we’ll use the Steinmetz dataset that you have seen previously. Recall that this dataset includes recordings of neurons as mice perform a decision task.</p>
<p>Mice had the task of turning a wheel to indicate whether they perceived a Gabor stimulus to the left, to the right, or not at all. Neuropixel probes measured spikes across the cortex. Check out the following task schematic from the BiorXiv preprint:</p>
<img src='http://kordinglab.com/images/others/steinmetz-task.png' width= '200'/>
<p>Today we’re going to <strong>decode the decision from neural data</strong> using Logistic Regression. We will only consider trials where the mouse chose “Left” or “Right” and ignore NoGo trials.</p>
<div class="section" id="data-format">
<h3>Data format<a class="headerlink" href="#data-format" title="Permalink to this headline">¶</a></h3>
<p>In the hidden <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">retrieval</span> <span class="pre">and</span> <span class="pre">loading</span></code> cell, there is a function that loads the data:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">spikes</span></code>: an array of normalized spike rates with shape <code class="docutils literal notranslate"><span class="pre">(n_trials,</span> <span class="pre">n_neurons)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">choices</span></code>: a vector of 0s and 1s, indicating the animal’s behavioral response, with length <code class="docutils literal notranslate"><span class="pre">n_trials</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">load_steinmetz_data</span><span class="p">()</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As with the GLMs you’ve seen in the previous tutorial (Linear and Poisson Regression), we will need two data structures:</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">X</span></code> matrix with shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code></p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">y</span></code> vector with length <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>.</p></li>
</ul>
<p>In the previous notebook, <code class="docutils literal notranslate"><span class="pre">y</span></code> corresponded to the neural data, and <code class="docutils literal notranslate"><span class="pre">X</span></code> corresponded to something about the experiment. Here, we are going to invert those relationships. That’s what makes this a <em>decoding</em> model: we are going to predict behavior (<code class="docutils literal notranslate"><span class="pre">y</span></code>) from the neural responses (<code class="docutils literal notranslate"><span class="pre">X</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;spikes&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-fitting-the-model">
<h2>Section 2.2: Fitting the model<a class="headerlink" href="#section-2-2-fitting-the-model" title="Permalink to this headline">¶</a></h2>
<p>Using a Logistic Regression model within <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is very simple.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First define the model</span>
<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="c1">#Then fit it to data</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There’s two steps here:</p>
<ul class="simple">
<li><p>We <em>initialized</em> the model with a hyperparameter, telling it what penalty to use (we’ll focus on this in the second part of the notebook)</p></li>
<li><p>We <em>fit</em> the model by passing it the <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> objects.</p></li>
</ul>
</div>
<div class="section" id="section-2-3-classifying-the-training-data">
<h2>Section 2.3: Classifying the training data<a class="headerlink" href="#section-2-3-classifying-the-training-data" title="Permalink to this headline">¶</a></h2>
<p>Fitting the model performs maximum likelihood optimization, learning a set of <em>feature weights</em>. We can use those learned weights to <em>classify</em> new data, or predict the labels for each sample:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-4-evaluating-the-model">
<h2>Section 2.4: Evaluating the model<a class="headerlink" href="#section-2-4-evaluating-the-model" title="Permalink to this headline">¶</a></h2>
<p>Now we need to evaluate the model’s predictions. We’ll do that with an <em>accuracy</em> score. The accuracy of the classifier is the proportion of trials where the predicted label matches the true label.</p>
<div class="section" id="exercise-2-classifier-accuracy">
<h3>Exercise 2: classifier accuracy<a class="headerlink" href="#exercise-2-classifier-accuracy" title="Permalink to this headline">¶</a></h3>
<p>For the first exercise, implement a function to evaluate a classifier using the accuracy score. Use it to get the accuracy of the classifier on the <em>training</em> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute accuracy of classifier predictions.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    model (sklearn estimator): Classifier with trained weights.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracy (float): Proportion of correct predictions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function, then remove the next line to test it</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Implement the compute_accuracy function&quot;</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">accuracy</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">accuracy</span>

<span class="c1"># Uncomment and run to test your function:</span>
<span class="c1"># train_accuracy = compute_accuracy(X, y, log_reg)</span>
<span class="c1"># print(f&quot;Accuracy on the training data: {train_accuracy:.2%}&quot;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute accuracy of classifier predictions.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    model (sklearn estimator): Classifier with trained weights.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracy (float): Proportion of correct predictions.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">accuracy</span>

<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">log_reg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy on the training data: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-5-cross-validating-the-classifer">
<h2>Section 2.5: Cross-validating the classifer<a class="headerlink" href="#section-2-5-cross-validating-the-classifer" title="Permalink to this headline">¶</a></h2>
<p>Classification accuracy on the training data is 100%! That might sound impressive, but you should recall from yesterday the concept of <em>overfitting</em>: the classifier may have learned something idiosyncratic about the training data. If that’s the case, it won’t have really learned the underlying data-&gt;decision function, and thus won’t generalize well to new data.</p>
<p>To check this, we can evaluate the <em>cross-validated</em> accuracy.</p>
<img src='http://kordinglab.com/images/others/justCV-01.png' width= '700'/><div class="section" id="cross-validating-using-scikit-learn-helper-functions">
<h3>Cross-validating using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> helper functions<a class="headerlink" href="#cross-validating-using-scikit-learn-helper-functions" title="Permalink to this headline">¶</a></h3>
<p>Yesterday, we asked you to write your own functions for implementing cross-validation. In practice, this won’t be necessary, because <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> offers a number of <a class="reference external" href="https://scikit-learn.org/stable/model_selection.html">helpful functions</a> that will do this for you. For example, you can cross-validate a classifer using <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> takes a <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> model like <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>, as well as your <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> data. It then retrains your model on test/train splits of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and returns the test accuracy on each of the test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># k=8 crossvalidation</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Run to plot out these `k=8` accuracy scores.</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">widths</span><span class="o">=.</span><span class="mi">7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span>
  <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span>
  <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Average test accuracy: </span><span class="si">{</span><span class="n">accuracies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The lower cross-validated accuracy compared to the training accuracy (100%) suggests that the model is being <em>overfit</em>. Is this surprising? Think about the shape of the <span class="math notranslate nohighlight">\(X\)</span> matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<p>The model has almost three times as many features as samples. This is a situation where overfitting is very likely (almost guaranteed).</p>
<p><strong>Link to neuroscience</strong>: Neuro data commonly has more features than samples. Having more neurons than independent trials is one example. In fMRI data, there are commonly more measured voxels than independent trials.</p>
</div>
<div class="section" id="why-more-features-than-samples-leads-to-overfitting">
<h3>Why more features than samples leads to overfitting<a class="headerlink" href="#why-more-features-than-samples-leads-to-overfitting" title="Permalink to this headline">¶</a></h3>
<p>In brief, the variance of model estimation increases when there are more features than samples. That is, you would get a very different model every time you get new data and run <code class="docutils literal notranslate"><span class="pre">.fit()</span></code>. This is very related to the <em>bias/variance tradeoff</em> you learned about on day 1.</p>
<p>Why does this happen? Here’s a tiny example to get your intuition going. Imagine trying to find a best-fit line in 2D when you only have 1 datapoint. There are simply a infinite number of lines that pass through that point. This is the situation we find ourselves in with more features than samples.</p>
</div>
<div class="section" id="what-we-can-do-about-it">
<h3>What we can do about it<a class="headerlink" href="#what-we-can-do-about-it" title="Permalink to this headline">¶</a></h3>
<p>As you learned on day 1, you can decrease model variance if you don’t mind increasing its bias. Here, we will increase bias by assuming that the correct parameters are all small. In our 2D example, this is like prefering the horizontal line to all others. This is one example of <em>regularization</em>.</p>
<hr class="docutils" />
<p>#Section 3: Regularization</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 2: Regularization</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;b2IaUCZ91bo&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtube.com/watch?v=&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>Regularization forces a model to learn a set solutions you <em>a priori</em> believe to be more correct, which reduces overfitting because it doesn’t have as much flexibility to fit idiosyncracies in the training data. This adds model bias, but it’s a good bias because you know (maybe) that parameters should be small or mostly 0.</p>
<p>In a GLM, a common form of regularization is to <em>shrink</em> the classifier weights. In a linear model, you can see its effect by plotting the weights. We’ve defined a helper function, <code class="docutils literal notranslate"><span class="pre">plot_weights</span></code>, that we’ll use extensively in this section.</p>
<p>Here is what the weights look like for a Logistic Regression model with no regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">({</span><span class="s2">&quot;No regularization&quot;</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>It’s important to understand this plot. Each dot visualizes a value in our parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>. (It’s the same style of plot as the one showing <span class="math notranslate nohighlight">\(\theta\)</span> in the video). Since each feature is the time-averaged response of a neuron, each dot shows how the model uses each neuron to estimate a decision.</p>
<p>Note the scale of the y-axis. Some neurons have values of about <span class="math notranslate nohighlight">\(20\)</span>, whereas others scale to <span class="math notranslate nohighlight">\(-20\)</span>.</p>
</div>
</div>
<div class="section" id="section-3-1-l-2-regularization">
<h2>Section 3.1: <span class="math notranslate nohighlight">\(L_2\)</span> regularization<a class="headerlink" href="#section-3-1-l-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>Regularization comes in different flavors. A very common one uses an <span class="math notranslate nohighlight">\(L_2\)</span> or “ridge” penalty. This changes the objective function to</p>
<div class="math notranslate nohighlight">
\[-\log\mathcal{L}'(\theta | X, y)=
-\log\mathcal{L}(\theta | X, y) +\frac\beta2\sum_i\theta_i^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a <em>hyperparameter</em> that sets the <em>strength</em> of the regularization.</p>
<p>You can use regularization in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> by changing the <code class="docutils literal notranslate"><span class="pre">penalty</span></code>, and you can set the strength of the regularization with the <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter (<span class="math notranslate nohighlight">\(C = \frac{1}{\beta}\)</span>, so this sets the <em>inverse</em> regularization).</p>
<p>Let’s compare the unregularized classifier weights with the classifier weights when we use the default <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg_l2</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># now show the two models</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;No regularization&quot;</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">,</span>
  <span class="s2">&quot;$L_2$ (C = 1)&quot;</span><span class="p">:</span> <span class="n">log_reg_l2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Using the same scale for the two y axes, it’s almost impossible to see the <span class="math notranslate nohighlight">\(L_2\)</span> weights. Let’s allow the y axis scales to adjust to each set of weights:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now you can see that the weights have the same basic pattern, but the regularized weights are an order-of-magnitude smaller.</p>
<div class="section" id="interactive-demo-the-effect-of-varying-c-on-parameter-size">
<h3>Interactive Demo: The effect of varying C on parameter size<a class="headerlink" href="#interactive-demo-the-effect-of-varying-c-on-parameter-size" title="Permalink to this headline">¶</a></h3>
<p>We can use this same approach to see how the weights depend on the <em>strength</em> of the regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Execute this cell to enable the widget!</span>

<span class="c1"># Precompute the models so the widget is responsive</span>
<span class="n">log_C_steps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">penalized_models</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">log_C</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">*</span><span class="n">log_C_steps</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">):</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
  <span class="n">penalized_models</span><span class="p">[</span><span class="n">log_C</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_observed</span><span class="p">(</span><span class="n">log_C</span><span class="o">=</span><span class="n">log_C_steps</span><span class="p">):</span>
  <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;No regularization&quot;</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">&quot;$L_2$ (C = $10^</span><span class="si">{</span><span class="n">log_C</span><span class="si">}</span><span class="s2">$)&quot;</span><span class="p">:</span> <span class="n">penalized_models</span><span class="p">[</span><span class="n">log_C</span><span class="p">]</span>
  <span class="p">}</span>
  <span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Recall from above that <span class="math notranslate nohighlight">\(C=\frac1\beta\)</span> so larger <code class="docutils literal notranslate"><span class="pre">C</span></code> is less regularization. The top panel corresponds to <span class="math notranslate nohighlight">\(C=\infty\)</span>.</p>
</div>
</div>
<div class="section" id="section-3-2-l-1-regularization">
<h2>Section 3.2: <span class="math notranslate nohighlight">\(L_1\)</span> regularization<a class="headerlink" href="#section-3-2-l-1-regularization" title="Permalink to this headline">¶</a></h2>
<p><span class="math notranslate nohighlight">\(L_2\)</span> is not the only option for regularization. There is also the <span class="math notranslate nohighlight">\(L_1\)</span>, or “Lasso” penalty. This changes the objective function to</p>
<div class="math notranslate nohighlight">
\[
-\log\mathcal{L}'(\theta | X, y)=
-\log\mathcal{L}(\theta | X, y) +\frac\beta2\sum_i|\theta_i|
\]</div>
<p>In practice, using the summed absolute values of the weights causes <em>sparsity</em>: instead of just getting smaller, some of the weights will get forced to <span class="math notranslate nohighlight">\(0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg_l1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">log_reg_l1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">&quot;$L_2$ (C = 1)&quot;</span><span class="p">:</span> <span class="n">log_reg_l2</span><span class="p">,</span>
  <span class="s2">&quot;$L_1$ (C = 1)&quot;</span><span class="p">:</span> <span class="n">log_reg_l1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note: You’ll notice that we added two additional parameters: <code class="docutils literal notranslate"><span class="pre">solver=&quot;saga&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">max_iter=5000</span></code>. The <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class can use several different optimization algorithms (“solvers”), and not all of them support the <span class="math notranslate nohighlight">\(L_1\)</span> penalty. At a certain point, the solver will give up if it hasn’t found a minimum value. The <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> parameter tells it to make more attempts; otherwise, we’d see an ugly warning about “convergence”.</p>
</div>
</div>
<div class="section" id="section-4-the-key-difference-between-l-1-and-l-2-regularization-sparsity">
<h1>Section 4: The key difference between <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization: sparsity<a class="headerlink" href="#section-4-the-key-difference-between-l-1-and-l-2-regularization-sparsity" title="Permalink to this headline">¶</a></h1>
<p>When should you use <span class="math notranslate nohighlight">\(L_1\)</span> vs. <span class="math notranslate nohighlight">\(L_2\)</span> regularization? Both penalties shrink parameters, and both will help reduce overfitting. However, the models they lead to are different.</p>
<p>In particular, the <span class="math notranslate nohighlight">\(L_1\)</span> penalty encourages <em>sparse</em> solutions in which most parameters are 0. Let’s unpack the notion of sparsity.</p>
<p>A “dense” vector has mostly nonzero elements:
<span class="math notranslate nohighlight">\(\begin{bmatrix}
  0.1 \\ -0.6\\-9.1\\0.07 
\end{bmatrix}\)</span>.
A “sparse” vector has mostly zero elements:
<span class="math notranslate nohighlight">\(\begin{bmatrix}
  0 \\ -0.7\\ 0\\0
\end{bmatrix}\)</span>.</p>
<p>The same is true of matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute to plot a dense and a sparse matrix</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">M_sparse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M_sparse</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;A dense matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;A sparse matrix&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">text_kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">iter_parts</span> <span class="o">=</span> <span class="n">axs</span><span class="p">,</span> <span class="p">[</span><span class="n">M</span><span class="p">,</span> <span class="n">M_sparse</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">{:.1f}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">fmt</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">iter_parts</span><span class="p">):</span>
      <span class="n">val</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
      <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;.1&quot;</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">7</span> <span class="k">else</span> <span class="s2">&quot;w&quot;</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="o">**</span><span class="n">text_kws</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="exercise-3-the-effect-of-l-1-regularization-on-parameter-sparsity">
<h2>Exercise 3: The effect of <span class="math notranslate nohighlight">\(L_1\)</span> regularization on parameter sparsity<a class="headerlink" href="#exercise-3-the-effect-of-l-1-regularization-on-parameter-sparsity" title="Permalink to this headline">¶</a></h2>
<p>Please complete the following function to fit a regularized <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model and return <strong>the number of coefficients in the parameter vector that are equal to 0</strong>.</p>
<p>Don’t forget to check out the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit-learn documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fit models with different L1 penalty values and count non-zero coefficients.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): List of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    non_zero_coefs (list): number of coefficients in each model that are nonzero</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Implement the count_non_zero_coefs function&quot;</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">non_zero_coefs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
    <span class="o">...</span>

    <span class="c1"># Get the coefs of the fit model</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Count the number of non-zero elements in coefs</span>
    <span class="n">non_zero</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">non_zero_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">non_zero_coefs</span>

<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Uncomment and run when the function is ready for testing</span>
<span class="c1"># non_zero_l1 = count_non_zero_coefs(X, y, C_values)</span>
<span class="c1"># plot_non_zero_coefs(C_values, non_zero_l1, n_voxels=X.shape[1])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fit models with different L1 penalty values and count non-zero coefficients.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): List of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    non_zero_coefs (list): number of coefficients in each model that are nonzero</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">non_zero_coefs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;saga&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Get the coefs of the fit model</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

    <span class="c1"># Count the number of non-zero elements in coefs</span>
    <span class="n">non_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coefs</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">non_zero_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">non_zero_coefs</span>

<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">non_zero_l1</span> <span class="o">=</span> <span class="n">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_non_zero_coefs</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">n_voxels</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> (bigger <span class="math notranslate nohighlight">\(\beta\)</span>) leads to sparser solutions.</p>
<p><strong>Link to neuroscience</strong>: When is it OK to assume that the parameter vector is sparse? Whenever it is true that most features don’t affect the outcome. One use-case might be decoding low-level visual features from whole-brain fMRI: we may expect only voxels in V1 and thalamus should be used in the prediction.</p>
<p><strong>WARNING</strong>: be careful when interpreting <span class="math notranslate nohighlight">\(\theta\)</span>. Never interpret the nonzero coefficients as <em>evidence</em> that only those voxels/neurons/features carry information about the outcome. This is a product of our regularization scheme, and thus <em>our prior assumption that the solution is sparse</em>. Other regularization types or models may find very distributed relationships across the brain. Never use a model as evidence for a phenomena when that phenomena is encoded in the assumptions of the model.</p>
</div>
<hr class="docutils" />
<div class="section" id="section-4-1-choosing-the-regularization-penalty">
<h2>Section 4.1: Choosing the regularization penalty<a class="headerlink" href="#section-4-1-choosing-the-regularization-penalty" title="Permalink to this headline">¶</a></h2>
<p>In the examples above, we just picked arbitrary numbers for the strength of regularization. How do you know what value of the hyperparameter to use?</p>
<p>The answer is the same as when you want to know whether you have learned good parameter values: use cross-validation. The best hyperparameter will be the one that allows the model to generalize best to unseen data.</p>
<div class="section" id="exercise-4-model-selection">
<h3>Exercise 4: Model selection<a class="headerlink" href="#exercise-4-model-selection" title="Permalink to this headline">¶</a></h3>
<p>In the final exercise, we will use cross-validation to evaluate a set of models, each with a different <span class="math notranslate nohighlight">\(L_2\)</span> penalty. Your <code class="docutils literal notranslate"><span class="pre">model_selection</span></code> function should have a for-loop that gets the mean cross-validated accuracy for each penalty value (use the <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> function that we introduced above).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute CV accuracy for each C value.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): Array of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracies (1D array): CV accuracy with each value of C</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Implement the model_selection function&quot;</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Get the accuracy for each test split</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Store the average test accuracy for this value of C</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">accuracies</span>

<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="c1"># Uncomment and run when the function is ready to test</span>
<span class="c1"># accuracies = model_selection(X, y, C_values)</span>
<span class="c1"># plot_model_selection(C_values, accuracies)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Compute CV accuracy for each C value.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): Array of hyperparameter values.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracies (1D array): CV accuracy with each value of C.</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

    <span class="c1"># Get the accuracy for each test split</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="c1"># Store the average test accuracy for this value of C</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accs</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">accuracies</span>

<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="n">accuracies</span> <span class="o">=</span> <span class="n">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_model_selection</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This plot suggests that the right value of <span class="math notranslate nohighlight">\(C\)</span> does matter — up to a point. Remember that C is the <em>inverse</em> regularization. The plot shows that models where the regularization was too strong (small C values) performed very poorly. For <span class="math notranslate nohighlight">\(C &gt; 10^{-2}\)</span>, the differences are marginal, but the best performance was obtained with an intermediate value (<span class="math notranslate nohighlight">\(C \approx 10^1\)</span>).</p>
</div>
</div>
</div>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we learned about Logistic Regression, a fundamental algorithm for <em>classification</em>. We applied the algorithm to a <em>neural decoding</em> problem: we tried to predict an animal’s behavioral choice from its neural activity. We saw again how important it is to use <em>cross-validation</em> to evaluate complex models that are at risk for <em>overfitting</em>, and we learned how <em>regularization</em> can be used to fit models that generalize better. Finally, we learned about some of the different options for regularization, and we saw how cross-validation can be useful for <em>model selection</em>.</p>
</div>
<hr class="docutils" />
<div class="section" id="appendix-the-logistic-regression-model-in-full">
<h1>Appendix: The Logistic Regression model in full<a class="headerlink" href="#appendix-the-logistic-regression-model-in-full" title="Permalink to this headline">¶</a></h1>
<p>The fundamental input/output equation of logistic regression is:</p>
<div class="math notranslate nohighlight">
\[p(y_i = 1 |x_i, \theta) = \sigma(\theta^Tx_i)\]</div>
<div class="section" id="the-logistic-link-function">
<h2>The logistic link function<a class="headerlink" href="#the-logistic-link-function" title="Permalink to this headline">¶</a></h2>
<p>You’ve seen <span class="math notranslate nohighlight">\(\theta^T x_i\)</span> before, but the <span class="math notranslate nohighlight">\(\sigma\)</span> is new. It’s the <em>sigmoidal</em> or <em>logistic</em> link function that “squashes” <span class="math notranslate nohighlight">\(\theta^T x_i\)</span> to keep it between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + \textrm{exp}(-z)}\]</div>
</div>
<div class="section" id="the-bernoulli-likelihood">
<h2>The Bernoulli likelihood<a class="headerlink" href="#the-bernoulli-likelihood" title="Permalink to this headline">¶</a></h2>
<p>You might have noticed that the output of the sigmoid, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is not a binary value (0 or 1), even though the true data <span class="math notranslate nohighlight">\(y\)</span> is! Instead, we interpret the value of <span class="math notranslate nohighlight">\(\hat{y}\)</span> as the <em>probability that y = 1</em>:</p>
<div class="math notranslate nohighlight">
\[ \hat{y_i} \equiv p(y_i=1|x_i,\theta) = \frac{1}{{1 + \textrm{exp}(-\theta^Tx_i)}}\]</div>
<p>To get the likelihood of the parameters, we need to define <em>the probability of seeing <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\hat{y}\)</span></em>. In logistic regression, we do this using the Bernoulli distribution:</p>
<div class="math notranslate nohighlight">
\[P(y_i\ |\ \hat{y}_i) = \hat{y}_i^{y_i}(1 - \hat{y}_i)^{(1 - y_i)}\]</div>
<p>So plugging in the regression model:</p>
<div class="math notranslate nohighlight">
\[P(y_i\ |\ \theta, x_i) = \sigma(\theta^Tx_i)^{y_i}(1 - \sigma(\theta^Tx_i))^{(1 - y_i)}.\]</div>
<p>This expression effectively measures how good our parameters <span class="math notranslate nohighlight">\(\theta\)</span> are. We can also write it as the likelihood of the parameters given the data:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta\ |\ y_i, x_i) = P(y_i\ |\ \theta, x_i),\]</div>
<p>and then use this as a target of optimization, considering all of the trials independently:</p>
<div class="math notranslate nohighlight">
\[\textrm{log}\mathcal{L}(\theta | X, y) = \sum_{i=1}^Ny_i\textrm{log}(\sigma(\theta^Tx_i))\ +\ (1-y_i)\textrm{log}(1 - \sigma(\theta^Tx_i)).\]</div>
</div>
</div>
<div class="section" id="appendix-more-detail-about-model-selection">
<h1>Appendix: More detail about model selection<a class="headerlink" href="#appendix-more-detail-about-model-selection" title="Permalink to this headline">¶</a></h1>
<p>In the final exercise, we used all of the data to choose the hyperparameters. That means we don’t have any fresh data left over to evaluate the performance of the selected model. In practice, you would want to have two <em>nested</em> layers of cross-validation, where the final evaluation is performed on data that played no role in selecting or training the model.</p>
<p>Indeed, the proper method for splitting your data to choose hyperparameters can get confusing. Here’s a guide that the authors of this notebook developed while writing a tutorial on using machine learning for neural decoding (<a class="reference external" href="https://arxiv.org/abs/1708.00909">https://arxiv.org/abs/1708.00909</a>).</p>
<img src='http://kordinglab.com/images/others/CV-01.png' width= '700'/>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D4_MachineLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="W1D4_Tutorial1.html" title="previous page">Neuromatch Academy: Week 1, Day 4, Tutorial 1</a>
    <a class='right-next' id="next-link" href="../W1D5_DimensionalityReduction/README.html" title="next page">W1D5 - Dimensionality Reduction</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>