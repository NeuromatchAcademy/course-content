# Machine Learning (GLMs)

Hi folks, welcome to the ‘GLM’ day, the highlight of neuromatch (really)! 
We will talk about generalized linear models, GLMs. The great thing about GLMs is that they provide a simple and elegant framework which unifies a number of useful ML algorithms, and allow you to tackle a wide range of data analysis questions in neuroscience.  Think of them as the Swiss Army Knife of Machine Learning algorithms. 
In particular, some algorithms you might know-- linear regression, logistic regression and Poisson regression, are all just special cases of GLMs.  They are very useful if you want to study encoding (i.e. predict neural activity from external covariates) or decoding (i.e. predict a behaviour or intention from neural activity). 
In addition, GLMs are great for teaching some general concepts in machine learning that are useful to know-- optimization of cost functions (and what is means for a cost function to be convex, and why that is useful), what overfitting is, how to spot it (cross-validation!) and how to protect against it (regularization!), what types of regularization exist (L1, L2), and how this connects to Reverend Bayes. 
Finally -- and just like a Swiss Army Knife’, GLMs have limitations, and sometimes we will need more powerful methods-- however, even in those cases, GLMs provide a baseline against which more complicated algorithms (e.g. deep nets) can be compared. In addition, GLMs can be used as building blocks for constructing more complicated algorithms. 

The day is structured as follows: 
In the ‘Intro’, Dr. Christina Savin will first give an overview over canonical analysis tasks in neuroscience, show what they have in common and how they can be tackled with GLMs. She will explain one task (receptive field fitting) as a running example, and use it to explain concepts like optimization, overfitting and regularization. 
In the first part of Tutorial 1, Dr. Anqi Qu (now assistant prof at Georgia Tech!) will delve deeper into our running example (receptive field estimation), and go through the details of the mathematics of this task, and show that spike-triggered averaging (a great but often heuristically motivated approach) is closely related to a special case of GLMs-- the good old linear regression. In the second part of Tutorial 2, she will show how ‘Poisson GLMs’ are a really powerful way to fit encoding models to spiking data. 
In Tutorial 3, she will turn to logistic regression (my favourite classification algorithm) and show how it can be used for decoding. She will also explain regularization, and the difference between different regularizers. 
In the ‘Outro’, Dr. Memming Park will first tell you about his cat, and go into much more detail  about applications of GLMs in neuroscience- after a general overview, he will talk about how GLMs can be used to model the dynamics of single neurons and neural population dynamics, and how they can be used to model the responses of neurons in decision-making tasks, and to decode behavioural choices from neurons. Finally-- but importantly-- he will talk about some potential pitfalls in interpreting GLMs, in particular when the danger of (falsely) interpreting their parameters in a causal manner. In the end, he will touch on some possible generalizations of GLMs. 

In terms of the overall course, the closest connections are with the day of ‘model fitting’, which will already introduce some of the basic ideas. Once you have been through the GLM day, you will then likely also notice the use of GLMs in many other days and tasks (even if they might not always be referred to as ‘GLMs’)-- e.g. for decoding behavioural data, or as building blocks as models of dynamics. 

So, enjoy the day!
