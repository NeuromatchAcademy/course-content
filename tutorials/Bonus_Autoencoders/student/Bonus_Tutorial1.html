
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Intro to Autoencoders — Neuromatch Academy: Computational Neuroscience</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.jpg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="Bonus_Tutorial2.html" rel="next" title="Tutorial 2: Autoencoder extensions"/>
<link href="../Bonus_Intro.html" rel="prev" title="Intro"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.jpg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/student/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/student/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/student/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/W0D3_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/W0D4_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D5_Statistics/chapter_title.html">
   Statistics (W0D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/W0D5_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial4.html">
     Tutorial 4: Model Dicussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelingPractice/chapter_title.html">
   Modeling Practice (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/student/W1D2_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelingPractice/W1D2_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_ModelFitting/chapter_title.html">
   Model Fitting (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/chapter_title.html">
   Generalized Linear Models (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_DeepLearning/chapter_title.html">
   Deep Learning (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial4.html">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_DeepLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Autoencoders (Bonus)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="../Bonus_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Intro to Autoencoders
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="Bonus_Tutorial2.html">
     Tutorial 2: Autoencoder extensions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="Bonus_Tutorial3.html">
     Tutorial 3: Autoencoders applications
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../Bonus_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/MachineLearning.html">
   Machine Learning Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial4.html">
     Bonus Tutorial: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial3.html">
     Bonus Tutorial: Extending the Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DynamicalSystems.html">
   Dynamical Systems Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial3.html">
     Bonus Tutorial : Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial3.html">
     Tutorial 3: The Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial4.html">
     Bonus Tutorial 4: The Kalman Filter, part 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/student/W3D2_Tutorial5.html">
     Bonus Tutorial 5: Expectation Maximization for spiking neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial4.html">
     Tutorial 4: From Reinforcement Learning to Planning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
<label for="toctree-checkbox-32">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/Bonus_Autoencoders/student/Bonus_Tutorial1.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/Bonus_Autoencoders/student/Bonus_Tutorial1.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Intro to Autoencoders
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#internal-representations-and-autoencoders">
     Internal representations and autoencoders
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-intro">
     Video 1: Intro
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-introduction-to-autoencoders">
   Section 1: Introduction to autoencoders
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-autoencoders">
     Video 2: Autoencoders
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-the-mnist-dataset">
   Section 2: The MNIST dataset
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-download-and-prepare-mnist-dataset">
     Section 2.1: Download  and prepare MNIST dataset
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualize-samples">
     Visualize samples
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-latent-space-visualization">
   Section 3: Latent space visualization
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-mnist-with-pca">
     Section 3.1: MNIST with PCA
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-1-visualize-pca-latent-space-2d">
     Exercise 1: Visualize PCA latent space (2D)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-qualitative-analysis-pca">
     Section 3.2: Qualitative analysis PCA
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-ann-autoencoder">
   Section 4: ANN autoencoder
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#design-ann-autoencoder-32d">
     Design ANN autoencoder (32D)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-2-design-ann-autoencoder">
       Exercise 2: Design ANN autoencoder
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-autoencoder-32d">
     Train autoencoder (32D)
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#choose-the-loss-function">
     Choose the loss function
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#design-ann-autoencoder-2d">
     Design ANN autoencoder (2D)
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-the-autoencoder-2d">
     Train the autoencoder (2D)
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#expressive-power-in-2d">
     Expressive power in 2D
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-wrap-up">
     Video 3: Wrap-up
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#failure-mode-with-relu-units-in-2d">
     Failure mode with ReLU units in 2D
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-choosing-weight-initialization">
       Exercise 3: Choosing weight initialization
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#choose-the-activation-function">
     Choose the activation function
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#qualitative-analysis-nmf">
     Qualitative analysis NMF
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/Bonus_Autoencoders/Bonus_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-1-intro-to-autoencoders">
<h1>Tutorial 1: Intro to Autoencoders<a class="headerlink" href="#tutorial-1-intro-to-autoencoders" title="Permalink to this headline">¶</a></h1>
<p><strong>Bonus Day: Autoencoders</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Marco Brigham and the <a class="reference external" href="https://www.ccnss.org/">CCNSS</a> team (2014-2018)</p>
<p><strong>Content reviewers:</strong> Itzel Olivos, Karen Schroeder, Karolina Stosio, Kshitij Dwivedi, Spiros Chavlis, Michael Waskom</p>
</div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<div class="section" id="internal-representations-and-autoencoders">
<h2>Internal representations and autoencoders<a class="headerlink" href="#internal-representations-and-autoencoders" title="Permalink to this headline">¶</a></h2>
<p>How can simple algorithms capture relevant aspects of data and build robust models of the world?</p>
<p>Autoencoders are a family of artificial neural networks (ANNs) that learn internal representations through auxiliary tasks, i.e., <em>learning by doing</em>.</p>
<p>The primary task is to reconstruct output images based on a compressed representation of the inputs. This task teaches the network which details to throw away while still producing images that are similar to the inputs.</p>
<p> </p>
<p>A fictitious <em>MNIST cognitive task</em> bundles more elaborate tasks such as removing noise from images, guessing occluded parts, and recovering original image orientation. We use the handwritten digits from the MNIST dataset since it is easier to identify similar images or issues with reconstructions than in other types of data, such as spiking data time series.</p>
<p> </p>
<p><img alt="MNIST cognitive task" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/mnist_task.png"/></p>
<p> </p>
<p>The beauty of autoencoders is the possibility to see these internal representations. The bottleneck layer enforces data compression by having fewer units than input and output layers. Further limiting this layer to two or three units enables us to see how the autoencoder is organizing the data internally in two or three-dimensional <em>latent space</em>.</p>
<p> </p>
<p>Our roadmap is the following: learn about typical elements of autoencoder architecture in Tutorial 1 (this tutorial), how to extend their performance in Tutorial 2, and use them to solve the MNIST cognitive task in Tutorial 3.</p>
<p> </p>
<p>In this tutorial, you will:</p>
<ul class="simple">
<li><p>Get acquainted with latent space visualizations and apply them to <em>Principal Component Analysis (PCA)</em> and <em>Non-negative Matrix Factorization (NMF)</em></p></li>
<li><p>Build and train a single hidden layer ANN autoencoder</p></li>
<li><p>Inspect the representational power of autoencoders with latent spaces of different dimensions</p></li>
</ul>
</div>
<div class="section" id="video-1-intro">
<h2>Video 1: Intro<a class="headerlink" href="#video-1-intro" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "c777f05e881742b4aa56223bccda4f33"}
</script></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>Please execute the cell(s) below to initialize the notebook environment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle"</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'figure.figsize'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>


<span class="k">def</span> <span class="nf">downloadMNIST</span><span class="p">():</span>
  <span class="sd">"""</span>
<span class="sd">  Download MNIST dataset and transform it to torch.Tensor</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    x_train : training images (torch.Tensor) (60000, 28, 28)</span>
<span class="sd">    x_test  : test images (torch.Tensor) (10000, 28, 28)</span>
<span class="sd">    y_train : training labels (torch.Tensor) (60000, )</span>
<span class="sd">    y_train : test labels (torch.Tensor) (10000, )</span>
<span class="sd">  """</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">'mnist_784'</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="c1"># Trunk the data</span>
  <span class="n">n_train</span> <span class="o">=</span> <span class="mi">60000</span>
  <span class="n">n_test</span> <span class="o">=</span> <span class="mi">10000</span>

  <span class="n">train_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_train</span><span class="p">)</span>
  <span class="n">test_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">n_train</span> <span class="o">+</span> <span class="n">n_test</span><span class="p">)</span>

  <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
  <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

  <span class="c1"># Transform np.ndarrays to torch.Tensor</span>
  <span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                                        <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span>
                                         <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span>
                                       <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span>
                                        <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

  <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
  <span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>

  <span class="k">return</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_weights_kaiming_uniform</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Initializes weights from linear PyTorch layer</span>
<span class="sd">  with kaiming uniform distribution.</span>

<span class="sd">  Args:</span>
<span class="sd">    layer (torch.Module)</span>
<span class="sd">        Pytorch layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="c1"># check for linear PyTorch layer</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="c1"># initialize weights with kaiming uniform distribution</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">init_weights_kaiming_normal</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Initializes weights from linear PyTorch layer</span>
<span class="sd">  with kaiming normal distribution.</span>

<span class="sd">  Args:</span>
<span class="sd">    layer (torch.Module)</span>
<span class="sd">        Pytorch layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="c1"># check for linear PyTorch layer</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="c1"># initialize weights with kaiming normal distribution</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_layer_weights</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Retrieves learnable parameters from PyTorch layer.</span>

<span class="sd">  Args:</span>
<span class="sd">    layer (torch.Module)</span>
<span class="sd">        Pytorch layer</span>

<span class="sd">  Returns:</span>
<span class="sd">    list with learnable parameters</span>
<span class="sd">  """</span>
  <span class="c1"># initialize output list</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># check whether layer has learnable parameters</span>
  <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="c1"># copy numpy array representation of each set of learnable parameters</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
      <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">weights</span>


<span class="k">def</span> <span class="nf">eval_mse</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Evaluates Mean Square Error (MSE) between y_pred and y_true</span>

<span class="sd">  Args:</span>
<span class="sd">    y_pred (torch.Tensor)</span>
<span class="sd">        prediction samples</span>

<span class="sd">    v (numpy array of floats)</span>
<span class="sd">        ground truth samples</span>

<span class="sd">  Returns:</span>
<span class="sd">    MSE(y_pred, y_true)</span>
<span class="sd">  """</span>

  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>

  <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">eval_bce</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Evaluates Binary Cross-Entropy (BCE) between y_pred and y_true</span>

<span class="sd">  Args:</span>
<span class="sd">    y_pred (torch.Tensor)</span>
<span class="sd">        prediction samples</span>

<span class="sd">    v (numpy array of floats)</span>
<span class="sd">        ground truth samples</span>

<span class="sd">  Returns:</span>
<span class="sd">    BCE(y_pred, y_true)</span>
<span class="sd">  """</span>

  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>

  <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_weights_ab</span><span class="p">(</span><span class="n">encoder_w_a</span><span class="p">,</span> <span class="n">encoder_w_b</span><span class="p">,</span> <span class="n">decoder_w_a</span><span class="p">,</span> <span class="n">decoder_w_b</span><span class="p">,</span>
                    <span class="n">label_a</span><span class="o">=</span><span class="s1">'init'</span><span class="p">,</span> <span class="n">label_b</span><span class="o">=</span><span class="s1">'train'</span><span class="p">,</span>
                    <span class="n">bins_encoder</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">bins_decoder</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots row of histograms with encoder and decoder weights</span>
<span class="sd">  between two training checkpoints.</span>

<span class="sd">  Args:</span>
<span class="sd">    encoder_w_a (iterable)</span>
<span class="sd">        encoder weights at checkpoint a</span>

<span class="sd">    encoder_w_b (iterable)</span>
<span class="sd">        encoder weights at checkpoint b</span>

<span class="sd">    decoder_w_a (iterable)</span>
<span class="sd">        decoder weights at checkpoint a</span>

<span class="sd">    decoder_w_b (iterable)</span>
<span class="sd">        decoder weights at checkpoint b</span>

<span class="sd">    label_a (string)</span>
<span class="sd">        label for checkpoint a</span>

<span class="sd">    label_b (string)</span>
<span class="sd">        label for checkpoint b</span>

<span class="sd">    bins_encoder (float)</span>
<span class="sd">        norm of extreme values for encoder bins</span>

<span class="sd">    bins_decoder (float)</span>
<span class="sd">        norm of extreme values for decoder bins</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_w</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">))</span>

  <span class="c1"># plot encoder weights</span>
  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">bins_encoder</span><span class="p">,</span> <span class="n">bins_encoder</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Encoder weights to unit 0'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">encoder_w_a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_a</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">encoder_w_b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_b</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Encoder weights to unit 1'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">encoder_w_a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_a</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">encoder_w_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_b</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="c1"># plot decoder weights</span>
  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">bins_decoder</span><span class="p">,</span> <span class="n">bins_decoder</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decoder weights from unit 0'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">decoder_w_a</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_a</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">decoder_w_b</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_b</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decoder weights from unit 1'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">decoder_w_a</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_a</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">decoder_w_b</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_b</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_row</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">show_n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots rows of images from list of iterables (iterables: list, numpy array</span>
<span class="sd">  or torch.Tensor). Also accepts single iterable.</span>
<span class="sd">  Randomly selects images in each list element if item count &gt; show_n.</span>

<span class="sd">  Args:</span>
<span class="sd">    images (iterable or list of iterables)</span>
<span class="sd">        single iterable with images, or list of iterables</span>

<span class="sd">    show_n (integer)</span>
<span class="sd">        maximum number of images per row</span>

<span class="sd">    image_shape (tuple or list)</span>
<span class="sd">        original shape of image if vectorized form</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">images</span><span class="p">]</span>

  <span class="k">for</span> <span class="n">items_idx</span><span class="p">,</span> <span class="n">items</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>

    <span class="n">items</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">items</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">items</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">show_n</span><span class="p">:</span>
      <span class="n">selected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">),</span> <span class="n">show_n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="n">items</span> <span class="o">=</span> <span class="n">items</span><span class="p">[</span><span class="n">selected</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">image_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">items</span> <span class="o">=</span> <span class="n">items</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">image_shape</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">image_idx</span><span class="p">,</span> <span class="n">image</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>

      <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">),</span> <span class="n">image_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">vmax</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

      <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">xy_lim</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Return arguments for plt.xlim and plt.ylim calculated from minimum</span>
<span class="sd">  and maximum of x.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        data to be plotted</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="n">x_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="n">x_min</span> <span class="o">=</span> <span class="n">x_min</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
  <span class="n">x_max</span> <span class="o">=</span> <span class="n">x_max</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

  <span class="k">return</span> <span class="p">[</span><span class="n">x_min</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_max</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">x_min</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_max</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">plot_generative</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">decoder_fn</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">,</span> <span class="n">n_row</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots images reconstructed by decoder_fn from a 2D grid in</span>
<span class="sd">  latent space that is determined by minimum and maximum values in x.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        2D coordinates in latent space</span>

<span class="sd">    decoder_fn (integer)</span>
<span class="sd">        function returning vectorized images from 2D latent space coordinates</span>

<span class="sd">    image_shape (tuple or list)</span>
<span class="sd">        original shape of image</span>

<span class="sd">    n_row</span>
<span class="sd">        number of rows in grid</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="n">xy_lim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

  <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">n_row</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">dx</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dx</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_row</span><span class="p">),</span>
          <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">dx</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dx</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_row</span><span class="p">)]</span>

  <span class="n">canvas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n_row</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>

  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'gray'</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">latent_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">latent_x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>

      <span class="n">latent</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">latent_x</span><span class="p">,</span> <span class="n">latent_y</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
          <span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder_fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">latent</span><span class="p">))</span>

      <span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">)</span>

      <span class="n">canvas</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
             <span class="n">i</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">x_decoded</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">canvas</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">canvas</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">vmax</span><span class="o">=</span><span class="n">canvas</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_latent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">show_n</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">xy_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Plots digit class of each sample in 2D latent space coordinates.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        2D coordinates in latent space</span>

<span class="sd">    y (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        digit class of each sample</span>

<span class="sd">    n_row (integer)</span>
<span class="sd">        number of samples</span>

<span class="sd">    fontdict (dictionary)</span>
<span class="sd">        optional style option for plt.text</span>

<span class="sd">    xy_labels (list)</span>
<span class="sd">        optional list with [xlabel, ylabel]</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="k">if</span> <span class="n">fontdict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="s1">'bold'</span><span class="p">,</span> <span class="s1">'size'</span><span class="p">:</span> <span class="mi">12</span><span class="p">}</span>

  <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'tab10'</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">show_n</span><span class="p">:</span>
    <span class="n">selected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">show_n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">selected</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">selected</span><span class="p">]</span>

  <span class="k">for</span> <span class="n">my_x</span><span class="p">,</span> <span class="n">my_y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">my_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">my_x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">my_y</span><span class="p">)),</span>
             <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">my_y</span><span class="p">)</span> <span class="o">/</span> <span class="mf">10.</span><span class="p">),</span>
             <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">,</span>
             <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
             <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">xy_labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">xy_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'$Z_1$'</span><span class="p">,</span> <span class="s1">'$Z_2$'</span><span class="p">]</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">xy_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">xy_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

  <span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="n">xy_lim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_latent_generative</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">decoder_fn</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">xy_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Two horizontal subplots generated with encoder map and decoder grid.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        2D coordinates in latent space</span>

<span class="sd">    y (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        digit class of each sample</span>

<span class="sd">    decoder_fn (integer)</span>
<span class="sd">        function returning vectorized images from 2D latent space coordinates</span>

<span class="sd">    image_shape (tuple or list)</span>
<span class="sd">        original shape of image</span>

<span class="sd">    title (string)</span>
<span class="sd">        plot title</span>

<span class="sd">    xy_labels (list)</span>
<span class="sd">        optional lsit with [xlabel, ylabel]</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Encoder map'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
  <span class="n">plot_latent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xy_labels</span><span class="o">=</span><span class="n">xy_labels</span><span class="p">)</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Decoder grid'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
  <span class="n">plot_generative</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">decoder_fn</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_latent_ab</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">selected_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">title_a</span><span class="o">=</span><span class="s1">'Before'</span><span class="p">,</span> <span class="n">title_b</span><span class="o">=</span><span class="s1">'After'</span><span class="p">,</span> <span class="n">show_n</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Two horizontal subplots with encoder maps.</span>

<span class="sd">  Args:</span>
<span class="sd">    x1 (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        2D coordinates in latent space (left plot)</span>

<span class="sd">    x2 (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        digit class of each sample (right plot)</span>

<span class="sd">    y (list, numpy array or torch.Tensor of floats)</span>
<span class="sd">        digit class of each sample</span>

<span class="sd">    selected_idx (list of integers)</span>
<span class="sd">        indexes of elements to be plotted</span>

<span class="sd">    show_n (integer)</span>
<span class="sd">        maximum number of samples in each plot</span>

<span class="sd">    s2 (boolean)</span>
<span class="sd">        convert 3D coordinates (x, y, z) to spherical coordinates (theta, phi)</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'weight'</span><span class="p">:</span> <span class="s1">'bold'</span><span class="p">,</span> <span class="s1">'size'</span><span class="p">:</span> <span class="mi">12</span><span class="p">}</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">show_n</span><span class="p">:</span>

    <span class="k">if</span> <span class="n">selected_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">selected_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">show_n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">selected_idx</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">[</span><span class="n">selected_idx</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">selected_idx</span><span class="p">]</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title_a</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
  <span class="n">plot_latent</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title_b</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
  <span class="n">plot_latent</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">runSGD</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
           <span class="n">n_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Trains autoencoder network with stochastic gradient descent with Adam</span>
<span class="sd">  optimizer and loss criterion. Train samples are shuffled, and loss is</span>
<span class="sd">  displayed at the end of each opoch for both MSE and BCE. Plots training loss</span>
<span class="sd">  at each minibatch (maximum of 500 randomly selected values).</span>

<span class="sd">  Args:</span>
<span class="sd">    net (torch network)</span>
<span class="sd">        ANN object (nn.Module)</span>

<span class="sd">    input_train (torch.Tensor)</span>
<span class="sd">        vectorized input images from train set</span>

<span class="sd">    input_test (torch.Tensor)</span>
<span class="sd">        vectorized input images from test set</span>

<span class="sd">    criterion (string)</span>
<span class="sd">        train loss: 'bce' or 'mse'</span>

<span class="sd">    n_epochs (boolean)</span>
<span class="sd">        number of full iterations of training data</span>

<span class="sd">    batch_size (integer)</span>
<span class="sd">        number of element in mini-batches</span>

<span class="sd">    verbose (boolean)</span>
<span class="sd">        whether to print final loss</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>

  <span class="c1"># Initialize loss function</span>
  <span class="k">if</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">'mse'</span><span class="p">:</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
  <span class="k">elif</span> <span class="n">criterion</span> <span class="o">==</span> <span class="s1">'bce'</span><span class="p">:</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Please specify either "mse" or "bce" for loss criterion'</span><span class="p">)</span>

  <span class="c1"># Initialize SGD optimizer</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

  <span class="c1"># Placeholder for loss</span>
  <span class="n">track_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'Loss train'</span><span class="p">,</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'Loss test'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>

    <span class="n">shuffle_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_train</span><span class="p">))</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">input_train</span><span class="p">[</span><span class="n">shuffle_idx</span><span class="p">],</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>

      <span class="n">output_train</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output_train</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

      <span class="c1"># Keep track of loss at each epoch</span>
      <span class="n">track_loss</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)]</span>

    <span class="n">loss_epoch</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1"> / </span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s1">'</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">output_train</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_train</span><span class="p">)</span>
      <span class="n">loss_train</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output_train</span><span class="p">,</span> <span class="n">input_train</span><span class="p">)</span>
      <span class="n">loss_epoch</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">'</span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">loss_train</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span>

      <span class="n">output_test</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>
      <span class="n">loss_test</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output_test</span><span class="p">,</span> <span class="n">input_test</span><span class="p">)</span>
      <span class="n">loss_epoch</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">'</span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">loss_test</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">loss_epoch</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
    <span class="c1"># Print final loss</span>
    <span class="n">loss_mse</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">MSE</span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">eval_mse</span><span class="p">(</span><span class="n">output_train</span><span class="p">,</span> <span class="n">input_train</span><span class="p">)</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">'</span>
    <span class="n">loss_mse</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">'</span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">eval_mse</span><span class="p">(</span><span class="n">output_test</span><span class="p">,</span> <span class="n">input_test</span><span class="p">)</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">'</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">loss_mse</span><span class="p">)</span>

    <span class="n">loss_bce</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'BCE</span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">eval_bce</span><span class="p">(</span><span class="n">output_train</span><span class="p">,</span> <span class="n">input_train</span><span class="p">)</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">'</span>
    <span class="n">loss_bce</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">'</span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">eval_bce</span><span class="p">(</span><span class="n">output_test</span><span class="p">,</span> <span class="n">input_test</span><span class="p">)</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">'</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">loss_bce</span><span class="p">)</span>

  <span class="c1"># Plot loss</span>
  <span class="n">step</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">track_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">500</span><span class="p">))</span>
  <span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">track_loss</span><span class="p">),</span> <span class="n">step</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">track_loss</span><span class="p">[::</span><span class="n">step</span><span class="p">],</span> <span class="s1">'C0'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Iterations'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-introduction-to-autoencoders">
<h1>Section 1: Introduction to autoencoders<a class="headerlink" href="#section-1-introduction-to-autoencoders" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-2-autoencoders">
<h2>Video 2: Autoencoders<a class="headerlink" href="#video-2-autoencoders" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "160a7214b27a4895ac8b594dca4ec3ee"}
</script></div>
</div>
<p>This tutorial introduces typical elements of autoencoders, that learn low dimensional representations of data through an auxiliary task of compression and decompression. In general, these networks are characterized by an equal number of input and output units and a <em>bottleneck layer</em> with fewer units.</p>
<p><img alt="Single hidden layer ANN autoencoder" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/ae-ann-1h.png"/></p>
<p>Autoencoder architectures have <em>encoder</em> and <em>decoder</em> components:</p>
<ul class="simple">
<li><p>The encoder network compresses high dimensional inputs into lower-dimensional coordinates of the <em>bottleneck layer</em></p></li>
<li><p>The <em>decoder</em> expands <em>bottleneck layer</em> coordinates back to the original dimensionality</p></li>
</ul>
<p>Each input presented to the autoencoder maps to a coordinate in the bottleneck layer that spans the lower-dimensional <em>latent space</em>.</p>
<p> </p>
<p>Differences between inputs and outputs trigger the backpropagation of loss to adjust weights and better compress/decompress data.  Autoencoders are examples of models that automatically build internal representations of the world and use them to predict unseen data.</p>
<p>We’ll use fully-connected AAN architectures due to their lower computational requirements. The inputs to ANNs are <em>vectorized</em> versions of the images (i.e., stretched as a line).</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-the-mnist-dataset">
<h1>Section 2: The MNIST dataset<a class="headerlink" href="#section-2-the-mnist-dataset" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a> contains handwritten digits in square images of 28x28 pixels of grayscale levels. There are 60,000 training images and 10,000 testing images from different writers.</p>
<p>Get acquainted with the data by inspecting data type, shape, and visualizing samples with the function <code class="docutils literal notranslate"><span class="pre">plot_row</span></code>.</p>
<p><strong>Helper function:</strong> <code class="docutils literal notranslate"><span class="pre">plot_row</span></code></p>
<p>Please uncomment the line below to inspect this function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># help(plot_row)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-2-1-download-and-prepare-mnist-dataset">
<h2>Section 2.1: Download  and prepare MNIST dataset<a class="headerlink" href="#section-2-1-download-and-prepare-mnist-dataset" title="Permalink to this headline">¶</a></h2>
<p>We use the helper function <code class="docutils literal notranslate"><span class="pre">downloadMNIST</span></code> to download the dataset, transform it into <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> and assign train and test datasets to (<code class="docutils literal notranslate"><span class="pre">x_train</span></code>, <code class="docutils literal notranslate"><span class="pre">y_train</span></code>) and (<code class="docutils literal notranslate"><span class="pre">x_test</span></code>, <code class="docutils literal notranslate"><span class="pre">y_test</span></code>), respectively.</p>
<p>(<code class="docutils literal notranslate"><span class="pre">x_train</span></code>, <code class="docutils literal notranslate"><span class="pre">x_test</span></code>) contain images and (<code class="docutils literal notranslate"><span class="pre">y_train</span></code>, <code class="docutils literal notranslate"><span class="pre">y_test</span></code>) contain labels from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">9</span></code>.</p>
<p>The original pixel values are integers between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">255</span></code>. We rescale them between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, a more favorable range for training the autoencoders in this tutorial.</p>
<p>The images are <em>vectorized</em>, i.e., stretched as a line. We reshape training and testing images to <em>vectorized</em> versions with the method <code class="docutils literal notranslate"><span class="pre">.reshape</span></code> and store them in variable <code class="docutils literal notranslate"><span class="pre">input_train</span></code> and <code class="docutils literal notranslate"><span class="pre">input_test</span></code>, respectively. The variable <code class="docutils literal notranslate"><span class="pre">image_shape</span></code> stores the shape of the images, and <code class="docutils literal notranslate"><span class="pre">input_size</span></code> stores the size of the <em>vectorized</em> versions.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Please execute the cell below</p></li>
</ul>
<p><strong>Questions:</strong></p>
<ul class="simple">
<li><p>What are the shape and numeric representations of <code class="docutils literal notranslate"><span class="pre">x_train</span></code> and <code class="docutils literal notranslate"><span class="pre">input_train</span></code>?</p></li>
<li><p>What is the image shape?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download MNIST</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">downloadMNIST</span><span class="p">()</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>

<span class="n">image_shape</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">image_shape</span><span class="p">)</span>

<span class="n">input_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>
<span class="n">input_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>

<span class="n">test_selected_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="mi">10</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_selected_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="mi">10</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'shape x_train </span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'shape x_test </span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'shape image </span><span class="se">\t\t</span><span class="s1"> </span><span class="si">{</span><span class="n">image_shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'shape input_train </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">input_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'shape input_test </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">input_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>shape x_train 		 torch.Size([60000, 28, 28])
shape x_test 		 torch.Size([10000, 28, 28])
shape image 		 torch.Size([28, 28])
shape input_train 	 torch.Size([60000, 784])
shape input_test 	 torch.Size([10000, 784])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-samples">
<h2>Visualize samples<a class="headerlink" href="#visualize-samples" title="Permalink to this headline">¶</a></h2>
<p>The variables <code class="docutils literal notranslate"><span class="pre">train_selected_idx</span></code> and <code class="docutils literal notranslate"><span class="pre">test_selected_idx</span></code> store 10 random indexes from the train and test data.</p>
<p>We use the function <code class="docutils literal notranslate"><span class="pre">np.random.choice</span></code> to select 10 indexes from <code class="docutils literal notranslate"><span class="pre">x_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code> without replacement (<code class="docutils literal notranslate"><span class="pre">replacement=False</span></code>).</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Please execute the cells below</p></li>
<li><p>The first cell display different samples each time, the second cell always displays the same samples</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># top row: random images from test set</span>
<span class="c1"># bottom row: images selected with test_selected_idx</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_21_0.png" src="../../../_images/Bonus_Tutorial1_21_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_21_1.png" src="../../../_images/Bonus_Tutorial1_21_1.png"/>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-latent-space-visualization">
<h1>Section 3: Latent space visualization<a class="headerlink" href="#section-3-latent-space-visualization" title="Permalink to this headline">¶</a></h1>
<p>This section introduces tools for visualization of latent space and applies them to <em>Principal Component Analysis (PCA)</em>, already introduced in tutorial <em>W1D5 Dimensionality reduction</em>. Please see the exercise in the <em>Bonus</em> section for <em>Non-negative Matrix Factorization (NMF)</em>.</p>
<p> </p>
<p>The plotting function <code class="docutils literal notranslate"><span class="pre">plot_latent_generative</span></code> helps visualize the encoding of inputs from high dimension into 2D latent space, and decoding back to the original dimension. This function produces two plots:</p>
<ul class="simple">
<li><p><strong>Encoder map</strong> shows the mapping from input images to coordinates <span class="math notranslate nohighlight">\((z_1, z_2)\)</span> in latent space, with overlaid digit labels</p></li>
<li><p><strong>Decoder grid</strong> shows reconstructions from a grid of latent space coordinates <span class="math notranslate nohighlight">\((z_1, z_2)\)</span></p></li>
</ul>
<p> </p>
<p><img alt="Latent space visualization" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/latent_space_plots_noaxis.png"/></p>
<p>The latent space representation is a new coordinate system <span class="math notranslate nohighlight">\((z_1, z_2)\)</span> that hopefully captures relevant structure from high-dimensional data. The coordinates of each input only matter relative to those of other inputs, i.e., we often care about separability between different classes of digits rather than their location.</p>
<p> </p>
<p>The encoder map provides direct insight into the organization of latent space. Keep in mind that latent space only contains coordinates <span class="math notranslate nohighlight">\((z_1, z_2)\)</span>. We overlay additional information such as digit labels for insight into the latent space structure.</p>
<p>The plot on the left is the raw latent space representation corresponding to the plot on the right with digit labels overlaid.</p>
<p><img alt="Raw latent space visualization" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/latent_space_plots_nolabel.png"/></p>
<div class="section" id="section-3-1-mnist-with-pca">
<h2>Section 3.1: MNIST with PCA<a class="headerlink" href="#section-3-1-mnist-with-pca" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="exercise-1-visualize-pca-latent-space-2d">
<h2>Exercise 1: Visualize PCA latent space (2D)<a class="headerlink" href="#exercise-1-visualize-pca-latent-space-2d" title="Permalink to this headline">¶</a></h2>
<p>The tutorial <em>W1D5 Dimensionality reduction</em> introduced PCA decomposition. The case of two principal components (PCA1 and PCA2) generates a latent space in 2D.</p>
<p><img alt="Latent space visualization PCA" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/latent_space_plots_pca.png"/></p>
<p>In tutorial W1D5, PCA decomposition was implemented directly and also by using module <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition">sklearn.decomposition</a> from the package <a class="reference external" href="https://scikit-learn.org">scikit-learn</a>. This module includes several matrix decomposition algorithms that are useful as dimensionality reduction techniques.</p>
<p>Their usage is very straightforward, as shown by this example for truncated SVD:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svd</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">svd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">input_train</span><span class="p">)</span>

<span class="n">svd_latent_train</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_train</span><span class="p">)</span>
<span class="n">svd_latent_test</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">svd_reconstruction_train</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">svd_latent_train</span><span class="p">)</span>
<span class="n">svd_reconstruction_test</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">svd_latent_test</span><span class="p">)</span>
</pre></div>
</div>
<p>in this exercise, we’ll use <code class="docutils literal notranslate"><span class="pre">decomposition.PCA</span></code> (docs <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">here</a>) for PCA decomposition.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Initialize <code class="docutils literal notranslate"><span class="pre">decomposition.PCA</span></code> in 2 dimensions</p></li>
<li><p>Fit <code class="docutils literal notranslate"><span class="pre">input_train</span></code> with <code class="docutils literal notranslate"><span class="pre">.fit</span></code> method of <code class="docutils literal notranslate"><span class="pre">decomposition.PCA</span></code></p></li>
<li><p>Obtain latent space representation of <code class="docutils literal notranslate"><span class="pre">input_test</span></code></p></li>
<li><p>Visualize latent space with <code class="docutils literal notranslate"><span class="pre">plot_latent_generative</span></code></p></li>
</ul>
<p><strong>Helper function:</strong> <code class="docutils literal notranslate"><span class="pre">plot_latent_generative</span></code></p>
<p>Please uncomment the line below to inspect this function.</p>
<div class="section" id="id1">
<h3><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Execute this cell to inspect <code class="docutils literal notranslate"><span class="pre">plot_latent_generative</span></code>!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title</span>

<span class="c1"># @markdown Execute this cell to inspect `plot_latent_generative`!</span>
<span class="n">help</span><span class="p">(</span><span class="n">plot_latent_generative</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Help on function plot_latent_generative in module __main__:

plot_latent_generative(x, y, decoder_fn, image_shape, title=None, xy_labels=None)
    Two horizontal subplots generated with encoder map and decoder grid.
    
    Args:
      x (list, numpy array or torch.Tensor of floats)
          2D coordinates in latent space
    
      y (list, numpy array or torch.Tensor of floats)
          digit class of each sample
    
      decoder_fn (integer)
          function returning vectorized images from 2D latent space coordinates
    
      image_shape (tuple or list)
          original shape of image
    
      title (string)
          plot title
    
      xy_labels (list)
          optional lsit with [xlabel, ylabel]
    
    Returns:
      Nothing.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####################################################</span>
<span class="c1">## TODO for students: perform PCA and visualize latent space and reconstruction</span>
<span class="c1">####################################################</span>
<span class="c1"># create the model</span>
<span class="c1"># pca = decomposition.PCA(...)</span>
<span class="c1"># fit the model on training data</span>
<span class="c1"># pca.fit(...)</span>
<span class="c1"># transformation on 2D space</span>
<span class="c1"># pca_latent_test = pca.transform(...)</span>

<span class="c1"># Uncomment to test your code!</span>
<span class="c1"># plot_latent_generative(pca_latent_test, y_test, pca.inverse_transform,</span>
<span class="c1">#                        image_shape=image_shape)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/Bonus_Autoencoders/solutions/Bonus_Tutorial1_Solution_464a2875.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/Bonus_Autoencoders/static/Bonus_Tutorial1_Solution_464a2875_0.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/Bonus_Autoencoders/static/Bonus_Tutorial1_Solution_464a2875_0.png" style="width: 1615.0px; height: 832.0px;"/></a>
</div>
</div>
<div class="section" id="section-3-2-qualitative-analysis-pca">
<h2>Section 3.2: Qualitative analysis PCA<a class="headerlink" href="#section-3-2-qualitative-analysis-pca" title="Permalink to this headline">¶</a></h2>
<p>The encoder map shows how well the encoder is distinguishing between digit classes. We see that digits <code class="docutils literal notranslate"><span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">0</span></code> are in opposite regions of the first principal component axis, and similarly for digits <code class="docutils literal notranslate"><span class="pre">9</span></code> and <code class="docutils literal notranslate"><span class="pre">3</span></code> for the second principal component.</p>
<p>The decoder grid indicates how well the decoder is recovering images from latent space coordinates. Overall, digits <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">0</span></code>, and <code class="docutils literal notranslate"><span class="pre">9</span></code> are the most recognizable.</p>
<p>Let’s inspect the principal components to understand these observations better. The principal components are available as <code class="docutils literal notranslate"><span class="pre">pca.components_</span></code> and shown below.</p>
<p><img alt="principal components" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/pca-components.png"/></p>
<p>Notice that the first principal component encodes digit <code class="docutils literal notranslate"><span class="pre">0</span></code> with positive values (in white) and digit <code class="docutils literal notranslate"><span class="pre">1</span></code> in negative values (in black). The colormap encodes the minimum values in black and maximum values in white, and we know their signs by looking at coordinates in the first principal component axis for digits <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<p> </p>
<p>The first principal component axis encodes the “thickness” of the digits: thin digits on the left and tick digits on the right.</p>
<p>Similarly, the second principal component encodes digit <code class="docutils literal notranslate"><span class="pre">9</span></code> with positive values (in white) and digit <code class="docutils literal notranslate"><span class="pre">3</span></code> with negative values (in black).</p>
<p>The second principal component axis is encoding, well, another aspect besides “thickness” of digits (why?).</p>
<p>The reconstruction grid also shows that digits <code class="docutils literal notranslate"><span class="pre">4</span></code> and <code class="docutils literal notranslate"><span class="pre">7</span></code> are indistinguishable from digit <code class="docutils literal notranslate"><span class="pre">9</span></code> and similarly for digits <code class="docutils literal notranslate"><span class="pre">2</span></code> and <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Please execute the cell(s) below</p></li>
<li><p>Plot reconstruction samples a few times to get a visual feel of the digit confusions (use keyword <code class="docutils literal notranslate"><span class="pre">image_shape</span></code> to visualize the vectorized images).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca_components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>

<span class="n">plot_row</span><span class="p">(</span><span class="n">pca_components</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca_output_test</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">pca_latent_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">,</span> <span class="n">pca_output_test</span><span class="p">],</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-ann-autoencoder">
<h1>Section 4: ANN autoencoder<a class="headerlink" href="#section-4-ann-autoencoder" title="Permalink to this headline">¶</a></h1>
<p>Let’s implement a <em>shallow</em> ANN autoencoder with a single hidden layer.</p>
<p><img alt="Single hidden layer ANN autoencoder" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/ae-ann-1h.png"/></p>
<div class="section" id="design-ann-autoencoder-32d">
<h2>Design ANN autoencoder (32D)<a class="headerlink" href="#design-ann-autoencoder-32d" title="Permalink to this headline">¶</a></h2>
<p>Here we introduce a technique for quickly building Pytorch models best suited for the initial exploration phase of your research project.</p>
<p>The object-oriented programming (OOP) presented in tutorial W3D4 is your top choice after better understanding the model’s architecture and components.</p>
<p>Using this concise technique, a network equivalent to <code class="docutils literal notranslate"><span class="pre">DeepNetReLU</span></code> from W3D4 tutorial 1 is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">))</span>
</pre></div>
</div>
<p>Designing and training efficient neural networks currently requires some thought, experience, and testing for choosing between available options, such as the number of hidden layers, loss function, optimizer function, mini-batch size, etc. Choosing these hyper-parameters may soon become more of an engineering process with our increasing analytical understanding of these systems and their learning dynamics.</p>
<p>The references below are great to learn more about neural network design and best practices:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://neuralnetworksanddeeplearning.com">Neural Networks and Deep Learning</a> by Michael Nielsen is an excellent reference for beginners</p></li>
<li><p><a class="reference external" href="http://www.deeplearningbook.org">Deep Learning</a> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville provides in-depth and extensive coverage</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1803.09820">A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay</a> by L. Smith covers efficient ways to set hyper-parameters</p></li>
</ul>
<div class="section" id="exercise-2-design-ann-autoencoder">
<h3>Exercise 2: Design ANN autoencoder<a class="headerlink" href="#exercise-2-design-ann-autoencoder" title="Permalink to this headline">¶</a></h3>
<p>We will use a rectifier <a class="reference external" href="https://icml.cc/Conferences/2010/papers/432.pdf">ReLU</a> units in the bottleneck layer with <code class="docutils literal notranslate"><span class="pre">encoding_dim=32</span></code> units, and sigmoid units in the output layer. You can read more about activation functions <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function">here</a> and rectifiers <a class="reference external" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">here</a>.</p>
<p><img alt="ReLU unit" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/relu.png"/></p>
<p><img alt="Single hidden layer ANN autoencoder" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/sigmoid.png"/></p>
<p>We rescaled images to values between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code> for compatibility with sigmoid units in the output (why?). Such mapping is without loss of generality since any (finite) range can map a one-to-one correspondence to values between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<p>Both ReLU and sigmoid units provide non-linear computation to the encoder and decoder components. The sigmoid units, additionally, ensure output values to be in the same range as the inputs. These units could be swapped by ReLU, in which case output values would sometimes be negative or greater than 1. The sigmoid units of the decoder enforce a numerical constraint that expresses our <em>domain knowledge</em> of the data.</p>
<p><strong>Instructions</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> defines and initializes an ANN with layer sizes (<code class="docutils literal notranslate"><span class="pre">input_shape,</span> <span class="pre">encoding_dim,</span> <span class="pre">input_shape</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> defines a linear layer with the size of the inputs and outputs as arguments</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> encode ReLU and sigmoid units</p></li>
<li><p>Visualize the initial output using <code class="docutils literal notranslate"><span class="pre">plot_row</span></code> with input and output images</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="c1">######################################################################</span>
    <span class="c1">## TODO for students: add linear and sigmoid layers</span>
    <span class="c1">######################################################################</span>
    <span class="c1"># insert your code here to add the layer</span>
    <span class="c1"># nn.Linear(...),</span>
    <span class="c1"># insert the activation function</span>
    <span class="c1"># ....</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Model structure </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model structure 

 Sequential(
  (0): Linear(in_features=784, out_features=32, bias=True)
  (1): ReLU()
)
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/Bonus_Autoencoders/solutions/Bonus_Tutorial1_Solution_e7182519.py"><em>Click for solution</em></a></p>
<p><strong>SAMPLE OUTPUT</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Sequential</span><span class="p">(</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Sigmoid</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">output_test</span><span class="p">],</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_2067</span><span class="o">/</span><span class="mf">2898546608.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span>   <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">output_test</span><span class="p">],</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>

<span class="nn">/tmp/ipykernel_2067/1528605679.py</span> in <span class="ni">plot_row</span><span class="nt">(images, show_n, image_shape)</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span> 
<span class="g g-Whitespace">    </span><span class="mi">250</span>     <span class="k">if</span> <span class="n">image_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">251</span>       <span class="n">items</span> <span class="o">=</span> <span class="n">items</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">image_shape</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span> 
<span class="g g-Whitespace">    </span><span class="mi">253</span>     <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">items</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="ne">ValueError</span>: cannot reshape array of size 320 into shape (28,28)
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_40_1.png" src="../../../_images/Bonus_Tutorial1_40_1.png"/>
</div>
</div>
</div>
</div>
<div class="section" id="train-autoencoder-32d">
<h2>Train autoencoder (32D)<a class="headerlink" href="#train-autoencoder-32d" title="Permalink to this headline">¶</a></h2>
<p>The function <code class="docutils literal notranslate"><span class="pre">runSGD</span></code> trains the autoencoder with stochastic gradient descent using Adam optimizer (<code class="docutils literal notranslate"><span class="pre">optim.Adam</span></code>) and provides a choice between Mean Square Errors (MSE  with <code class="docutils literal notranslate"><span class="pre">nn.MSELoss</span></code>) and Binary Cross-entropy (BCE with <code class="docutils literal notranslate"><span class="pre">nn.BCELoss</span></code>).</p>
<p>The figures below illustrate these losses, where <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is the output value, and <span class="math notranslate nohighlight">\(Y\)</span> is the target value.</p>
<p><img alt="MSE loss" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/mse.png"/></p>
<p><img alt="BCE loss" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/bce.png"/></p>
<p>Train the network for <code class="docutils literal notranslate"><span class="pre">n_epochs=10</span></code> epochs and <code class="docutils literal notranslate"><span class="pre">batch_size=64</span></code> with <code class="docutils literal notranslate"><span class="pre">runSGD</span></code> and MSE loss, and visualize a few reconstructed samples.</p>
<p>Please execute the cells below to construct and train the model!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'mse'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 10	 0.0285		 0.0281
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 10	 0.0214		 0.0211
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3 / 10	 0.0180		 0.0176
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 / 10	 0.0170		 0.0166
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5 / 10	 0.0159		 0.0156
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6 / 10	 0.0156		 0.0152
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7 / 10	 0.0154		 0.0150
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8 / 10	 0.0153		 0.0149
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9 / 10	 0.0152		 0.0149
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 / 10	 0.0151		 0.0148
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_42_11.png" src="../../../_images/Bonus_Tutorial1_42_11.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">],</span> <span class="n">output_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]],</span>
         <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_43_0.png" src="../../../_images/Bonus_Tutorial1_43_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_43_1.png" src="../../../_images/Bonus_Tutorial1_43_1.png"/>
</div>
</div>
</div>
<div class="section" id="choose-the-loss-function">
<h2>Choose the loss function<a class="headerlink" href="#choose-the-loss-function" title="Permalink to this headline">¶</a></h2>
<p>The loss function determines what the network is optimizing during training, and this translates to the visual aspect of reconstructed images.</p>
<p>For example, isolated black pixels in the middle of white regions are very unlikely and look noisy. The network can prioritize avoiding such scenarios by maximally penalizing white pixels that turn out black and vice-versa.</p>
<p>The figure below compares MSE with BCE with a target pixel value <span class="math notranslate nohighlight">\(Y=1\)</span>, and the output ranging from <span class="math notranslate nohighlight">\(\hat{Y}\in [0, 1]\)</span>. The MSE loss has a gentle quadratic rise in this range. Notice how BCE loss dramatically increases for dark pixels <span class="math notranslate nohighlight">\(\hat{Y}\)</span> lower than 0.4.</p>
<p><img alt="bce vs. MSE loss" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/bce-mse.png"/></p>
<p>Let’s look at their derivatives <span class="math notranslate nohighlight">\(d\,\text{Loss}/d\,\hat{Y}\)</span> to make this comparison more objective. The derivative of MSE loss is linear with slope <span class="math notranslate nohighlight">\(-2\)</span>, whereas BCE takes off as <span class="math notranslate nohighlight">\(1/\hat{Y}\)</span> for dark pixel values (why?).</p>
<p><img alt="bce vs. MSE loss" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/bce-mse-dloss.png"/></p>
<p>We reduced the plotting range to <span class="math notranslate nohighlight">\([0.05, 1]\)</span> to share the same y-axis scale for both loss functions (why?).</p>
<p>Let’s switch to BCE loss and verify the effects of maximally penalizing white pixels that turn out black and vice-versa. The visual differences between losses will be subtle since the network is converging well in both cases.</p>
<p><strong>Look for isolated white/black pixel areas in MSE loss reconstructions.</strong></p>
<p>We will first retrain under MSE loss for <code class="docutils literal notranslate"><span class="pre">2</span></code> epochs to accentuate differences, and similarly under BCE loss.</p>
<p>Please execute the cells below to train with MSE and BCE, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'mse'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 2	 0.0268		 0.0264
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 2	 0.0187		 0.0183

MSE	 0.0187		 0.0183
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BCE	 0.1348		 0.1326
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_45_4.png" src="../../../_images/Bonus_Tutorial1_45_4.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">],</span> <span class="n">output_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]],</span>
         <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_46_0.png" src="../../../_images/Bonus_Tutorial1_46_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_46_1.png" src="../../../_images/Bonus_Tutorial1_46_1.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 2	 0.1406		 0.1391
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 2	 0.1181		 0.1165

MSE	 0.0176		 0.0172
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BCE	 0.1181		 0.1165
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_47_4.png" src="../../../_images/Bonus_Tutorial1_47_4.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">],</span> <span class="n">output_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]],</span>
         <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_48_0.png" src="../../../_images/Bonus_Tutorial1_48_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_48_1.png" src="../../../_images/Bonus_Tutorial1_48_1.png"/>
</div>
</div>
</div>
<div class="section" id="design-ann-autoencoder-2d">
<h2>Design ANN autoencoder (2D)<a class="headerlink" href="#design-ann-autoencoder-2d" title="Permalink to this headline">¶</a></h2>
<p>Reducing the number of bottleneck units to <code class="docutils literal notranslate"><span class="pre">encoding_size=2</span></code> generates a 2D latent space as for PCA before. The coordinates <span class="math notranslate nohighlight">\((z_1, z_2)\)</span>  of the encoder map represent unit activations in the bottleneck layer.</p>
<p> </p>
<p><img alt="encoder map for autoencoder" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/latent_space_plots_ae.png"/></p>
<p> </p>
<p>The <code class="docutils literal notranslate"><span class="pre">encoder</span></code> component provides (<span class="math notranslate nohighlight">\(z_1, z_2\)</span>) coordinates in latent space, and the <code class="docutils literal notranslate"><span class="pre">decoder</span></code> component generates image reconstructions from (<span class="math notranslate nohighlight">\(z_1, z_2\)</span>). Specifying a sequence of layers from the autoencoder network defines these sub-networks.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>
</pre></div>
</div>
<p>This architecture works well with a bottleneck layer with 32 units but fails to converge with two units.  Check the exercises in <em>Bonus</em> section to understand this failure more and two options to address it: better weight initialization and changing the activation function.</p>
<p>Here we opt for <a class="reference external" href="https://arxiv.org/abs/1502.01852">PReLU units</a> in the bottleneck layer to add negative activations with a learnable parameter. This change affords additional wiggle room for the autoencoder to model data with only two units in the bottleneck layer.</p>
<p><img alt="PreLU unit" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/prelu.png"/></p>
<p><strong>Instructions</strong></p>
<ul class="simple">
<li><p>Please execute the cells below:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Autoencoder </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Autoencoder 

 Sequential(
  (0): Linear(in_features=784, out_features=2, bias=True)
  (1): PReLU(num_parameters=1)
  (2): Linear(in_features=2, out_features=784, bias=True)
  (3): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Encoder </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">encoder</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Encoder 

 Sequential(
  (0): Linear(in_features=784, out_features=2, bias=True)
  (1): PReLU(num_parameters=1)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Decoder </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">decoder</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Decoder 

 Sequential(
  (2): Linear(in_features=2, out_features=784, bias=True)
  (3): Sigmoid()
)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-the-autoencoder-2d">
<h2>Train the autoencoder (2D)<a class="headerlink" href="#train-the-autoencoder-2d" title="Permalink to this headline">¶</a></h2>
<p>Train the network for <code class="docutils literal notranslate"><span class="pre">n_epochs=10</span></code> epochs and <code class="docutils literal notranslate"><span class="pre">batch_size=64</span></code> with <code class="docutils literal notranslate"><span class="pre">runSGD</span></code> and BCE loss, and visualize latent space.</p>
<p>Please execute the cells below to train the autoencoder!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># train the autoencoder</span>
<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 10	 0.2632		 0.2628
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 10	 0.2565		 0.2562
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3 / 10	 0.2505		 0.2504
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 / 10	 0.2443		 0.2441
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5 / 10	 0.2394		 0.2391
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6 / 10	 0.2358		 0.2354
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7 / 10	 0.2329		 0.2323
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8 / 10	 0.2310		 0.2304
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9 / 10	 0.2297		 0.2290
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 / 10	 0.2289		 0.2280
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_54_11.png" src="../../../_images/Bonus_Tutorial1_54_11.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">latent_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_latent_generative</span><span class="p">(</span><span class="n">latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span>
                       <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_55_0.png" src="../../../_images/Bonus_Tutorial1_55_0.png"/>
</div>
</div>
</div>
<div class="section" id="expressive-power-in-2d">
<h2>Expressive power in 2D<a class="headerlink" href="#expressive-power-in-2d" title="Permalink to this headline">¶</a></h2>
<p>The latent space representation of shallow autoencoder with a 2D bottleneck is similar to that of PCA. How can this linear dimensionality reduction technique be comparable to our non-linear autoencoder?</p>
<p>Training an autoencoder with linear activation functions under MSE loss is <a class="reference external" href="https://arxiv.org/abs/1804.10253">very similar to performing PCA</a>. Using piece-wise linear units, sigmoidal output unit, and BCE loss doesn’t seem to change this behavior qualitatively. The network lacks capacity in terms of learnable parameters to make good use of its non-linear operations and capture non-linear aspects of the data.</p>
<p>The similarity between representations is apparent when plotting decoder maps side-by-side. Look for classes of digits that cluster successfully, and those still mixing with others.</p>
<p>Execute the cell below for a PCA vs. Autoencoder (2D) comparison!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_latent_ab</span><span class="p">(</span><span class="n">pca_latent_test</span><span class="p">,</span> <span class="n">latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>
               <span class="n">title_a</span><span class="o">=</span><span class="s1">'PCA'</span><span class="p">,</span> <span class="n">title_b</span><span class="o">=</span><span class="s1">'Autoencoder (2D)'</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we got comfortable with the basic techniques to create and visualize low-dimensional representations and build shallow autoencoders.</p>
<p><strong>We saw that PCA and shallow autoencoder have similar expressive power in 2D latent space, despite the autoencoder’s non-linear character.</strong></p>
<p>The shallow autoencoder lacks learnable parameters to take advantage of non-linear operations in encoding/decoding and capture non-linear patterns in data.</p>
<p>The next tutorial extends the autoencoder architecture to learn richer internal representations of data required for tackling the MNIST cognitive task.</p>
<div class="section" id="video-3-wrap-up">
<h2>Video 3: Wrap-up<a class="headerlink" href="#video-3-wrap-up" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "77325d00cd5d4089bc3e34dca52ac8c8"}
</script></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<div class="section" id="failure-mode-with-relu-units-in-2d">
<h2>Failure mode with ReLU units in 2D<a class="headerlink" href="#failure-mode-with-relu-units-in-2d" title="Permalink to this headline">¶</a></h2>
<p>An architecture with two units in the bottleneck layer, ReLU units, and default weight initialization may fail to converge, depending on the minibatch sequence, choice of the optimizer, etc. To illustrate this failure mode, we first set the random number generators (RNGs) to reproduce an example of failed convergence:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Afterward, we set the RNGs to reproduce an example of successful convergence:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p> </p>
<p>Train the network for <code class="docutils literal notranslate"><span class="pre">n_epochs=10</span></code> epochs and <code class="docutils literal notranslate"><span class="pre">batch_size=64</span></code> and check the encoder map and reconstruction grid in each case.</p>
<p>We then activate our x-ray vision and check the distribution of weights in encoder and decoder components. Recall that encoder maps input pixels to bottleneck units (encoder weights <code class="docutils literal notranslate"><span class="pre">shape=(2,</span> <span class="pre">784)</span></code>), and decoder maps bottleneck units to output pixels (decoder weights <code class="docutils literal notranslate"><span class="pre">shape=(784,</span> <span class="pre">2)</span></code>).</p>
<p>Network models often initialize with random weights close to 0. The default weight initialization for linear layers in Pytorch is sampled from a uniform distribution <code class="docutils literal notranslate"><span class="pre">[-limit,</span> <span class="pre">limit]</span></code> with <code class="docutils literal notranslate"><span class="pre">limit=1/sqrt(fan_in)</span></code>, where <code class="docutils literal notranslate"><span class="pre">fan_in</span></code> is the number of input units in the weight tensor.</p>
<p>We compare the distribution of weights on network initialization to that after training. Weights that fail to learn during training keep to their initial distribution. On the other hand, weights that are adjusted by SGD during training are likely to have a change in distribution.</p>
<p>Encoder weights may even acquire a bell-shaped form. This effect may be related to the following: SGD adds a sequence of positive and negative increments to each initial weight. The Central Limit Theorem (CLT) would predict a gaussian histogram if increments were independent in sequences and between sequences. The deviation from gaussianity is a measure of the inter-dependency of SGD increments.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Please execute the cells below</p></li>
<li><p>Start with <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span> <span class="pre">=</span> <span class="pre">0</span></code> for an example of failed convergence</p></li>
<li><p>Check encoder mapping collapsed into a single axis</p></li>
<li><p>Verify collapsed dimension corresponds to unchanged weights</p></li>
<li><p>Change <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span> <span class="pre">=</span> <span class="pre">1</span></code> for an example of successful convergence</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">help(get_layer_weights)</span></code> for additional details on retrieving learnable parameters (weights and biases)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># set PyTorch RNG seed</span>
<span class="n">torch_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># reset RNG for weight initialization</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># retrieve weights and biases from the encoder before training</span>
<span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_init</span><span class="p">,</span> <span class="n">decoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># reset RNG for minibatch sequence</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># train the autoencoder</span>
<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># retrieve weights and biases from the encoder after training</span>
<span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">encoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_train</span><span class="p">,</span> <span class="n">decoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 10	 0.2654		 0.2651
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 10	 0.2600		 0.2598
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3 / 10	 0.2573		 0.2572
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 / 10	 0.2554		 0.2552
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5 / 10	 0.2539		 0.2538
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6 / 10	 0.2524		 0.2523
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7 / 10	 0.2509		 0.2508
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8 / 10	 0.2493		 0.2491
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9 / 10	 0.2476		 0.2474
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 / 10	 0.2460		 0.2458
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_63_11.png" src="../../../_images/Bonus_Tutorial1_63_11.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">latent_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_latent_generative</span><span class="p">(</span><span class="n">latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">],</span> <span class="n">output_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]],</span>
         <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_64_0.png" src="../../../_images/Bonus_Tutorial1_64_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_64_1.png" src="../../../_images/Bonus_Tutorial1_64_1.png"/>
<img alt="../../../_images/Bonus_Tutorial1_64_2.png" src="../../../_images/Bonus_Tutorial1_64_2.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights_ab</span><span class="p">(</span><span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">decoder_w_init</span><span class="p">,</span>
                <span class="n">decoder_w_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_65_0.png" src="../../../_images/Bonus_Tutorial1_65_0.png"/>
</div>
</div>
<div class="section" id="exercise-3-choosing-weight-initialization">
<h3>Exercise 3: Choosing weight initialization<a class="headerlink" href="#exercise-3-choosing-weight-initialization" title="Permalink to this headline">¶</a></h3>
<p>An improved weight initialization for ReLU units avoids the failure mode from the previous exercise. A popular choice for rectifier units is <em>Kaiming uniform</em>: sampling from uniform distribution <span class="math notranslate nohighlight">\(\mathcal{U}(-limit, limit)\)</span> with <span class="math notranslate nohighlight">\(limit=\sqrt{6/fan\_in}\)</span>, where <span class="math notranslate nohighlight">\(fan\_in\)</span> is the number of input units in the weight tensor (see the relevant <a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">article</a> for details). Example of resetting all autoencoder weights to Kaiming uniform:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights_kaiming_uniform</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative is to sample from a gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu, \sigma^2)\)</span> with <span class="math notranslate nohighlight">\(\mu=0\)</span> and <span class="math notranslate nohighlight">\(\sigma=1/\sqrt{fan\_in}\)</span>. Example for reseting all but the two last autoencoder layers to Kaiming normal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights_kaiming_normal</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information on weight initialization, the references below are a good starting point:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient Backprop</a></p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></p></li>
<li><p><a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</a></p></li>
</ul>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Reset encoder weights with <code class="docutils literal notranslate"><span class="pre">init_weights_kaiming_uniform</span></code></p></li>
<li><p>Compare with resetting with <code class="docutils literal notranslate"><span class="pre">init_weights_kaiming_normal</span></code></p></li>
<li><p>See <code class="docutils literal notranslate"><span class="pre">help(init_weights_kaiming_uniform)</span></code> and <code class="docutils literal notranslate"><span class="pre">help(init_weights_kaiming_normal)</span></code> for additional details</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># set PyTorch RNG seed</span>
<span class="n">torch_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># reset RNGs for weight initialization</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">######################################################################</span>
<span class="c1">## TODO for students: reset encoder weights and biases</span>
<span class="c1">######################################################################</span>
<span class="c1"># reset encoder weights and biases</span>
<span class="c1"># encoder.apply(...)</span>

<span class="c1"># retrieve weights and biases from the encoder before training</span>
<span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_init</span><span class="p">,</span> <span class="n">decoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># reset RNGs for minibatch sequence</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># train the autoencoder</span>
<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># retrieve weights and biases from the encoder after training</span>
<span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">encoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_train</span><span class="p">,</span> <span class="n">decoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 10	 0.2654		 0.2651
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 10	 0.2600		 0.2598
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3 / 10	 0.2573		 0.2572
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 / 10	 0.2554		 0.2552
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5 / 10	 0.2539		 0.2538
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6 / 10	 0.2524		 0.2523
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7 / 10	 0.2509		 0.2508
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8 / 10	 0.2493		 0.2491
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9 / 10	 0.2476		 0.2474
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 / 10	 0.2460		 0.2458
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_67_11.png" src="../../../_images/Bonus_Tutorial1_67_11.png"/>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/Bonus_Autoencoders/solutions/Bonus_Tutorial1_Solution_9d6c1017.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/Bonus_Autoencoders/static/Bonus_Tutorial1_Solution_9d6c1017_11.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/Bonus_Autoencoders/static/Bonus_Tutorial1_Solution_9d6c1017_11.png" style="width: 1116.0px; height: 827.0px;"/></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">latent_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>
  <span class="n">output_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_latent_generative</span><span class="p">(</span><span class="n">latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span>
                       <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">],</span> <span class="n">output_test</span><span class="p">[</span><span class="n">test_selected_idx</span><span class="p">]],</span>
         <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_69_0.png" src="../../../_images/Bonus_Tutorial1_69_0.png"/>
<img alt="../../../_images/Bonus_Tutorial1_69_1.png" src="../../../_images/Bonus_Tutorial1_69_1.png"/>
<img alt="../../../_images/Bonus_Tutorial1_69_2.png" src="../../../_images/Bonus_Tutorial1_69_2.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights_ab</span><span class="p">(</span><span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">decoder_w_init</span><span class="p">,</span>
                <span class="n">decoder_w_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_70_0.png" src="../../../_images/Bonus_Tutorial1_70_0.png"/>
</div>
</div>
</div>
</div>
<div class="section" id="choose-the-activation-function">
<h2>Choose the activation function<a class="headerlink" href="#choose-the-activation-function" title="Permalink to this headline">¶</a></h2>
<p>An alternative to specific weight initialization is to choose an activation unit that performs better in this context. We will use <a class="reference external" href="https://arxiv.org/abs/1502.01852">PReLU</a> units in the bottleneck layer, which adds a learnable parameter for negative activations.</p>
<p>This change affords a little bit more of wiggle room for the autoencoder to model data compared to ReLU units.</p>
<p><img alt="PreLU unit" src="https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/prelu.png"/></p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Please execute the cells below</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">encoding_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># set PyTorch RNG seed</span>
<span class="n">torch_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># reset RNGs for weight initialization</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">encoding_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
    <span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

<span class="c1"># retrieve weights and biases from the encoder before training</span>
<span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_init</span><span class="p">,</span> <span class="n">decoder_b_init</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># reset RNGs for minibatch sequence</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">torch_seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># train the autoencoder</span>
<span class="n">runSGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_train</span><span class="p">,</span> <span class="n">input_test</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">'bce'</span><span class="p">,</span>
       <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># retrieve weights and biases from the encoder after training</span>
<span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">encoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">decoder_w_train</span><span class="p">,</span> <span class="n">decoder_b_train</span> <span class="o">=</span> <span class="n">get_layer_weights</span><span class="p">(</span><span class="n">decoder</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 	 Loss train 	 Loss test
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1 / 10	 0.2622		 0.2619
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 / 10	 0.2560		 0.2558
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3 / 10	 0.2509		 0.2505
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 / 10	 0.2457		 0.2451
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5 / 10	 0.2411		 0.2405
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6 / 10	 0.2378		 0.2373
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7 / 10	 0.2356		 0.2352
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8 / 10	 0.2340		 0.2337
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9 / 10	 0.2327		 0.2325
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 / 10	 0.2318		 0.2319
</pre></div>
</div>
<img alt="../../../_images/Bonus_Tutorial1_72_11.png" src="../../../_images/Bonus_Tutorial1_72_11.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">latent_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_test</span><span class="p">)</span>

<span class="n">plot_latent_generative</span><span class="p">(</span><span class="n">latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_73_0.png" src="../../../_images/Bonus_Tutorial1_73_0.png"/>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights_ab</span><span class="p">(</span><span class="n">encoder_w_init</span><span class="p">,</span> <span class="n">encoder_w_train</span><span class="p">,</span> <span class="n">decoder_w_init</span><span class="p">,</span>
                <span class="n">decoder_w_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/Bonus_Tutorial1_74_0.png" src="../../../_images/Bonus_Tutorial1_74_0.png"/>
</div>
</div>
</div>
<div class="section" id="qualitative-analysis-nmf">
<h2>Qualitative analysis NMF<a class="headerlink" href="#qualitative-analysis-nmf" title="Permalink to this headline">¶</a></h2>
<p>We proceed with <em>non-negative matrix factorization (NMF)</em> using <code class="docutils literal notranslate"><span class="pre">sk.decomposition.NMF</span></code> (docs <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">here</a>).</p>
<p>A product of positive matrices <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(H\)</span> approximates data matrix <span class="math notranslate nohighlight">\(X\)</span>, i.e., <span class="math notranslate nohighlight">\(X \approx W H\)</span>.</p>
<p>The columns of <span class="math notranslate nohighlight">\(W\)</span> play the same role as the principal components in PCA.</p>
<p>Digit classes <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code> are the furthest apart in latent space and better clustered.</p>
<p>Looking at the first component, we see that images gradually resemble digit class <code class="docutils literal notranslate"><span class="pre">0</span></code>. A mix between digits classes <code class="docutils literal notranslate"><span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">9</span></code> in the second component shows a similar progression.</p>
<p>That data is shifted by <code class="docutils literal notranslate"><span class="pre">0.5</span></code> to avoid failure modes near <code class="docutils literal notranslate"><span class="pre">0</span></code> - this is probably related to our scaling choice. Try it without shifting by <code class="docutils literal notranslate"><span class="pre">0.5</span></code>.</p>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">init='random'</span></code> scales the initial non-negative random matrices and often provides better results - try it as well!</p>
<p>Please execute the cells below, to run NMF.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nmf</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'random'</span><span class="p">)</span>

<span class="n">nmf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">input_train</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">nmf_latent_test</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_test</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">plot_latent_generative</span><span class="p">(</span><span class="n">nmf_latent_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">nmf</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">,</span>
                       <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_2067</span><span class="o">/</span><span class="mf">1930777134.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">nmf</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'random'</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> 
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">nmf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">input_train</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> 
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">nmf_latent_test</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_test</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py</span> in <span class="ni">fit</span><span class="nt">(self, X, y, **params)</span>
<span class="g g-Whitespace">   </span><span class="mi">1340</span>         <span class="bp">self</span>
<span class="g g-Whitespace">   </span><span class="mi">1341</span>         <span class="s2">"""</span>
<span class="ne">-&gt; </span><span class="mi">1342</span><span class="s2">         self.fit_transform(X, **params)</span>
<span class="g g-Whitespace">   </span><span class="mi">1343</span><span class="s2">         return self</span>
<span class="g g-Whitespace">   </span><span class="mi">1344</span><span class="s2"> </span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py</span> in <span class="ni">fit_transform</span><span class="nt">(self, X, y, W, H)</span>
<span class="g g-Whitespace">   </span><span class="mi">1315</span><span class="s2">                 l1_ratio=self.l1_ratio, regularization=self.regularization,</span>
<span class="g g-Whitespace">   </span><span class="mi">1316</span><span class="s2">                 random_state=self.random_state, verbose=self.verbose,</span>
<span class="ne">-&gt; </span><span class="mi">1317</span><span class="s2">                 shuffle=self.shuffle)</span>
<span class="g g-Whitespace">   </span><span class="mi">1318</span><span class="s2"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1319</span><span class="s2">         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/utils/validation.py</span> in <span class="ni">inner_f</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span><span class="s2">             extra_args = len(args) - len(all_args)</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span><span class="s2">             if extra_args &lt;= 0:</span>
<span class="ne">---&gt; </span><span class="mi">63</span><span class="s2">                 return f(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">65</span><span class="s2">             # extra_args &gt; 0</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py</span> in <span class="ni">non_negative_factorization</span><span class="nt">(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)</span>
<span class="g g-Whitespace">   </span><span class="mi">1077</span><span class="s2">                                                verbose=verbose,</span>
<span class="g g-Whitespace">   </span><span class="mi">1078</span><span class="s2">                                                shuffle=shuffle,</span>
<span class="ne">-&gt; </span><span class="mi">1079</span><span class="s2">                                                random_state=random_state)</span>
<span class="g g-Whitespace">   </span><span class="mi">1080</span><span class="s2">     elif solver == 'mu':</span>
<span class="g g-Whitespace">   </span><span class="mi">1081</span><span class="s2">         W, H, n_iter = _fit_multiplicative_update(X, W, H, beta_loss, max_iter,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py</span> in <span class="ni">_fit_coordinate_descent</span><span class="nt">(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)</span>
<span class="g g-Whitespace">    </span><span class="mi">516</span><span class="s2">         # Update W</span>
<span class="g g-Whitespace">    </span><span class="mi">517</span><span class="s2">         violation += _update_coordinate_descent(X, W, Ht, l1_reg_W,</span>
<span class="ne">--&gt; </span><span class="mi">518</span><span class="s2">                                                 l2_reg_W, shuffle, rng)</span>
<span class="g g-Whitespace">    </span><span class="mi">519</span><span class="s2">         # Update H</span>
<span class="g g-Whitespace">    </span><span class="mi">520</span><span class="s2">         if update_H:</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py</span> in <span class="ni">_update_coordinate_descent</span><span class="nt">(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)</span>
<span class="g g-Whitespace">    </span><span class="mi">414</span><span class="s2"> </span>
<span class="g g-Whitespace">    </span><span class="mi">415</span><span class="s2">     HHt = np.dot(Ht.T, Ht)</span>
<span class="ne">--&gt; </span><span class="mi">416</span><span class="s2">     XHt = safe_sparse_dot(X, Ht)</span>
<span class="g g-Whitespace">    </span><span class="mi">417</span><span class="s2"> </span>
<span class="g g-Whitespace">    </span><span class="mi">418</span><span class="s2">     # L2 regularization corresponds to increase of the diagonal of HHt</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/utils/validation.py</span> in <span class="ni">inner_f</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span><span class="s2">             extra_args = len(args) - len(all_args)</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span><span class="s2">             if extra_args &lt;= 0:</span>
<span class="ne">---&gt; </span><span class="mi">63</span><span class="s2">                 return f(*args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">65</span><span class="s2">             # extra_args &gt; 0</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/sklearn/utils/extmath.py</span> in <span class="ni">safe_sparse_dot</span><span class="nt">(a, b, dense_output)</span>
<span class="g g-Whitespace">    </span><span class="mi">150</span><span class="s2">             ret = np.dot(a, b)</span>
<span class="g g-Whitespace">    </span><span class="mi">151</span><span class="s2">     else:</span>
<span class="ne">--&gt; </span><span class="mi">152</span><span class="s2">         ret = a @ b</span>
<span class="g g-Whitespace">    </span><span class="mi">153</span><span class="s2"> </span>
<span class="g g-Whitespace">    </span><span class="mi">154</span><span class="s2">     if (sparse.issparse(a) and sparse.issparse(b)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nmf_components</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">components_</span>

<span class="n">plot_row</span><span class="p">(</span><span class="n">nmf_components</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nmf_output_test</span> <span class="o">=</span> <span class="n">nmf</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">nmf_latent_test</span><span class="p">)</span>

<span class="n">plot_row</span><span class="p">([</span><span class="n">input_test</span><span class="p">,</span> <span class="n">nmf_output_test</span><span class="p">],</span> <span class="n">image_shape</span><span class="o">=</span><span class="n">image_shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/Bonus_Autoencoders/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="../Bonus_Intro.html" id="prev-link" title="previous page">Intro</a>
<a class="right-next" href="Bonus_Tutorial2.html" id="next-link" title="next page">Tutorial 2: Autoencoder extensions</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>