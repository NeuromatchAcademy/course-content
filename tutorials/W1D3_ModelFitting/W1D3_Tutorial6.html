
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neuromatch Academy: Week 1, Day 3, Tutorial 6 &#8212; Neuromatch Computational Neuroscience</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="W1D4 - Machine Learning" href="../W1D4_MachineLearning/README.html" />
    <link rel="prev" title="Neuromatch Academy: Week 1, Day 3, Tutorial 5" href="W1D3_Tutorial5.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   NMA 2021
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preliminary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W0D1_PythonWorkshop1/README.html">
   Python Workshop 1
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W0D1_PythonWorkshop1/W0D1_Tutorial1.html">
     Neuromatch Academy: Week 0, Day 1, Tutorial 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W0D2_PythonWorkshop2/README.html">
   Python Workshop 2
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W0D2_PythonWorkshop2/W0D2_Tutorial1.html">
     Neuromatch Academy: Week 0, Day 2, Tutorial 1
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Week 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D1_ModelTypes/README.html">
   Model Types
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 1, Tutorial 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D2_ModelingPractice/README.html">
   Modeling Practice
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D2_ModelingPractice/W1D2_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 2, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D2_ModelingPractice/W1D2_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 2, Tutorial 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="README.html">
   Model Fitting
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial4.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 4
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial5.html">
     Neuromatch Academy: Week 1, Day 3, Tutorial 5
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Neuromatch Academy: Week 1, Day 3, Tutorial 6
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D4_MachineLearning/README.html">
   Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D4_MachineLearning/W1D4_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 4, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D4_MachineLearning/W1D4_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 4, Tutorial 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D5_DimensionalityReduction/README.html">
   Dimensionality Reduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial2.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial3.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial4.html">
     Neuromatch Academy: Week 1, Day 5, Tutorial 4
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Week 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D2_LinearSystems/README.html">
   Linear Systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial1.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial2.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial3.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial4.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D3_DecisionMaking/README.html">
   Decision Making
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D4_OptimalControl/README.html">
   Optimal Control
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/tutorials/W1D3_ModelFitting/W1D3_Tutorial6.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/NeuromatchAcademy/course_content/blob/master/book/tutorials/W1D3_ModelFitting/W1D3_Tutorial6.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neuromatch Academy: Week 1, Day 3, Tutorial 6
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-selection-cross-validation">
   Model Selection: Cross-validation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-cross-validation">
   Section 1: Cross-validation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1-implement-cross-validation">
     Exercise 1: Implement cross-validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#akaike-s-information-criterion-aic">
     Akaike’s Information Criterion (AIC)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bonus-exercise-compute-aic">
       Bonus Exercise: Compute AIC
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D3_ModelFitting/W1D3_Tutorial6.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="neuromatch-academy-week-1-day-3-tutorial-6">
<h1>Neuromatch Academy: Week 1, Day 3, Tutorial 6<a class="headerlink" href="#neuromatch-academy-week-1-day-3-tutorial-6" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="model-selection-cross-validation">
<h1>Model Selection: Cross-validation<a class="headerlink" href="#model-selection-cross-validation" title="Permalink to this headline">¶</a></h1>
<p><strong>Content creators</strong>: Pierre-Étienne Fiquet, Anqi Wu, Alex Hyafil with help from Ella Batty</p>
<p><strong>Content reviewers</strong>: Lina Teichmann, Patrick Mineault, Michael Waskom</p>
<hr class="docutils" />
<p>#Tutorial Objectives</p>
<p>This is Tutorial 6 of a series on fitting models to data. We start with simple linear regression, using least squares optimization (Tutorial 1) and Maximum Likelihood Estimation (Tutorial 2). We will use bootstrapping to build confidence intervals around the inferred linear model parameters (Tutorial 3). We’ll finish our exploration of regression models by generalizing to multiple linear regression and polynomial regression (Tutorial 4). We end by learning how to choose between these various models. We discuss the bias-variance trade-off (Tutorial 5) and Cross Validation for model selection (Tutorial 6).</p>
<p>Tutorial objectives:</p>
<ul class="simple">
<li><p>Implement cross-validation and use it to compare polynomial regression model</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure Settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper functions</span>
<span class="k">def</span> <span class="nf">ordinary_least_squares</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Ordinary least squares estimator for linear regression.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): design matrix of shape (n_samples, n_regressors)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: estimated parameter values of shape (n_regressors)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create the design matrix of inputs for use in polynomial regression</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    order (scalar): polynomial regression order</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: design matrix for polynomial regression of shape (samples, order+1)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Broadcast to shape (n x 1)</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

  <span class="c1">#if x has more than one feature, we don&#39;t want multiple columns of ones so we assign</span>
  <span class="c1"># x^0 here</span>
  <span class="n">design_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Loop through rest of degrees and stack columns</span>
  <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">design_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">design_matrix</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="n">degree</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">design_matrix</span>


<span class="k">def</span> <span class="nf">solve_poly_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fit a polynomial regression model for each order 0 through max_order.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>
<span class="sd">    max_order (scalar): max order for polynomial fits</span>

<span class="sd">  Returns:</span>
<span class="sd">    dict: fitted weights for each polynomial model (dict key is order)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Create a dictionary with polynomial order as keys, and np array of theta</span>
  <span class="c1"># (weights) as the values</span>
  <span class="n">theta_hats</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="c1"># Loop over polynomial orders from 0 through max_order</span>
  <span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
    <span class="n">this_theta</span> <span class="o">=</span> <span class="n">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="n">this_theta</span>

  <span class="k">return</span> <span class="n">theta_hats</span>

<span class="k">def</span> <span class="nf">evaluate_poly_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">,</span> <span class="n">max_order</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Evaluates MSE of polynomial regression models on data</span>

<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">      y (ndarray): vector of measurements of shape (n_samples)</span>
<span class="sd">      theta_hat (dict):  fitted weights for each polynomial model (dict key is order)</span>
<span class="sd">      max_order (scalar): max order of polynomial fit</span>

<span class="sd">    Returns</span>
<span class="sd">      (ndarray): mean squared error for each order, shape (max_order)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
      <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_design</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>
      <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>
      <span class="n">mse</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">residuals</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mse</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-1-cross-validation">
<h1>Section 1: Cross-validation<a class="headerlink" href="#section-1-cross-validation" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 1: Cross-Validation</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;OtKw0rSRxo4&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtube.com/watch?v=&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>We now have multiple choices for which model to use for a given problem: we could use linear regression, order 2 polynomial regression, order 3 polynomial regression, etc. As we saw in Tutorial 5, different models will have different quality of predictions, both on the training data and on the test data.</p>
<p>A commonly used method for model selection is to asks how well the model predicts new data that it hasn’t seen yet. But we don’t want to use test data to do this, otherwise that would mean using it during the training process! One approach is to use another kind of held-out data which we call <strong>validation data</strong>: we do not fit the model with this data but we use it to select our best model.</p>
<p>We often have a limited amount of data though (especially in neuroscience), so we do not want to further reduce our potential training data by reassigning some as validation. Luckily, we can use <strong>k-fold cross-validation</strong>! In k-fold cross validation, we divide up the training data into k subsets (that are called <em>folds</em>, see diagram below), train our model on the first k-1 folds, and then compute error on the last held-out fold. We can then repeat this process k times, once on each k-1 folds of the data. Each of these k instances (which are called <em>splits</em>, see diagram below) excludes a different fold from fitting. We then average the error of each of the k trained models on its held-out subset - this is the final measure of performance which we can use to do model selection.</p>
<p>To make this explicit, let’s say we have 1000 samples of training data and choose 4-fold cross-validation. Samples 0 - 250 would be subset 1, samples 250 - 500 subset 2, samples 500 - 750 subset 3, and samples 750-1000 subset 4. First, we train an order 3 polynomial regression on subsets 1, 2, 3 and evaluate on subset 4. Next, we train an order 3 polynomial model on subsets 1, 2, 4 and evalute on subset 3. We continue until we have 4 instances of a trained order 3 polynomial regression model, each with a different subset as held-out data, and average the held-out error from each instance.</p>
<p>We can now compare the error of different models to pick a model that generalizes well to held-out data. We can choose the measure of prediction quality to report error on the held-out subsets to suit our purposes. We will use MSE here but we could also use log likelihood of the data and so on.</p>
<p>As a final step, it is common to retrain this model on all of the training data (without subset divisions) to get our final model that we will evaluate on test data. This approach allows us to evaluate the quality of predictions on new data without sacrificing any of our precious training data.</p>
<p>Note that the held-out subsets are called either validation or test subsets. There is not a consensus and may depend on the exact use of k-fold cross validation. Sometimes people use k-fold cross validation to choose between different models/parameters to then apply to held-out test data and sometimes people report the averaged error on the held-out subsets as the model performance.   If you are doing the former (using k-fold cross validation for model selection), you must report performance on held-out test data! In this text/code, we will refer to them as validation subsets to differentiate from our completely held-out test data.</p>
<p>These steps are summarized in this diagram from Scikit-learn (<a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a>)</p>
<p><img alt="Diagram from Sklearn" src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" /></p>
<p>Importantly, we need to be very careful when dividing the data into subsets.  The held-out subset should not be used in any way to fit the model. We should not do any preprocessing (e.g. normalization) before we divide into subsets or the held-out subset could influence the training subsets. A lot of false-positives in cross-validation come from wrongly dividing.</p>
<p>An important consideration in the choice of model selection method are the relevant biases. If we just fit using MSE on training data, we will generally find that fits get better as we add more parameters because the model will overfit the data, as we saw in Tutorial 5. When using cross-validation, the bias is the other way around. Models with more parameters are more affected by variance so cross-validation will generally prefer models with fewer parameters.</p>
<p>We will again simulate some train and test data and fit polynomial regression models</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to simulate data and fit polynomial regression models</span>

<span class="c1">### Generate training data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_train_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">n_train_samples</span><span class="p">)</span>  <span class="c1"># sample from a uniform distribution over [-2, 2.5)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_train_samples</span><span class="p">)</span>  <span class="c1"># sample from a standard normal distribution</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x_train</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1">### Generate testing data</span>
<span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_test_samples</span><span class="p">)</span>  <span class="c1"># sample from a uniform distribution over [-2, 2.5)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)</span>  <span class="c1"># sample from a standard normal distribution</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x_test</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">noise</span>

<span class="c1">### Fit polynomial regression models</span>
<span class="n">max_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">theta_hats</span> <span class="o">=</span> <span class="n">solve_poly_reg</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">max_order</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="exercise-1-implement-cross-validation">
<h2>Exercise 1: Implement cross-validation<a class="headerlink" href="#exercise-1-implement-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>Given our set of models to evaluate (polynomial regression models with orders 0 through 5), we will use cross-validation to determine which model has the best predictions on new data according to MSE.</p>
<p>In this code, we split the data into 10 subsets using <code class="docutils literal notranslate"><span class="pre">Kfold</span></code> (from <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code>). <code class="docutils literal notranslate"><span class="pre">KFold</span></code> handles cross-validation subset splitting and train/val assignments.  In particular, the <code class="docutils literal notranslate"><span class="pre">Kfold.split</span></code> method returns an iterator which we can loop through. On each loop, this iterator assigns a different subset as validation and returns new training and validation indices with which to split the data.</p>
<p>We will loop through the 10 train/validation splits and fit several different polynomial regression models (with different orders) for each split. You will need to use the <code class="docutils literal notranslate"><span class="pre">solve_poly_reg</span></code> method from Tutorial 4 and <code class="docutils literal notranslate"><span class="pre">evaluate_poly_reg</span></code> from Tutorial 5 (already implemented in this notebook).</p>
<p>We will visualize the validation MSE over 10 splits of the data for each polynomial order using box plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cross_validate</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">max_order</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Compute MSE for k-fold validation for each order polynomial</span>

<span class="sd">  Args:</span>
<span class="sd">    x_train (ndarray): training data input vector of shape (n_samples)</span>
<span class="sd">    y_train (ndarray): training vector of measurements of shape (n_samples)</span>
<span class="sd">    max_order (scalar): max order of polynomial fit</span>
<span class="sd">    n_split (scalar): number of folds for k-fold validation</span>

<span class="sd">  Return:</span>
<span class="sd">    ndarray: MSE over splits for each model order, shape (n_splits, max_order + 1)</span>

<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Initialize the split method</span>
  <span class="n">kfold_iterator</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span>

  <span class="c1"># Initialize np array mse values for all models for each split</span>
  <span class="n">mse_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">i_split</span><span class="p">,</span> <span class="p">(</span><span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kfold_iterator</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x_train</span><span class="p">)):</span>

      <span class="c1"># Split up the overall training data into cross-validation training and validation sets</span>
      <span class="n">x_cv_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
      <span class="n">y_cv_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
      <span class="n">x_cv_val</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>
      <span class="n">y_cv_val</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>

      <span class="c1">#############################################################################</span>
      <span class="c1">## TODO for students: Fill in missing ... in code below to choose which data</span>
      <span class="c1">## to fit to and compute MSE for</span>
      <span class="c1"># Fill out function and remove</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Student exercise: implement cross-validation&quot;</span><span class="p">)</span>
      <span class="c1">#############################################################################</span>

      <span class="c1"># Fit models</span>
      <span class="n">theta_hats</span> <span class="o">=</span> <span class="o">...</span>

      <span class="c1"># Compute MSE</span>
      <span class="n">mse_this_split</span> <span class="o">=</span> <span class="o">...</span>

      <span class="n">mse_all</span><span class="p">[</span><span class="n">i_split</span><span class="p">]</span> <span class="o">=</span> <span class="n">mse_this_split</span>

  <span class="k">return</span> <span class="n">mse_all</span>


<span class="n">max_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="c1"># Uncomment below to test function</span>
<span class="c1"># mse_all = cross_validate(x_train, y_train, max_order, n_splits)</span>
<span class="c1"># plt.boxplot(mse_all, labels=np.arange(0, max_order + 1))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial Order&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Validation MSE&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Validation MSE over </span><span class="si">{</span><span class="n">n_splits</span><span class="si">}</span><span class="s1"> splits of the data&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">cross_validate</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">max_order</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Compute MSE for k-fold validation for each order polynomial</span>

<span class="sd">  Args:</span>
<span class="sd">    x_train (ndarray): training data input vector of shape (n_samples)</span>
<span class="sd">    y_train (ndarray): training vector of measurements of shape (n_samples)</span>
<span class="sd">    max_order (scalar): max order of polynomial fit</span>
<span class="sd">    n_split (scalar): number of folds for k-fold validation</span>

<span class="sd">  Return:</span>
<span class="sd">    ndarray: MSE over splits for each model order, shape (n_splits, max_order + 1)</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Initialize the split method</span>
  <span class="n">kfold_iterator</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span>

  <span class="c1"># Initialize np array mse values for all models for each split</span>
  <span class="n">mse_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">i_split</span><span class="p">,</span> <span class="p">(</span><span class="n">train_indices</span><span class="p">,</span> <span class="n">val_indices</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kfold_iterator</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x_train</span><span class="p">)):</span>

      <span class="c1"># Split up the overall training data into cross-validation training and validation sets</span>
      <span class="n">x_cv_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
      <span class="n">y_cv_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
      <span class="n">x_cv_val</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>
      <span class="n">y_cv_val</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>

      <span class="c1"># Fit models</span>
      <span class="n">theta_hats</span> <span class="o">=</span> <span class="n">solve_poly_reg</span><span class="p">(</span><span class="n">x_cv_train</span><span class="p">,</span> <span class="n">y_cv_train</span><span class="p">,</span> <span class="n">max_order</span><span class="p">)</span>

      <span class="c1"># Compute MSE</span>
      <span class="n">mse_this_split</span> <span class="o">=</span> <span class="n">evaluate_poly_reg</span><span class="p">(</span><span class="n">x_cv_val</span><span class="p">,</span> <span class="n">y_cv_val</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">,</span> <span class="n">max_order</span><span class="p">)</span>

      <span class="n">mse_all</span><span class="p">[</span><span class="n">i_split</span><span class="p">]</span> <span class="o">=</span> <span class="n">mse_this_split</span>

  <span class="k">return</span> <span class="n">mse_all</span>


<span class="n">max_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

  <span class="n">mse_all</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">max_order</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_all</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial Order&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Validation MSE&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Validation MSE over </span><span class="si">{</span><span class="n">n_splits</span><span class="si">}</span><span class="s1"> splits of the data&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Which polynomial order do you think is a better model of the data?</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>We need to use model selection methods to determine the best model to use for a given problem.</p>
<p>Cross-validation focuses on how well the model predicts new data.</p>
</div>
<hr class="docutils" />
<div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<div class="section" id="akaike-s-information-criterion-aic">
<h2>Akaike’s Information Criterion (AIC)<a class="headerlink" href="#akaike-s-information-criterion-aic" title="Permalink to this headline">¶</a></h2>
<p>In order to choose the best model for a given problem, we can ask how likely the data is under a given model. We want to choose a model that assigns high probability to the data. A commonly used method for model selection that uses this approach is <strong>Akaike’s Information Criterion (AIC)</strong>.</p>
<p>Essentially, AIC estimates how much information would be lost if the model predictions were used instead of the true data (the relative information value of the model). We compute the AIC for each model and choose the model with the lowest AIC. Note that AIC only tells us relative qualities, not absolute - we do not know from AIC how good our model is independent of others.</p>
<p>AIC strives for a good tradeoff between overfitting and underfitting by taking into account the complexity of the model and the information lost. AIC is calculated as:</p>
<div class="math notranslate nohighlight">
\[ AIC = 2K - 2 log(L)\]</div>
<p>where K is the number of parameters in your model and L is the likelihood that the model could have produced the output data.</p>
<p>Now we know what AIC is, we want to use it to pick between our polynomial regression models. We haven’t been thinking in terms of likelihoods though - so how will we calculate L?</p>
<p>As we saw in Tutorial 2, there is a link between mean squared error and the likelihood estimates for linear regression models that we can take advantage of.</p>
<p><em>Derivation time!</em></p>
<p>We start with our formula for AIC from above:</p>
<div class="math notranslate nohighlight">
\[ AIC = 2k - 2 log L \]</div>
<p>For a model with normal errors, we can use the log likelihood of the normal distribution:</p>
<div class="math notranslate nohighlight">
\[ \log L = -\frac{n}{2} \log(2 \pi) -\frac{n}{2}log(\sigma^2) - \sum_i^N \frac{1}{2 \sigma^2} (y_i - \tilde y_i)^2\]</div>
<p>We can drop the first as it is a constant and we’re only assessing relative information with AIC. The last term is actually also a constant: we don’t know <span class="math notranslate nohighlight">\(\sigma^2\)</span> in advance so we use the empirical estimate from the residual (<span class="math notranslate nohighlight">\(\hat{\sigma}^2 = 1/N\sum_i^N (y_i - \tilde y_i)^2\)</span>). Once we plug this in, the two <span class="math notranslate nohighlight">\(\sum [(y - \tilde y)^2]\)</span> terms (in the numerator and denominator, respectively) cancel out and we are left with the last term as <span class="math notranslate nohighlight">\(\frac N 2\)</span>.</p>
<p>Once we drop the constant terms and incorporate into the AIC formula we get:</p>
<div class="math notranslate nohighlight">
\[AIC = 2k + nlog(\sigma^2)\]</div>
<p>We can replace <span class="math notranslate nohighlight">\(\sigma^2\)</span> with the computation for variance (the sum of squared errors divided by number of samples). Thus, we end up with the following formula for AIC for linear and polynomial regression:</p>
<div class="math notranslate nohighlight">
\[ AIC = 2K + n log(\frac{SSE}{n})\]</div>
<p>where k is the number of parameters, n is the number of samples, and SSE is the summed squared error.</p>
<div class="section" id="bonus-exercise-compute-aic">
<h3>Bonus Exercise: Compute AIC<a class="headerlink" href="#bonus-exercise-compute-aic" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">AIC_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_list</span><span class="p">:</span>

  <span class="c1"># Compute predictions for this model</span>
  <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
  <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_design</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>

  <span class="c1">############################################################################</span>
  <span class="c1">## TODO for students: Compute AIC for this order polynomial regression model</span>
  <span class="c1"># 1) Compute sum of squared errors (SSE) given prediction y_hat and y_train</span>
  <span class="c1"># 2) Identify number of parameters in this model (K in formula above)</span>
  <span class="c1"># 3) Compute AIC (call this_AIC) according to formula above</span>
  <span class="c1">############################################################################</span>

  <span class="c1"># Compute SSE</span>
  <span class="n">residuals</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">sse</span> <span class="o">=</span> <span class="o">...</span>

  <span class="c1"># Get K</span>
  <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>

  <span class="c1"># Compute AIC</span>
  <span class="n">AIC</span> <span class="o">=</span> <span class="o">...</span>

  <span class="n">AIC_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIC</span><span class="p">)</span>

<span class="c1"># Uncomment to test your code</span>
<span class="c1"># plt.bar(order_list, AIC_list)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AIC&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;polynomial order&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;comparing polynomial fits&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="n">AIC_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_list</span><span class="p">:</span>

  <span class="c1"># Compute predictions for this model</span>
  <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
  <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_design</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>

  <span class="c1"># Compute SSE</span>
  <span class="n">residuals</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">-</span> <span class="n">y_hat</span>
  <span class="n">sse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">residuals</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

  <span class="c1"># Get K</span>
  <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>

  <span class="c1"># Compute AIC</span>
  <span class="n">AIC</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n_train_samples</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sse</span> <span class="o">/</span> <span class="n">n_train_samples</span><span class="p">)</span>

  <span class="n">AIC_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIC</span><span class="p">)</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">order_list</span><span class="p">,</span> <span class="n">AIC_list</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AIC&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;polynomial order&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;comparing polynomial fits&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Which model would we choose based on AIC?</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D3_ModelFitting"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="W1D3_Tutorial5.html" title="previous page">Neuromatch Academy: Week 1, Day 3, Tutorial 5</a>
    <a class='right-next' id="next-link" href="../W1D4_MachineLearning/README.html" title="next page">W1D4 - Machine Learning</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>