{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W3D4_Tutorial2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/jupyterbook/tutorials/W3D4_ReinforcementLearning/W3D4_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGnYqoKGx9it"
      },
      "source": [
        "# Tutorial 2: Learning to Act: Multi-Armed Bandits\n",
        "**Week 3, Day 4: Reinforcement Learning**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Marcelo Mattar and Eric DeWitt with help from Byron Galbraith\n",
        "\n",
        "__Content reviewers:__ Matt Krause and Michael Waskom\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niR7FW59x9iv"
      },
      "source": [
        "---\n",
        "\n",
        "# Tutorial Objectives\n",
        "  \n",
        "In this tutorial you will use 'bandits' to understand the fundementals of how a policy interacts with the learning algorithm in reinforcement learning.\n",
        "    \n",
        "* You will understand the fundemental tradeoff between exploration and exploitation in a policy.\n",
        "* You will understand how the learning rate interacts with exploration to find the best available action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BfrcReqx9iw"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:08.801535Z",
          "iopub.status.busy": "2021-06-03T22:13:08.801070Z",
          "iopub.status.idle": "2021-06-03T22:13:09.071587Z",
          "shell.execute_reply": "2021-06-03T22:13:09.070661Z"
        },
        "id": "O0EXGlLJx9iw"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.076595Z",
          "iopub.status.busy": "2021-06-03T22:13:09.075080Z",
          "iopub.status.idle": "2021-06-03T22:13:09.151406Z",
          "shell.execute_reply": "2021-06-03T22:13:09.150597Z"
        },
        "id": "eWAl0-EEx9ix"
      },
      "source": [
        "#@title Figure settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.161978Z",
          "iopub.status.busy": "2021-06-03T22:13:09.160670Z",
          "iopub.status.idle": "2021-06-03T22:13:09.162567Z",
          "shell.execute_reply": "2021-06-03T22:13:09.162965Z"
        },
        "id": "tq1UZuBqx9ix"
      },
      "source": [
        "#@title Helper functions\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "def plot_choices(q, epsilon, choice_fn, n_steps=1000, rng_seed=1):\n",
        "  np.random.seed(rng_seed)\n",
        "  counts = np.zeros_like(q)\n",
        "  for t in range(n_steps):\n",
        "    action = choice_fn(q, epsilon)\n",
        "    counts[action] += 1\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(range(len(q)), counts/n_steps)\n",
        "  ax.set(ylabel='% chosen', xlabel='action', ylim=(0,1), xticks=range(len(q)))\n",
        "\n",
        "\n",
        "def plot_multi_armed_bandit_results(results):\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 4))\n",
        "  ax1.plot(results['rewards'])\n",
        "  ax1.set(title=f\"Total Reward: {np.sum(results['rewards']):.2f}\",\n",
        "          xlabel='step', ylabel='reward')\n",
        "  ax2.plot(results['qs'])\n",
        "  ax2.set(xlabel='step', ylabel='value')\n",
        "  ax2.legend(range(len(results['mu'])))\n",
        "  ax3.plot(results['mu'], label='latent')\n",
        "  ax3.plot(results['qs'][-1], label='learned')\n",
        "  ax3.set(xlabel='action', ylabel='value')\n",
        "  ax3.legend()\n",
        "\n",
        "\n",
        "def plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal):\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "\n",
        "  ax1.plot(np.mean(trial_rewards, axis=1).T)\n",
        "  ax1.set(title=f'Average Reward ({fixed})', xlabel='step', ylabel='reward')\n",
        "  ax1.legend(labels)\n",
        "\n",
        "  ax2.plot(np.mean(trial_optimal, axis=1).T)\n",
        "  ax2.set(title=f'Performance ({fixed})', xlabel='step', ylabel='% optimal')\n",
        "  ax2.legend(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCFDcZKx9iy"
      },
      "source": [
        "---\n",
        "# Section 1: Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.169786Z",
          "iopub.status.busy": "2021-06-03T22:13:09.169301Z",
          "iopub.status.idle": "2021-06-03T22:13:09.221042Z",
          "shell.execute_reply": "2021-06-03T22:13:09.221496Z"
        },
        "id": "C2xu5DHsx9iy"
      },
      "source": [
        "#@title Video 1: Multi-Armed Bandits\n",
        "# Insert the ID of the corresponding youtube video\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"kdiXr1zsfo0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtu.be/\" + video.id)\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAScZG4xx9iz"
      },
      "source": [
        "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a reward signal in the form of a numerical value, where the larger value is the better. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
        "\n",
        "This is the original form of the k-armed bandit problem. This name derives from the colloquial name for a slot machine, the \"one-armed bandit\", because it has the one lever to pull, and it is often rigged to take more money than it pays out over time. The multi-armed bandit extension is to imagine, for instance, that you are faced with multiple slot machines that you can play, but only one at a time. Which machine should you play, i.e. which arm should you pull, which action should you take, at any given time to maximize your total payout.\n",
        "\n",
        "<img alt=\"MultiArmedBandit\" width=\"625\" height=\"269\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D5_ReinforcementLearning/static/W2D5_Tutorial2_MultiarmedBandit.png?raw=true\">\n",
        "\n",
        "\n",
        "While there are many different levels of sophistication and assumptions in how the rewards are determined, for simplicity's sake we will assume that each action results in a reward drawn from a fixed Gaussian distribution with unknown mean and unit variance. This problem setting is referred to as the *environment*, and goal is to find the arm with the highest mean value.\n",
        "\n",
        "We will solve this *optimization problem* with an *agent*, in this case an algorithm that takes in rewards and returns actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazV2l00x9iz"
      },
      "source": [
        "---\n",
        "# Section 2: Choosing an Action\n",
        "   \n",
        "The first thing our agent needs to be able to do is choose which arm to pull. The strategy for choosing actions based on our expectations is called a *policy* (often denoted $\\pi$). We could have a random policy -- just pick an arm at random each time -- though this doesn't seem likely to be capable of optimizing our reward. We want some intentionality, and to do that we need a way of describing our beliefs about the arms' reward potential. We do this with an action-value function\n",
        "\n",
        "\\begin{align}\n",
        "q(a) = \\mathbb{E} [r_{t} | a_{t} = a]\n",
        "\\end{align}\n",
        "\n",
        "where the value $q$ for taking action $a \\in A$ at time $t$ is equal to the expected value of the reward $r_t$ given that we took action $a$ at that time. In practice, this is often represented as an array of values, where each action's value is a different element in the array.\n",
        "\n",
        "Great, now that we have a way to describe our beliefs about the values each action should return, let's come up with a policy.\n",
        "\n",
        "An obvious choice would be to take the action with the highest expected value. This is referred to as the *greedy* policy\n",
        "\n",
        "\\begin{align}\n",
        "a_{t} = \\text{argmax}_{a} \\; q_{t} (a)\n",
        "\\end{align}\n",
        "\n",
        "where our choice action is the one that maximizes the current value function.\n",
        "\n",
        "So far so good, but it can't be this easy. And, in fact, the greedy policy does have a fatal flaw: it easily gets trapped in local maxima. It never explores to see what it hasn't seen before if one option is already better than the others. This leads us to a fundamental challenge in coming up with effective polices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCDMPM32x9i0"
      },
      "source": [
        "## Section 2.1: The Exploitation-Exploration Dilemma\n",
        "\n",
        "If we never try anything new, if we always stick to the safe bet, we don't know what we are missing. Sometimes we aren't missing much of anything, and regret not sticking with our preferred choice, yet other times we stumble upon something new that was way better than we thought.\n",
        "\n",
        "This is the exploitation-exploration dilemma: do you go with you best choice now, or risk the less certain option with the hope of finding something better. Too much exploration, however, means you may end up with a sub-optimal reward once it's time to stop.\n",
        "\n",
        "In order to avoid getting stuck in local minima while also maximizing reward, effective policies need some way to balance between these two aims.\n",
        "\n",
        "A simple extension to our greedy policy is to add some randomness. For instance, a coin flip -- heads we take the best choice now, tails we pick one at random. This is referred to as the $\\epsilon$-greedy policy:\n",
        "\n",
        "\\begin{align}\n",
        "P (a_{t} = a) = \n",
        "        \\begin{cases}\n",
        "        1 - \\epsilon + \\epsilon/N    & \\quad \\text{if } a_{t} = \\text{argmax}_{a} \\; q_{t} (a) \\\\\n",
        "        \\epsilon/N        & \\quad \\text{else} \n",
        "        \\end{cases} \n",
        "\\end{align}\n",
        "\n",
        "which is to say that with probability 1 - $\\epsilon$ for $\\epsilon \\in [0,1]$ we select the greedy choice, and otherwise we select an action at random (including the greedy option).\n",
        "\n",
        "Despite its relative simplicity, the epsilon-greedy policy is quite effective, which leads to its general popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uVAlz6Bx9i0"
      },
      "source": [
        "### Exercise 1: Implement Epsilon-Greedy\n",
        "\n",
        "In this exercise you will implement the epsilon-greedy algorithm for deciding which action to take from a set of possible actions given their value function and a probability $\\epsilon$ of simply chosing one at random. \n",
        "\n",
        "TIP: You may find [`np.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html), [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.227096Z",
          "iopub.status.busy": "2021-06-03T22:13:09.225919Z",
          "iopub.status.idle": "2021-06-03T22:13:09.227656Z",
          "shell.execute_reply": "2021-06-03T22:13:09.228055Z"
        },
        "id": "sjI0a6fQx9i1"
      },
      "source": [
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  #####################################################################\n",
        "  ## TODO for students: implement the epsilon greedy decision algorithm\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: implement the epsilon greedy decision algorithm\")\n",
        "  #####################################################################\n",
        "  # write a boolean expression that determines if we should take the best action\n",
        "  be_greedy = ...\n",
        "  if be_greedy:\n",
        "    # write an expression for selecting the best action from the action values\n",
        "    action = ...\n",
        "  else:\n",
        "    # write an expression for selecting a random action\n",
        "    action = ...\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "# Uncomment once the epsilon_greedy function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# epsilon = 0.1\n",
        "# plot_choices(q, epsilon, epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.233772Z",
          "iopub.status.busy": "2021-06-03T22:13:09.232383Z",
          "iopub.status.idle": "2021-06-03T22:13:09.431171Z",
          "shell.execute_reply": "2021-06-03T22:13:09.430711Z"
        },
        "id": "suaXTjmzx9i1"
      },
      "source": [
        "# to_remove solution\n",
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  # write a boolean expression that determines if we should take the best action\n",
        "  be_greedy = np.random.random() > epsilon\n",
        "  if be_greedy:\n",
        "    # write an expression for selecting the best action from the action values\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    # write an expression for selecting a random action\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "q = [-2, 5, 0, 1]\n",
        "epsilon = 0.1\n",
        "with plt.xkcd():\n",
        "  plot_choices(q, epsilon, epsilon_greedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeTnbpGCx9i2"
      },
      "source": [
        "This is what we should expect, that the action with the largest value (action 1) is selected about (1-$\\epsilon$) of the time, or 90% for $\\epsilon = 0.1$, and the remaining 10% is split evenly amongst the other options. Use the demo below to explore how changing $\\epsilon$ affects the distribution of selected actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WcGtkkgx9i2"
      },
      "source": [
        "### Interactive Demo: Changing Epsilon\n",
        "\n",
        "Epsilon is our one parameter for balancing exploitation and exploration.  Given a set of values $q = [-2, 5, 0, 1]$, use the widget below to see how changing $\\epsilon$ influences our selection of the max value 5 (action = 1) vs the others. \n",
        "\n",
        "At the extremes of its range (0 and 1), the $\\epsilon$-greedy policy reproduces two other policies. What are they?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.450887Z",
          "iopub.status.busy": "2021-06-03T22:13:09.450403Z",
          "iopub.status.idle": "2021-06-03T22:13:09.603472Z",
          "shell.execute_reply": "2021-06-03T22:13:09.603049Z"
        },
        "id": "gv7JHjBzx9i2"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact(epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0))\n",
        "def explore_epilson_values(epsilon=0.1):\n",
        "  q = [-2, 5, 0, 1]\n",
        "  plot_choices(q, epsilon, epsilon_greedy, rng_seed=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.607752Z",
          "iopub.status.busy": "2021-06-03T22:13:09.607271Z",
          "iopub.status.idle": "2021-06-03T22:13:09.609995Z",
          "shell.execute_reply": "2021-06-03T22:13:09.609568Z"
        },
        "id": "fkVHrHIvx9i2"
      },
      "source": [
        "#to_remove explanation\n",
        "\"\"\"\n",
        "When epsilon is zero, the agent always chooses the currently best option; it\n",
        "becomes greedy. When epsilon is 1, the agent chooses randomly.\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SOgX2Jtx9i3"
      },
      "source": [
        "---\n",
        "# Section 3: Learning from Rewards\n",
        "\n",
        "Now that we have a policy for deciding what to do, how do we learn from our actions?\n",
        "\n",
        "One way to do this is just keep a record of every result we ever got and use the averages for each action. If we have a potentially very long running episode, the computational cost of keeping all these values and recomputing the mean over and over again isn't ideal. Instead we can use a streaming mean calculation, which looks like this:\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\frac{1}{n_t} (r_{t} - q_{t}(a))\n",
        "\\end{align}\n",
        "\n",
        "where our action-value function $q_t(a)$ is the mean of the rewards seen so far, $n_t$ is the number of actions taken by time $t$, and $r_t$ is the reward just received for taking action $a$.\n",
        "\n",
        "This still requires us to remember how many actions we've taken, so let's generalize this a bit further and replace the action total with a general parameter $\\alpha$, which we will call the learning rate\n",
        "\n",
        "\\begin{align}\n",
        "q_{t+1}(a) \\leftarrow q_{t}(a) + \\alpha (r_{t} - q_{t}(a)).\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bj27fpVx9i3"
      },
      "source": [
        "## Exercise 2: Updating Action Values\n",
        "\n",
        "In this exercise you will implement the action-value update rule above. The function will take in the action-value function represented as an array `q`, the action taken, the reward received, and the learning rate, `alpha`. The function will return the updated value for the selection action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.613811Z",
          "iopub.status.busy": "2021-06-03T22:13:09.613321Z",
          "iopub.status.idle": "2021-06-03T22:13:09.615125Z",
          "shell.execute_reply": "2021-06-03T22:13:09.615517Z"
        },
        "id": "YXfw23kcx9i3"
      },
      "source": [
        "def update_action_value(q, action, reward, alpha):\n",
        "  \"\"\" Compute the updated action value given the learning rate and observed\n",
        "  reward.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received for taking the action\n",
        "    alpha (float): the learning rate\n",
        "\n",
        "  Returns:\n",
        "    float: the updated value for the selected action\n",
        "  \"\"\"\n",
        "  #####################################################\n",
        "  ## TODO for students: compute the action value update\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student excercise: compute the action value update\")\n",
        "  #####################################################\n",
        "  # write an expression for the updated action value\n",
        "  value = ...\n",
        "  return value\n",
        "\n",
        "\n",
        "# Uncomment once the update_action_value function is complete\n",
        "# q = [-2, 5, 0, 1]\n",
        "# action = 2\n",
        "# print(f\"Original q({action}) value = {q[action]}\")\n",
        "# q[action] = update_action_value(q, 2, 10, 0.01)\n",
        "# print(f\"Updated q({action}) value = {q[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.620971Z",
          "iopub.status.busy": "2021-06-03T22:13:09.620251Z",
          "iopub.status.idle": "2021-06-03T22:13:09.622125Z",
          "shell.execute_reply": "2021-06-03T22:13:09.622529Z"
        },
        "id": "v7LrBOatx9i3"
      },
      "source": [
        "# to_remove solution\n",
        "def update_action_value(q, action, reward, alpha):\n",
        "  \"\"\" Compute the updated action value given the learning rate and observed\n",
        "  reward.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received for taking the action\n",
        "    alpha (float): the learning rate\n",
        "\n",
        "  Returns:\n",
        "    float: the updated value for the selected action\n",
        "  \"\"\"\n",
        "  # write an expression for the updated action value\n",
        "  value = q[action] + alpha * (reward - q[action])\n",
        "  return value\n",
        "\n",
        "\n",
        "q = [-2, 5, 0, 1]\n",
        "action = 2\n",
        "print(f\"Original q({action}) value = {q[action]}\")\n",
        "q[action] = update_action_value(q, 2, 10, 0.01)\n",
        "print(f\"Updated q({action}) value = {q[action]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vuQU1N9x9i3"
      },
      "source": [
        "---\n",
        "# Section 4: Solving Multi-Armed Bandits\n",
        "\n",
        "Now that we have both a policy and a learning rule, we can combine these to solve our original multi-armed bandit task. Recall that we have some number of arms that give rewards drawn from Gaussian distributions with unknown mean and unit variance, and our goal is to find the arm with the highest mean.\n",
        "\n",
        "First, let's see how we will simulate this environment by reading through the annotated code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.629411Z",
          "iopub.status.busy": "2021-06-03T22:13:09.628446Z",
          "iopub.status.idle": "2021-06-03T22:13:09.630056Z",
          "shell.execute_reply": "2021-06-03T22:13:09.630470Z"
        },
        "id": "iI11V_mox9i3"
      },
      "source": [
        "def multi_armed_bandit(n_arms, epsilon, alpha, n_steps):\n",
        "  \"\"\" A Gaussian multi-armed bandit using an epsilon-greedy policy. For each\n",
        "  action, rewards are randomly sampled from normal distribution, with a mean\n",
        "  associated with that arm and unit variance.\n",
        "\n",
        "  Args:\n",
        "    n_arms (int): number of arms or actions\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "    alpha (float): the learning rate\n",
        "    n_steps (int): number of steps to evaluate\n",
        "\n",
        "  Returns:\n",
        "    dict: a dictionary containing the action values, actions, and rewards from\n",
        "    the evaluation along with the true arm parameters mu and the optimality of\n",
        "    the chosen actions.\n",
        "  \"\"\"\n",
        "  # Gaussian bandit parameters\n",
        "  mu = np.random.normal(size=n_arms)\n",
        "\n",
        "  # evaluation and reporting state\n",
        "  q = np.zeros(n_arms)\n",
        "  qs = np.zeros((n_steps, n_arms))\n",
        "  rewards = np.zeros(n_steps)\n",
        "  actions = np.zeros(n_steps)\n",
        "  optimal = np.zeros(n_steps)\n",
        "\n",
        "  # run the bandit\n",
        "  for t in range(n_steps):\n",
        "    # choose an action\n",
        "    action = epsilon_greedy(q, epsilon)\n",
        "    actions[t] = action\n",
        "\n",
        "    # copmute rewards for all actions\n",
        "    all_rewards = np.random.normal(mu)\n",
        "    # observe the reward for the chosen action\n",
        "    reward = all_rewards[action]\n",
        "    rewards[t] = reward\n",
        "    # was it the best possible choice?\n",
        "    optimal_action = np.argmax(all_rewards)\n",
        "    optimal[t] = action == optimal_action\n",
        "\n",
        "    # update the action value\n",
        "    q[action] = update_action_value(q, action, reward, alpha)\n",
        "    qs[t] = q\n",
        "\n",
        "  results = {\n",
        "      'qs': qs,\n",
        "      'actions': actions,\n",
        "      'rewards': rewards,\n",
        "      'mu': mu,\n",
        "      'optimal': optimal\n",
        "  }\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T0AWm5vx9i4"
      },
      "source": [
        "We can use our multi-armed bandit method to evaluate how our epsilon-greedy policy and learning rule perform at solving the task. First we will set our environment to have 10 arms and our agent parameters to $\\epsilon=0.1$ and $\\alpha=0.01$. In order to get a good sense of the agent's performance, we will run the episode for 1000 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:09.636066Z",
          "iopub.status.busy": "2021-06-03T22:13:09.635566Z",
          "iopub.status.idle": "2021-06-03T22:13:10.295255Z",
          "shell.execute_reply": "2021-06-03T22:13:10.295671Z"
        },
        "id": "Bk9n3j6yx9i4"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "n_arms = 10\n",
        "epsilon = 0.1\n",
        "alpha = 0.01\n",
        "n_steps = 1000\n",
        "\n",
        "results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
        "ax1.plot(results['rewards'])\n",
        "ax1.set(title=f'Observed Reward ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='reward')\n",
        "ax2.plot(results['qs'])\n",
        "ax2.set(title=f'Action Values ($\\epsilon$={epsilon}, $\\\\alpha$={alpha})',\n",
        "        xlabel='step', ylabel='value')\n",
        "ax2.legend(range(n_arms));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebP78cMx9i4"
      },
      "source": [
        "Alright, we got some rewards that are kind of all over the place, but the agent seemed to settle in on the first arm as the preferred choice of action relatively quickly. Let's see how well we did at recovering the true means of the Gaussian random variables behind the arms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:10.321765Z",
          "iopub.status.busy": "2021-06-03T22:13:10.321195Z",
          "iopub.status.idle": "2021-06-03T22:13:10.508435Z",
          "shell.execute_reply": "2021-06-03T22:13:10.508869Z"
        },
        "id": "tv7Ysa9jx9i4"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(results['mu'], label='latent')\n",
        "ax.plot(results['qs'][-1], label='learned')\n",
        "ax.set(title=f'$\\epsilon$={epsilon}, $\\\\alpha$={alpha}',\n",
        "       xlabel='action', ylabel='value')\n",
        "ax.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHBRegWsx9i4"
      },
      "source": [
        "Well, we seem to have found a very good estimate for action 0, but most of the others are not great. In fact, we can see the effect of the local maxima trap at work -- the greedy part of our algorithm locked onto action 0, which is actually the 2nd best choice to action 6. Since these are the means of Gaussian random variables, we can see that the overlap between the two would be quite high, so even if we did explore action 6, we may draw a sample that is still lower than our estimate for action 0.\n",
        "\n",
        "However, this was just one choice of parameters. Perhaps there is a better combination?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltnaREuZx9i4"
      },
      "source": [
        "## Interactive Demo: Changing Epsilon and Alpha\n",
        "\n",
        "Use the widget below to explore how varying the values of $\\epsilon$ (exploitation-exploration tradeoff), $\\alpha$ (learning rate), and even the number of actions $k$, changes the behavior of our agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:10.546943Z",
          "iopub.status.busy": "2021-06-03T22:13:10.546335Z",
          "iopub.status.idle": "2021-06-03T22:13:10.560119Z",
          "shell.execute_reply": "2021-06-03T22:13:10.558468Z"
        },
        "id": "RQBC7Gpox9i4"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "@widgets.interact_manual(k=widgets.IntSlider(10, min=2, max=15),\n",
        "                         epsilon=widgets.FloatSlider(0.1, min=0.0, max=1.0),\n",
        "                         alpha=widgets.FloatLogSlider(0.01, min=-3, max=0))\n",
        "def explore_bandit_parameters(k=10, epsilon=0.1, alpha=0.001):\n",
        "  results = multi_armed_bandit(k, epsilon, alpha, 1000)\n",
        "  plot_multi_armed_bandit_results(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP4gMKuox9i5"
      },
      "source": [
        "While we can see how changing the epsilon and alpha values impact the agent's behavior, this doesn't give as a great sense of which combination is optimal. Due to the stochastic nature of both our rewards and our policy, a single trial run isn't sufficient to give us this information. Let's run mulitple trials and compare the average performance.\n",
        "\n",
        "First we will look at differet values for $\\epsilon \\in [0.0, 0.1, 0.2]$ to a fixed $\\alpha=0.1$. We will run 200 trials as a nice balance between speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:10.565486Z",
          "iopub.status.busy": "2021-06-03T22:13:10.564997Z",
          "iopub.status.idle": "2021-06-03T22:13:26.418441Z",
          "shell.execute_reply": "2021-06-03T22:13:26.418868Z"
        },
        "id": "VGS-tBdFx9i5"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilons = [0.0, 0.1, 0.2]\n",
        "alpha = 0.1\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, epsilon in enumerate(epsilons):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\epsilon$={e}' for e in epsilons]\n",
        "fixed = f'$\\\\alpha$={alpha}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RwJIfGjx9i5"
      },
      "source": [
        "On the left we have plotted the average reward over time, and we see that while $\\epsilon=0$ (the greedy policy) does well initially, $\\epsilon=0.1$ starts to do slightly better in the long run, while $\\epsilon=0.2$ does the worst. Looking on the right, we see the percentage of times the optimal action (the best possible choice at time $t$) was taken, and here again we see a similar pattern of $\\epsilon=0.1$ starting out a bit slower but eventually having a slight edge in the longer run.\n",
        "\n",
        "We can also do the same for the learning rates. We will evaluate $\\alpha \\in [0.01, 0.1, 1.0]$ to a fixed $\\epsilon=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-03T22:13:26.424610Z",
          "iopub.status.busy": "2021-06-03T22:13:26.424125Z",
          "iopub.status.idle": "2021-06-03T22:13:42.262644Z",
          "shell.execute_reply": "2021-06-03T22:13:42.263064Z"
        },
        "id": "A0qTdfYCx9i5"
      },
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "epsilon = 0.1\n",
        "alphas = [0.01, 0.1, 1.0]\n",
        "n_trials = 200\n",
        "trial_rewards = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "trial_optimal = np.zeros((len(epsilons), n_trials, n_steps))\n",
        "for i, alpha in enumerate(alphas):\n",
        "  for n in range(n_trials):\n",
        "    results = multi_armed_bandit(n_arms, epsilon, alpha, n_steps)\n",
        "    trial_rewards[i, n] = results['rewards']\n",
        "    trial_optimal[i, n] = results['optimal']\n",
        "\n",
        "labels = [f'$\\\\alpha$={a}' for a in alphas]\n",
        "fixed = f'$\\epsilon$={epsilon}'\n",
        "plot_parameter_performance(labels, fixed, trial_rewards, trial_optimal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20Lp5ZCx9i5"
      },
      "source": [
        "Again we see a balance between an effective learning rate. $\\alpha=0.01$ is too weak to quickly incorporate good values, while $\\alpha=1$ is too strong likely resulting in high variance in values due to the Gaussian nature of the rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZT-sxd5x9i5"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "In this tutorial you implemented both the epsilon-greedy descision algorithm and a learning rule for solving a multi-armed bandit scenario. You saw how balancing exploitation and exploration in action selection is crtical in finding optimal solutions. You also saw how choosing an appropriate learning rate determines how well an agent can generalize the information they receive from rewards.\n"
      ]
    }
  ]
}