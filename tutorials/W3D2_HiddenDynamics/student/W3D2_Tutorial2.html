
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorial 2: Hidden Markov Model &#8212; Neuromatch Computational Neuroscience</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial 3: 1D Kalman Filter" href="W3D2_Tutorial3.html" />
    <link rel="prev" title="Tutorial 1: Sequential Probability Ratio Test" href="W3D2_Tutorial1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      <img src="../../../_static/nma-logo-square-4xp.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W0D1_PythonWorkshop1/intro_text.html">
   Python Workshop 1 (W0D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D1_PythonWorkshop1/student/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron - Part I
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W0D2_PythonWorkshop2/intro_text.html">
   Python Workshop 2 (W0D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D2_PythonWorkshop2/student/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W0D3_LinearAlgebra/intro_text.html">
   Linear Algebra (W0D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D3_LinearAlgebra/student/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W0D4_Calculus/intro_text.html">
   Calculus (W0D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial1.html">
     Tutorial 1: Basics of Differential and Integral Calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D4_Calculus/student/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W0D5_Statistics/intro_text.html">
   Statistics (W0D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial1.html">
     Neuromatch Academy: Precourse Week, Day 5, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W0D5_Statistics/student/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W1D1_ModelTypes/intro_text.html">
   Model Types (W1D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/student/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W1D2_ModelingPractice/intro_text.html">
   Modeling Practice (W1D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D2_ModelingPractice/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D2_ModelingPractice/student/W1D2_Tutorial1.html">
     Tutorial: Framing the Question
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D2_ModelingPractice/outro_vid.html">
     Outro Video
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W1D3_ModelFitting/intro_text.html">
   Model Fitting (W1D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/student/W1D3_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D3_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/intro_text.html">
   Generalized Linear Models (W1D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/student/W1D4_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D4_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W1D5_DimensionalityReduction/intro_text.html">
   Dimensionality Reduction (W1D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/student/W1D5_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W1D5_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W2D1_DeepLearning/intro_text.html">
   Deep Learning (W2D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/student/W2D1_Tutorial3.html">
     Tutorial 2: Building and Evaluating Normative Encoding Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D1_DeepLearning/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W2D2_LinearSystems/intro_text.html">
   Linear Systems (W2D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/student/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/intro_text.html">
   Biological Neuron Models (W2D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/student/W2D3_Tutorial4.html">
     Tutorial 4: Spike-timing dependent plasticity (STDP)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W2D4_DynamicNetworks/intro_text.html">
   Dynamic Networks (W2D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D4_DynamicNetworks/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D4_DynamicNetworks/student/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D4_DynamicNetworks/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W3D1_BayesianDecisions/intro_text.html">
   Bayesian Decisions (W3D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/student/W3D1_Tutorial3.html">
     Bonus Tutorial:Fitting to data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../intro_text.html">
   Hidden Dynamics (W3D2)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Tutorial 2: Hidden Markov Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W3D2_Tutorial3.html">
     Tutorial 3: 1D Kalman Filter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W3D2_Tutorial4.html">
     Tutorial 4: 2D Kalman Filter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W3D3_OptimalControl/intro_text.html">
   Optimal Control (W3D3)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D3_OptimalControl/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D3_OptimalControl/student/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D3_OptimalControl/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W3D4_ReinforcementLearning/intro_text.html">
   Reinforcement Learning (W3D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/student/W3D4_Tutorial4.html">
     Tutorial 4: From Reinforcement Learning to Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../W3D5_NetworkCausality/intro_text.html">
   Network Causality (W3D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/intro_vid.html">
     Intro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/student/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/outro_vid.html">
     Outro Video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/tutorials/W3D2_HiddenDynamics/student/W3D2_Tutorial2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/NeuromatchAcademy/course_content/blob/master/book/tutorials/W3D2_HiddenDynamics/student/W3D2_Tutorial2.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Tutorial 2: Hidden Markov Model
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-binary-hmm-with-gaussian-observations">
   Section 1: Binary HMM with Gaussian observations
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coding-exercise-1-simulate-a-binary-hmm-with-gaussian-observations">
     Coding Exercise 1: Simulate a binary HMM with Gaussian observations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interactive-demo-1-binary-hmm">
     Interactive Demo 1: Binary HMM
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-forgetting-information-and-gaining-confidence-with-a-known-initial-state">
   Section 2: Forgetting information and gaining confidence with a known initial state
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-1-forgetting-information">
     Section 2.1: Forgetting information
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#coding-exercise-2-forgetting-in-a-changing-world">
       Coding Exercise 2: Forgetting in a changing world
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interactive-demo-2-forgetting">
       Interactive Demo 2: Forgetting
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-2-gaining-confidence">
     Section 2.2: Gaining confidence
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#coding-exercise-3-gain-confidence-from-evidence">
       Coding Exercise 3: Gain confidence from evidence
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interactive-demo-2-2-noise-in-confidence">
       Interactive Demo 2.2: Noise in confidence
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-inference-in-a-dynamic-world">
   Section 3: Inference in a dynamic world
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#coding-exercise-4-forward-inference-of-hmm">
     Coding Exercise 4: Forward inference of HMM
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interactive-demo-4-forward-inference">
     Interactive Demo 4: Forward inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bonus-section-1-hmm-for-poisson-spiking-neuronal-network">
     Bonus Section 1: HMM for Poisson spiking neuronal network
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-section-define-model-and-generate-data">
     <strong>
      Optional
     </strong>
     Section: Define model and generate data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-and-simulation-parameters">
       Model and simulation parameters
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#initialize-true-model">
       Initialize true model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-data-with-frozen-sequence-and-plot">
       Generate data with frozen sequence and plot
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-data-for-em-learning">
       Generate data for EM learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-section-em-algorithm-for-hmm">
     <strong>
      Optional
     </strong>
     Section: EM algorithm for HMM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-step-forward-backward-algorithm">
       E-step: Forward-backward algorithm
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#m-step">
       M-step
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#e-step-forward-and-backward-algorithm">
       E-step: forward and backward algorithm
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-5-implement-the-m-step">
       EXERCISE 5: Implement the M-step
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#run-em">
       Run EM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optional-section-plotting-the-training-process-and-learnt-model">
     <strong>
      Optional
     </strong>
     Section: Plotting the training process and learnt model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plotting-progress-during-em">
       Plotting progress during EM!
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plot-learnt-parameters-vs-true-parameters">
       Plot learnt parameters vs. true parameters
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W3D2_HiddenDynamics/student/W3D2_Tutorial2.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-2-hidden-markov-model">
<h1>Tutorial 2: Hidden Markov Model<a class="headerlink" href="#tutorial-2-hidden-markov-model" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 3, Day 2: Hidden Dynamics</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Yicheng Fei with help from Jesse Livezey</p>
<p><strong>Content reviewers:</strong> John Butler, Matt Krause, Meenakshi Khosla, Spiros Chavlis, Michael Waskom</p>
</div>
<hr class="docutils" />
<div class="section" id="tutorial-objectives">
<h1>Tutorial objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>The world around us is often changing state over time, but we may only have access to these states through noisy sensory measurements. Similarly, organisms and neural systems often are thought to transition between a set of discrete states (up/down states, sleep/wake, etc.) which may only be indirectly observable through their impact on neural activity. Hidden Markov Models are a class of models that allow us to reason about the dynamics of a set of unobserved states that lead to the changing sensory inputs or data we observe.</p>
<p>In this notebook, we’ll first simulate a Hidden Markov Model and observe how changing the transition probability and observation noise impact what the samples look like. Then we’ll look at how uncertainty increases as we make future predictions without evidence (from observations) and how to gain information from the observations.
The HMM model we use in the first part of the tutorial will have a binary latent variable <span class="math notranslate nohighlight">\(s_t \in \{0,1\}\)</span> that switches randomly between the two states, and a 1D Gaussian emission model <span class="math notranslate nohighlight">\(m_t|s_t \sim \mathcal{N}(\mu_{s_t},\sigma^2_{s_t})\)</span> that provides evidence about the current state. You will learn how to:</p>
<ul class="simple">
<li><p>Build an HMM in Python and generate sample data.</p></li>
<li><p>Calculate how predictive probabilities propagates in a Markov Chain with no evidence.</p></li>
<li><p>Combine new evidence and prediction from past evidence  to estimate latent states.</p></li>
</ul>
<hr class="docutils" />
<p>There is an an <strong>optional</strong> part for you to get a sense of how to perform parameter estimation of an HMM using the EM algorithm. <strong>We encourage you to do these bonus exercises only <em>after</em> you complete the core material in Tutorials 3 and 4.</strong></p>
<p>In the optional part, you will implement an HMM of a network of Poisson spiking neurons mentioned in today’s intro and:</p>
<ul class="simple">
<li><p>Implement the forward-backward algorithm</p></li>
<li><p>Complete the E-step and M-step</p></li>
<li><p>Learn parameters for the example problem using the EM algorithm</p></li>
<li><p>Get an intuition of how the EM algorithm monotonically increases data likelihood</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 1: Introduction</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;ceQXN0OUaFo&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip install hmmlearn --quiet

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">linear_sum_assignment</span>
<span class="kn">from</span> <span class="nn">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">patches</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>       <span class="c1"># interactive display</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper functions</span>

<span class="k">def</span> <span class="nf">plot_hmm1</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plots HMM states and observations for 1d states and observations.</span>

<span class="sd">  Args:</span>
<span class="sd">    model (hmmlearn model):               hmmlearn model used to get state means.</span>
<span class="sd">    states (numpy array of floats):       Samples of the states.</span>
<span class="sd">    observations (numpy array of floats): Samples of the states.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">nsteps</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">size</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">states_forplot</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">means_</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">states</span><span class="p">))</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nstep</span><span class="p">),</span> <span class="n">states_forplot</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s2">&quot;mid&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Latent State&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;State 1&quot;</span><span class="p">,</span> <span class="s2">&quot;State 0&quot;</span><span class="p">])</span>

  <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nstep</span><span class="p">),</span> <span class="n">observations</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Observations&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">())</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_marginal_seq</span><span class="p">(</span><span class="n">predictive_probs</span><span class="p">,</span> <span class="n">switch_prob</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plots the sequence of marginal predictive distributions.</span>

<span class="sd">    Args:</span>
<span class="sd">      predictive_probs (list of numpy vectors): sequence of predictive probability vectors</span>
<span class="sd">      switch_prob (float):                      Probability of switching states.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictive_probs</span><span class="p">)</span>
  <span class="n">prob_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">p_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p_vec</span> <span class="ow">in</span> <span class="n">predictive_probs</span><span class="p">]</span>
  <span class="n">prob_1</span> <span class="o">=</span> <span class="p">[</span><span class="n">p_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p_vec</span> <span class="ow">in</span> <span class="n">predictive_probs</span><span class="p">]</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">prob_0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">prob_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span>
    <span class="s2">&quot;prob in state 0&quot;</span><span class="p">,</span> <span class="s2">&quot;prob in state 1&quot;</span>
  <span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;switching probability=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">switch_prob</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
          <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s2">&quot;round&quot;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;wheat&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Forgetting curve in a changing world&quot;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_evidence_vs_noevidence</span><span class="p">(</span><span class="n">posterior_matrix</span><span class="p">,</span> <span class="n">predictive_probs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plots the average posterior probabilities with evidence v.s. no evidence</span>

<span class="sd">  Args:</span>
<span class="sd">    posterior_matrix: (2d numpy array of floats): The posterior probabilities in state 1 from evidence (samples, time)</span>
<span class="sd">    predictive_probs (numpy array of floats):  Predictive probabilities in state 1 without evidence</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">nsample</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">posterior_matrix</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">posterior_mean</span> <span class="o">=</span> <span class="n">posterior_matrix</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">T</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">predictive_probs</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;No evidence&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="p">(</span><span class="n">nsample</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">posterior_matrix</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;With evidence(Sample)&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">posterior_mean</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;With evidence(Average)&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability in State 0&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Gain confidence with evidence&quot;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">simulate_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Given HMM `model`, calculate posterior marginal predictions of x_t for T-1 time steps ahead based on</span>
<span class="sd">  evidence `data`. If `data` is not give, generate a sequence of observations from first component.</span>

<span class="sd">  Args:</span>
<span class="sd">    model (GaussianHMM instance): the HMM</span>
<span class="sd">    T (int): length of returned array</span>

<span class="sd">  Returns:</span>
<span class="sd">    predictive_state1: predictive probabilities in first state w.r.t no evidence</span>
<span class="sd">    posterior_state1: posterior probabilities in first state w.r.t evidence</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># First re-calculate hte predictive probabilities without evidence</span>
  <span class="n">predictive_probs</span> <span class="o">=</span> <span class="n">simulate_prediction_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
  <span class="c1"># Generate an observation trajectory condtioned on that latent state x is always 1</span>
  <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">data</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">_generate_sample_from_state</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)])</span>
  <span class="c1"># Calculate marginal for each latent state x_t</span>
  <span class="n">pt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_compute_log_likelihood</span><span class="p">(</span><span class="n">Y</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]))</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">startprob_</span>
  <span class="n">pt</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pt</span><span class="p">)</span>
  <span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
  <span class="n">posterior_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pt</span>

  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">one_step_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Y</span><span class="p">[[</span><span class="n">t</span><span class="p">]])</span>
    <span class="c1"># normalize and add to the list</span>
    <span class="n">posterior</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>
    <span class="n">posterior_probs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">posterior</span>
  <span class="n">posterior_state1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">posterior_probs</span><span class="p">])</span>
  <span class="n">predictive_state1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictive_probs</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">predictive_state1</span><span class="p">,</span> <span class="n">posterior_state1</span>

<span class="k">def</span> <span class="nf">plot_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">states_inferred</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Plot ground truth state sequence with noisy observations, and ground truth states v.s. inferred ones</span>

<span class="sd">        Args:</span>
<span class="sd">            model (instance of hmmlearn.GaussianHMM): an instance of HMM</span>
<span class="sd">            states (numpy vector): vector of 0 or 1(int or Bool), the sequences of true latent states</span>
<span class="sd">            observations (numpy vector of numpy vector): the un-flattened Gaussian observations at each time point, element has size (1,)</span>
<span class="sd">            states_inferred (numpy vector): vector of 0 or 1(int or Bool), the sequences of inferred latent states</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plot_hmm1</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="c1"># state 0 has larger mean</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nstep</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">states</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ground Truth&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nstep</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">states_inferred</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Inferred&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Infer latent states from data&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Latent State&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;State 1&quot;</span><span class="p">,</span> <span class="s2">&quot;State 0&quot;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-1-binary-hmm-with-gaussian-observations">
<h1>Section 1: Binary HMM with Gaussian observations<a class="headerlink" href="#section-1-binary-hmm-with-gaussian-observations" title="Permalink to this headline">¶</a></h1>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 2: Simulating a binary HMM with Gaussian observations</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;7cTnoe6Xt80&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>In contrast with the Sequential Probability Ratio Test, the latent state in an HMM is no longer fixed over time. Instead, it can probabilistically switch or jump to a different state at each time step. However, the  time dependence of states at different times is simple: the probability of the state at time <span class="math notranslate nohighlight">\(t\)</span> is wholely determined by the state at time <span class="math notranslate nohighlight">\(t-1\)</span>. This is called called the <strong>Markov property</strong> and the dependency of the whole state sequence <span class="math notranslate nohighlight">\(\{s_1,...,s_t\}\)</span> can be described by a chain structure called a Markov Chain:</p>
<img src=" https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D3_DecisionMaking/static/W2D3_Tutorial2_markov_chain_diagram.png?raw=true" alt="Markov chain drawing" width="400"/>
<p>(Please note that this should be s in diagram above, it will be changed)</p>
<p><strong>Markov model for latent dynamics</strong></p>
<p>Here we will reuse the switching process or telegraph process you saw in a previous tutorial. Quantitatively, the probability of switching to state <span class="math notranslate nohighlight">\(s_t=j\)</span> from the previous state <span class="math notranslate nohighlight">\(s_{t-1}=i\)</span> is a conditional probability distribution <span class="math notranslate nohighlight">\(p(s_t=j|s_{t-1}=i)\)</span>.</p>
<p>Since the states are binary, we can represent the probability of the current state as a 2-dimensional vector <span class="math notranslate nohighlight">\(p(s=i)=p_{i}\)</span> (or, including time, as <span class="math notranslate nohighlight">\(p(s_t=i)=p_{ti}\)</span>), and can represent the transition probability as a 2<span class="math notranslate nohighlight">\(\times\)</span>2 matrix <span class="math notranslate nohighlight">\(A_{ij}\)</span>. This is a convenient representation for coding. We can then use this representation to update the probabilities over time following the Markov process.
$<span class="math notranslate nohighlight">\(p(s_t=j) = \sum_{i} p(s_t=j|s_{t-1}=i)p(s_{t-1}=i)\)</span><span class="math notranslate nohighlight">\(
or equivalently
\)</span><span class="math notranslate nohighlight">\(p_{tj}=\sum_j A_{ij} p_{(t-1)i} \tag{1}\)</span><span class="math notranslate nohighlight">\(
or, using vectors, \)</span>p_t=Ap_{t-1}<span class="math notranslate nohighlight">\(. Note that here \)</span>A_{ij}<span class="math notranslate nohighlight">\( represents the transition probability to switch **FROM state \)</span>i<span class="math notranslate nohighlight">\( TO state \)</span>j$** at next time step.</p>
<p><strong>Measurements</strong></p>
<p>In a <em>Hidden</em> Markov model, we cannot directly observe the latent states <span class="math notranslate nohighlight">\(s_t\)</span>. What we can observe instead is a noisy measurement <span class="math notranslate nohighlight">\(m_t\)</span> generated from <span class="math notranslate nohighlight">\(s_t\)</span>.</p>
<div class="section" id="coding-exercise-1-simulate-a-binary-hmm-with-gaussian-observations">
<h2>Coding Exercise 1: Simulate a binary HMM with Gaussian observations<a class="headerlink" href="#coding-exercise-1-simulate-a-binary-hmm-with-gaussian-observations" title="Permalink to this headline">¶</a></h2>
<p>In this exercise, you will use the package <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> to implement a two-state HMM with Gaussian measurements (sometimes called emissions). Your HMM will start in State 0 and transition between states (both <span class="math notranslate nohighlight">\(0 \rightarrow 1\)</span> and <span class="math notranslate nohighlight">\(1 \rightarrow 0\)</span>) with probability <code class="docutils literal notranslate"><span class="pre">switch_prob</span></code>. Each state emits observations drawn from a Gaussian with <span class="math notranslate nohighlight">\(\mu = 1\)</span> for State 0 and <span class="math notranslate nohighlight">\(\mu = -1\)</span> for State 1. The variance of both states is fixed at <code class="docutils literal notranslate"><span class="pre">noise_level</span></code>.</p>
<p>Please familiarize yourself with the code and complete the following exercises in the next cell. You will need to:</p>
<ol class="simple">
<li><p>To implement the state transitions, complete the transition matrix  <code class="docutils literal notranslate"><span class="pre">transmat_</span></code> (i.e., <span class="math notranslate nohighlight">\(A_{ij}\)</span>) in the code below.</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
A_{i,j} = 
\begin{pmatrix}
p_{\rm stay} &amp; p_{\rm switch} \\
p_{\rm switch} &amp; p_{\rm stay} \\
\end{pmatrix}
\end{equation*}\]</div>
<p>with <span class="math notranslate nohighlight">\(p_{\rm stay} = 1 - p_{\rm switch}\)</span>.</p>
<ol class="simple">
<li><p>The <em>hidden</em> part of HMM means that we do not directly output the current state <span class="math notranslate nohighlight">\(s_t\)</span>, but instead observe a noisy emission <span class="math notranslate nohighlight">\(m_t | s_t\)</span>, here generated by a Gaussian. The  means have already been filled in for you, but you must complete the covariance matrix <code class="docutils literal notranslate"><span class="pre">covars_</span></code>. Set each state’s observation variance to <code class="docutils literal notranslate"><span class="pre">noise_level</span></code>. In the code, the required shape given below is <span class="math notranslate nohighlight">\(2\times 1\times 1\)</span>, for two <span class="math notranslate nohighlight">\(1\times 1\)</span> covariances which are really scalar variances. This seems like a weird shape for storing two numbers, but it makes things easier for the rest of the code.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">startprob</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;Create an HMM with binary state variable and 1D Gaussian observations</span>
<span class="sd">    The probability to switch to the other state is `switch_prob`. Two</span>
<span class="sd">    observation models have mean 1.0 and -1.0 respectively. `noise_level`</span>
<span class="sd">    specifies the standard deviation of the observation models.</span>

<span class="sd">    Args:</span>
<span class="sd">        switch_prob (float): probability to jump to the other state</span>
<span class="sd">        noise_level (float): standard deviation of observation models. Same for</span>
<span class="sd">        two components</span>

<span class="sd">    Returns:</span>
<span class="sd">        model (hmm.GaussianHMM instance): the described HMM</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">############################################################################</span>
    <span class="c1"># Insert your code here to:</span>
    <span class="c1">#    * Create the transition matrix, `transmat_` so that the odds of</span>
    <span class="c1">#      switching is `switch_prob`</span>
    <span class="c1">#		* Set the observation model variances, `covars_`, to `noise_level`</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`create_HMM` is incomplete&quot;</span><span class="p">)</span>
    <span class="c1">############################################################################</span>
    <span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Initialize model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">GaussianHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">startprob_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">startprob</span><span class="p">)</span>

    <span class="c1"># Make transition matrix, should be shape (2, 2), i.e., a transition matrix for 2 states</span>
    <span class="n">model</span><span class="o">.</span><span class="n">transmat_</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Create means</span>
    <span class="n">model</span><span class="o">.</span><span class="n">means_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">]])</span>

    <span class="c1"># Create covariance matrices, should be shape (2, 1, 1), i.e., 2 1x1 covariance matrices</span>
    <span class="n">model</span><span class="o">.</span><span class="n">covars_</span> <span class="o">=</span> <span class="o">...</span>

    <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Number of steps</span>
<span class="n">nstep</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Create HMM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">()</span>

<span class="c1"># Sample from HMM</span>
<span class="n">observations</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nstep</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plot_hmm1</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_9e86d4ae.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial2_Solution_9e86d4ae_0.png>
</div>
<div class="section" id="interactive-demo-1-binary-hmm">
<h2>Interactive Demo 1: Binary HMM<a class="headerlink" href="#interactive-demo-1-binary-hmm" title="Permalink to this headline">¶</a></h2>
<p>In the demo below, we simulate a similar HMM and plot. You can change the probability of switching states and the noise level.</p>
<p>First, think and discuss these questions.</p>
<ol class="simple">
<li><p>What will happen if the switching probability is zero? What about if it’s one?</p></li>
<li><p>What will happen with high noise? Low?</p></li>
</ol>
<p>Then, play with the demo to see if you were correct or not.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">nstep</span> <span class="o">=</span> <span class="mi">100</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">.01</span><span class="p">),</span> <span class="n">log10_noise_level</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.01</span><span class="p">)):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="n">switch_prob</span><span class="p">,</span>
                     <span class="n">noise_level</span><span class="o">=</span><span class="mf">10.</span><span class="o">**</span><span class="n">log10_noise_level</span><span class="p">)</span>

  <span class="n">observations</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nstep</span><span class="p">)</span>
  <span class="n">observations</span> <span class="o">=</span> <span class="n">observations</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
  <span class="n">plot_hmm1</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_41ccf32d.py"><em>Click for solution</em></a></p>
<p><strong>Harkening</strong> back to our fishing example, you can imagine that the time series you measure is related to the number of fish caught at different times as the school of fish move from left to right. Or you could envision it as the voltage across a membrane affected by an ion channel in two states, open and closed. Or it could represent EEG frequency measurements as the brain moves between sleep states. What phenomena can you imagine modeling with these HMMs?</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-2-forgetting-information-and-gaining-confidence-with-a-known-initial-state">
<h1>Section 2: Forgetting information and gaining confidence with a known initial state<a class="headerlink" href="#section-2-forgetting-information-and-gaining-confidence-with-a-known-initial-state" title="Permalink to this headline">¶</a></h1>
<div class="section" id="section-2-1-forgetting-information">
<h2>Section 2.1: Forgetting information<a class="headerlink" href="#section-2-1-forgetting-information" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 3: Forgetting in a changing world</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;pRRo_L-n8nc&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-2-forgetting-in-a-changing-world">
<h3>Coding Exercise 2: Forgetting in a changing world<a class="headerlink" href="#coding-exercise-2-forgetting-in-a-changing-world" title="Permalink to this headline">¶</a></h3>
<p>Since the world (latent state) is changing over time, even if we know for sure that we are in state 0 at some time, we will be more and more uncertain that we’ll remain in state 0 as time goes. In other words, when we try to make predictions of future states in a Markov Chain based on our current knowledge without future evidence, the influence of current state will decay over time.</p>
<p>In this exercise, we’ll inspect how we “forget” the current state information when predicting future states without any observation.</p>
<p>Using the model you just defined, let’s now make some predictions about <span class="math notranslate nohighlight">\(s_t\)</span> given that we know <span class="math notranslate nohighlight">\(s_0=0\)</span> for sure. We’ve already imposed this assumption by setting prior probabilities of <span class="math notranslate nohighlight">\(p(s_0)\)</span> to <span class="math notranslate nohighlight">\([1,0]\)</span> earlier.</p>
<ol class="simple">
<li><p>Complete the code in function <code class="docutils literal notranslate"><span class="pre">markov_forward</span></code> to calculate the predictive marginal distribution at next time step using <code class="docutils literal notranslate"><span class="pre">p_next</span> <span class="pre">=</span> <span class="pre">A.T</span> <span class="pre">&#64;</span> <span class="pre">p_current</span></code></p></li>
<li><p>Take a look at function <code class="docutils literal notranslate"><span class="pre">simulate_prediction_only</span></code> and understand how the predictive distribution propagates along the Markov chain</p></li>
<li><p>Using our provided code, plot the predictive probabilities as a function of time</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to enable the function simulate_prediction_only</span>
<span class="k">def</span> <span class="nf">simulate_prediction_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nstep</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Simulate the diffusion of HMM with no observations</span>

<span class="sd">  Args:</span>
<span class="sd">    model (hmm.GaussianHMM instance): the HMM instance</span>
<span class="sd">    nstep (int): total number of time steps to simulate(include initial time)</span>

<span class="sd">  Returns:</span>
<span class="sd">    predictive_probs (list of numpy vector): the list of marginal probabilities</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">entropy_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">predictive_probs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">startprob_</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nstep</span><span class="p">):</span>
    <span class="c1"># calculate entropy</span>
    <span class="n">predictive_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
    <span class="c1"># one step forward</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">markov_forward</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">transmat_</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">predictive_probs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">markov_forward</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculate the forward predictive distribution in a discrete Markov chain</span>

<span class="sd">  Args:</span>
<span class="sd">    p0 (numpy vector): a discrete probability vector</span>
<span class="sd">    A (numpy matrix): the transition matrix, A[i,j] means the prob. to</span>
<span class="sd">    switch FROM i TO j</span>

<span class="sd">  Returns:</span>
<span class="sd">    p1 (numpy vector): the predictive probabilities in next time step</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">############################################################################</span>
  <span class="c1"># Insert your code here to:</span>
  <span class="c1">#      Compute the marginal distribution of Markov chain in next time step</span>
  <span class="c1">#      Hint: use matrix multiply and be careful about the index orders</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;function `markov_forward` incomplete&quot;</span><span class="p">)</span>
  <span class="c1">############################################################################</span>
  <span class="n">p1</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">return</span> <span class="n">p1</span>


<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>

<span class="c1"># Set parameters of HMM</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">switch_prob</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">noise_level</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># Create HMM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="n">switch_prob</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="n">noise_level</span><span class="p">)</span>

<span class="c1"># Get predictive probabilities</span>
<span class="n">predictive_probs</span> <span class="o">=</span> <span class="n">simulate_prediction_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plot_marginal_seq</span><span class="p">(</span><span class="n">predictive_probs</span><span class="p">,</span> <span class="n">switch_prob</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_61403d8f.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial2_Solution_61403d8f_0.png>
</div>
<div class="section" id="interactive-demo-2-forgetting">
<h3>Interactive Demo 2: Forgetting<a class="headerlink" href="#interactive-demo-2-forgetting" title="Permalink to this headline">¶</a></h3>
<p>In the following demo, we look at the same visualization but you can play with the probability of switching states, using the slider.</p>
<ol class="simple">
<li><p>Do you forget more quickly with low or high switching probability?</p></li>
<li><p>How does the curve look at when <code class="docutils literal notranslate"><span class="pre">prob_switch</span></code> $&gt;0.5?</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">.99</span><span class="p">,</span> <span class="mf">.01</span><span class="p">)):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="n">switch_prob</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="n">noise_level</span><span class="p">)</span>
  <span class="n">predictive_probs</span> <span class="o">=</span> <span class="n">simulate_prediction_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
  <span class="n">plot_marginal_seq</span><span class="p">(</span><span class="n">predictive_probs</span><span class="p">,</span> <span class="n">switch_prob</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_d90e17f2.py"><em>Click for solution</em></a></p>
</div>
</div>
<div class="section" id="section-2-2-gaining-confidence">
<h2>Section 2.2: Gaining confidence<a class="headerlink" href="#section-2-2-gaining-confidence" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 4: Gain confidence from evidence</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;dDjoxUxMgC0&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>As shown in Exercise 2, you lose information and increase uncertainty exponentially when predicting into future in the absence of further evidence, because the state randomly diffuses from its last known value. However the HMM also generates a measurement <span class="math notranslate nohighlight">\(m_t\)</span> at each time step, and we can use this evidence to improve our state estimate.</p>
<img src="https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/W2D3_DecisionMaking/static/W2D3_Tutorial2_HMM_diagram.png?raw=true" alt="HMM drawing" width="400"/>
<p>(We will UPDATE FIGURE with <span class="math notranslate nohighlight">\(x,y\to s,m\)</span>**)</p>
<p>Now let’s incorporate evidence into our inference. In this exercise we will calculate the <strong>posterior marginal</strong> distribution <span class="math notranslate nohighlight">\(p(s_t|m_{1:t})\)</span>, ie the marginal probability of the current state given the entire history of measurements. This is a crucial computation, and it is tractable because of the simple structure of the HMM.</p>
<p>We compute this probability recursively. Suppose we know the posterior marginal probability for the previous time step, <span class="math notranslate nohighlight">\(p(s_{t-1}|m_{1:t-1})\)</span>. Now we receive a new measurement <span class="math notranslate nohighlight">\(m_t\)</span>. From Bayes’ rule and the Markov property, we can calculate <span class="math notranslate nohighlight">\(p(s_{t}|m_{1:t})\)</span>. We do this in two steps.</p>
<p>First, we make a prediction for <span class="math notranslate nohighlight">\(s_t\)</span> given our previous knowledge. We can say “yesterday’s posterior becomes today’s prior,” where the Markov transition matrix accounts for the change from <span class="math notranslate nohighlight">\(t\)</span> to <span class="math notranslate nohighlight">\(t-1\)</span>, as in the last exercise. This gives us the prior probability
$<span class="math notranslate nohighlight">\(p(s_t|m_{1:t-1})=\sum_{s_{t-1}}p(s_t|s_{t-1})p(s_{t-1}|m_{1:t-1}) \tag{2}\)</span><span class="math notranslate nohighlight">\(
Observe that the history \)</span>m_{1:t-1}<span class="math notranslate nohighlight">\( does not yet include the new measurement at time \)</span>t$.</p>
<p>Second, we use the usual Bayesian inference to incorporate this new evidence, multiplying our prior <span class="math notranslate nohighlight">\(p(s_t|m_{1:t-1})\)</span> times our likelihood <span class="math notranslate nohighlight">\(p(m_t|s_t)\)</span>, to obtain
$<span class="math notranslate nohighlight">\(p(s_t|m_{1:t})\propto p(s_t|m_{1:t-1})p(s_{t}|m_t) \tag{3}\)</span>$.</p>
<p>Putting (2) and (3) together, we obtain the forward recursion equation for a Hidden Markov Model,
$<span class="math notranslate nohighlight">\(p(s_t|m_{1:t-1})\propto p(m_t|s_t)\sum_{s_{t-1}}p(s_t|s_{t-1})p(s_{t-1}|m_{1:t-1}) \tag{4}\)</span>$</p>
<div class="section" id="coding-exercise-3-gain-confidence-from-evidence">
<h3>Coding Exercise 3: Gain confidence from evidence<a class="headerlink" href="#coding-exercise-3-gain-confidence-from-evidence" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Create a model with switching probability <span class="math notranslate nohighlight">\(0.1\)</span> and noise level <span class="math notranslate nohighlight">\(0.5\)</span> using function <code class="docutils literal notranslate"><span class="pre">create_HMM(switch_prob,</span> <span class="pre">noise_level)</span></code></p></li>
<li><p>Complete the code to calculate marginal posterior distribution <span class="math notranslate nohighlight">\(p(s_t|m_{1:t-1})\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> from last posterior <span class="math notranslate nohighlight">\(p(s_{t-1}|m_{1:t-1})\)</span> at time <span class="math notranslate nohighlight">\(t-1\)</span></p>
<ul class="simple">
<li><p>Calculate the predictive distribution <span class="math notranslate nohighlight">\(p(s_t =j|m_{1:t-1})=\sum_i A_{ij} p(s_{t-1}=i|m_{1:t-1})\)</span></p></li>
<li><p>Calculate the likelihood of new data under each component using <code class="docutils literal notranslate"><span class="pre">exp(model._compute_log_likelihood(yt))</span></code></p></li>
<li><p>Multiply likelihood and prediction element-wise and normalize over two components to get the new posterior probabilities</p></li>
</ul>
</li>
<li><p>Using provided code, plot the average posterior probabilities over time due to evidence together with predictive probabilities without evidence</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">one_step_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">posterior_tm1</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Given a HMM model, calculate the one-time-step updates to the posterior.</span>

<span class="sd">  Args:</span>
<span class="sd">    model (GaussianHMM instance): the HMM</span>
<span class="sd">    posterior_tm1 (numpy array): Posterior at `t-1`</span>
<span class="sd">    Y_t (numpy array): Observation at `t`</span>

<span class="sd">    Returns:</span>
<span class="sd">    posterior_t (numpy array): Posterior at `t`</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">##############################################################################</span>
  <span class="c1"># Insert your code here to:</span>
  <span class="c1">#      1. Calculate the predicted state given the previous</span>
  <span class="c1">#      estimate (`posterior_tm1`). Note that `model.transmat_` is equvalent</span>
  <span class="c1">#      to `A.T`, not `A`.</span>
  <span class="c1">#      2. Using `model._compute_log_likelihood()`, calculate the likelihood</span>
  <span class="c1">#      given `Y_t`.</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`one_step_update` is incomplete&quot;</span><span class="p">)</span>
  <span class="c1">##############################################################################</span>
  <span class="n">prediction</span> <span class="o">=</span> <span class="o">...</span> <span class="o">@</span> <span class="n">posterior_tm1</span>
  <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">posterior_t</span> <span class="o">=</span> <span class="n">prediction</span> <span class="o">*</span> <span class="n">likelihood</span>
  <span class="k">return</span> <span class="n">posterior_t</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">switch_prob</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">nsample</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">160</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">)</span>

<span class="n">posterior_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsample</span><span class="p">):</span>
  <span class="n">predictive_probs</span><span class="p">,</span> <span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">simulate_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
  <span class="n">posterior_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">posterior_probs</span><span class="p">)</span>
<span class="n">posterior_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">posterior_list</span><span class="p">)</span>

<span class="n">plot_evidence_vs_noevidence</span><span class="p">(</span><span class="n">posterior_matrix</span><span class="p">,</span> <span class="n">predictive_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_0601fea5.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial2_Solution_0601fea5_0.png>
<p>Now you’ve got a plot of information loss due to diffusion together with the information recovered/uncertainty reduced due to evidence. The difference between the former and the latter is the amount of uncertainty that still remains because of observation noise, as we’ll see in the next demo.</p>
</div>
<div class="section" id="interactive-demo-2-2-noise-in-confidence">
<h3>Interactive Demo 2.2: Noise in confidence<a class="headerlink" href="#interactive-demo-2-2-noise-in-confidence" title="Permalink to this headline">¶</a></h3>
<p>In this demo, you can adjust the switch probability and noise level and observe how information gain changes with signal-to-noise ratio and/or switch probability.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">.99</span><span class="p">,</span> <span class="mf">.01</span><span class="p">),</span> <span class="n">noise_level</span><span class="o">=</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">.05</span><span class="p">),</span>
         <span class="n">nsample</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">T</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">5</span><span class="p">)):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">)</span>
  <span class="n">posterior_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsample</span><span class="p">):</span>
    <span class="n">predictive_probs</span><span class="p">,</span> <span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">simulate_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">posterior_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">posterior_probs</span><span class="p">)</span>
  <span class="n">posterior_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">posterior_list</span><span class="p">)</span>
  <span class="n">plot_evidence_vs_noevidence</span><span class="p">(</span><span class="n">posterior_matrix</span><span class="p">,</span> <span class="n">predictive_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-3-inference-in-a-dynamic-world">
<h1>Section 3: Inference in a dynamic world<a class="headerlink" href="#section-3-inference-in-a-dynamic-world" title="Permalink to this headline">¶</a></h1>
<div class="section" id="coding-exercise-4-forward-inference-of-hmm">
<h2>Coding Exercise 4: Forward inference of HMM<a class="headerlink" href="#coding-exercise-4-forward-inference-of-hmm" title="Permalink to this headline">¶</a></h2>
<p>If you set <code class="docutils literal notranslate"><span class="pre">switch_prob</span></code> or <code class="docutils literal notranslate"><span class="pre">noise_level</span></code> to be large in the last exercise, you will observe that some sample inference dots fall below 0.5. This means we are making false inferences about which latent state we are in.</p>
<p>In this exercise, let’s make a forward inference of a random state sequence rather than a constant one by observing its noisy Gaussian outputs. Different from Exercise 1, here we assume we know the switching probability but don’t know the prior (<code class="docutils literal notranslate"><span class="pre">startprob_</span></code>).</p>
<ol class="simple">
<li><p>Build a HMM with prior probabilities= <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>, switching probability=<span class="math notranslate nohighlight">\(0.1\)</span>, and noise level=<span class="math notranslate nohighlight">\(1.0\)</span> by calling <code class="docutils literal notranslate"><span class="pre">create_HMM(switch_prob,</span> <span class="pre">noise_level,</span> <span class="pre">startprob)</span></code></p></li>
<li><p>Generate a sample sequence along with observations by calling <code class="docutils literal notranslate"><span class="pre">model.sample(nstep)</span></code>, and use our provided code to visualize the latent trajectory and observations</p></li>
<li><p>Calculate posterior probabilities given data by calling <code class="docutils literal notranslate"><span class="pre">simulate_forward_inference(model,</span> <span class="pre">nstep,</span> <span class="pre">observations)</span></code>, and make inference of latent states by picking the component with larger posterior probability</p></li>
<li><p>Use our provided code to visualize the inferred state sequence together with the ground truth</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">nstep</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">switch_prob</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">log10_noise_level</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># Build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="n">switch_prob</span><span class="p">,</span>
                     <span class="n">noise_level</span><span class="o">=</span><span class="mf">10.</span><span class="o">**</span><span class="n">log10_noise_level</span><span class="p">,</span>
                     <span class="n">startprob</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">observations</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nstep</span><span class="p">)</span>
<span class="c1"># Infer state sequence</span>
<span class="n">predictive_probs</span><span class="p">,</span> <span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">simulate_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nstep</span><span class="p">,</span>
                                                               <span class="n">observations</span><span class="p">)</span>

<span class="c1">############################################################################</span>
<span class="c1"># Insert your code here to:</span>
<span class="c1">#      Calculate inferred states from posterior probabilities at state 0</span>
<span class="c1">#      Hint: Compare the probabilities with 0.5 and note that you should</span>
<span class="c1">#            return 0 if prob &gt; 0.5</span>
<span class="c1">############################################################################</span>
<span class="c1"># states_inferred = ...</span>

<span class="c1">################################################################################</span>
<span class="c1"># After finishing the above exercises, please un-comment the following lines</span>
<span class="c1">################################################################################</span>
<span class="c1">#plot_forward_inference(model, states, observations, states_inferred)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_c36f28a6.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial2_Solution_c36f28a6_0.png>
<img alt='Solution hint' align='left' width=560 height=416 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial2_Solution_c36f28a6_1.png>
</div>
<div class="section" id="interactive-demo-4-forward-inference">
<h2>Interactive Demo 4: Forward inference<a class="headerlink" href="#interactive-demo-4-forward-inference" title="Permalink to this headline">¶</a></h2>
<p>Try different values of switching probability (<code class="docutils literal notranslate"><span class="pre">prob_switch</span></code>) and noise level (<code class="docutils literal notranslate"><span class="pre">noise_level</span></code>) either by hand or the widget in section <strong>Interactive Cell</strong>. When do we start to make false inferences?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">nstep</span> <span class="o">=</span> <span class="mi">100</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">.99</span><span class="p">,</span> <span class="mf">.01</span><span class="p">),</span> <span class="n">log10_noise_level</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">.01</span><span class="p">)):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">create_HMM</span><span class="p">(</span><span class="n">switch_prob</span><span class="o">=</span><span class="n">switch_prob</span><span class="p">,</span>
                      <span class="n">noise_level</span><span class="o">=</span><span class="mf">10.</span><span class="o">**</span><span class="n">log10_noise_level</span><span class="p">,</span>
                      <span class="n">startprob</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
  <span class="n">observations</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nstep</span><span class="p">)</span>
  <span class="c1"># observations = observations.flatten()</span>
  <span class="c1"># Infer state sequence</span>
  <span class="n">predictive_probs</span><span class="p">,</span> <span class="n">posterior_probs</span> <span class="o">=</span> <span class="n">simulate_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nstep</span><span class="p">,</span> <span class="n">observations</span><span class="p">)</span>
  <span class="n">states_inferred</span> <span class="o">=</span> <span class="n">posterior_probs</span> <span class="o">&lt;=</span> <span class="mf">0.5</span>
  <span class="n">plot_forward_inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">states_inferred</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<p>We, the organizers, know that the next sections are much longer and more challenging than most other tutorial content. <strong>We do not expect you to finish it—or even start it—right now</strong>. In fact, we strongly suggest saving your time and energy for the Kalman Filtering introduced in Tutorials 3 and 4, because it will play an important role in tomorrow’s material too.</p>
<p>That said, the EM algorithm can be a very useful and powerful optimization tool. Since it is typically taught in the context of Hidden Markov Models, we have included it here for your reference.</p>
<p>To reiterate, the remainder of this notebook is <em>completely</em> and <em>absolutely</em> optional. It is not essential to understand the rest of the NMA content. By this point in Tutorial 2, we believe that you will have seen enough about HMMs to know when/if they might be relevant for your own research. When that day comes, or you are just feeling curious, this material will be here waiting!</p>
<hr class="docutils" />
<div class="section" id="bonus-section-1-hmm-for-poisson-spiking-neuronal-network">
<h2>Bonus Section 1: HMM for Poisson spiking neuronal network<a class="headerlink" href="#bonus-section-1-hmm-for-poisson-spiking-neuronal-network" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 5: HMM for Poisson spiking neurons case study</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Wb8mf5chmyI&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>Given noisy neural or behavioral measurements, we as neuroscientists often want to infer the unobserved latent variables as they change over time. Thalamic relay neurons fire in two distinct modes: a tonic mode where spikes are produced one at a time, and a ‘burst mode’ where several action potentials are produced in rapid succession. These modes are thought to differentially encode how the neurons relay information from sensory receptors to cortex. A distinct molecular mechanism, T-type calcium channels, switches neurons between modes, but it is very challenging to measure in the brain of a living monkey. However, statistical approaches let us recover the hidden state of those calcium channels purely from their spiking activity, which can be measured in a behaving monkey.</p>
<p>Here, we’re going to tackle a simplified version of that problem.</p>
<p>Let’s consider the formulation mentioned in the intro lecture.
We have a network of <span class="math notranslate nohighlight">\(C\)</span> neurons switching between <span class="math notranslate nohighlight">\(K\)</span> states. Neuron <span class="math notranslate nohighlight">\(c\)</span> has firing rate <span class="math notranslate nohighlight">\(\lambda_i^c\)</span> in state <span class="math notranslate nohighlight">\(i\)</span>. The transition between states are represented by the <span class="math notranslate nohighlight">\(K\times K\)</span> transition matrix <span class="math notranslate nohighlight">\(A_{ij}\)</span> and initial probability vector <span class="math notranslate nohighlight">\(\psi\)</span> with length <span class="math notranslate nohighlight">\(K\)</span> at time <span class="math notranslate nohighlight">\(t=1\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(y_t^c\)</span> be the number of spikes for cell <span class="math notranslate nohighlight">\(c\)</span> in time bin <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>In the following exercises (4 and 5) and tutorials, you will</p>
<ul class="simple">
<li><p>Define an instance of such model with <span class="math notranslate nohighlight">\(C=5\)</span> and <span class="math notranslate nohighlight">\(K=3\)</span></p></li>
<li><p>(<strong>Exercise 4</strong>) Generate a dataset from this model</p></li>
<li><p>(<strong>Exercise 5</strong>) Implement the M-step for this HMM</p></li>
<li><p>Run EM to estimate all parameters <span class="math notranslate nohighlight">\(A,\psi,\lambda_i^c\)</span></p></li>
<li><p>Plot the learning likelihood curve</p></li>
<li><p>Plot expected complete log likelihood versus data log likelihood</p></li>
<li><p>Compare learnt parameters versus true parameters</p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="optional-section-define-model-and-generate-data">
<h2><strong>Optional</strong> Section: Define model and generate data<a class="headerlink" href="#optional-section-define-model-and-generate-data" title="Permalink to this headline">¶</a></h2>
<p>Let’s first generate a random state sequence from the hidden Markov Chain, and generate <code class="docutils literal notranslate"><span class="pre">n_frozen_trials</span></code> different trials of spike trains for each cell assuming they all use the same underlying sequence we just generated.</p>
<p><strong>Suggestions</strong></p>
<ol class="simple">
<li><p>Run the following two sections <strong>Model and simulation parameters</strong> and <strong>Initialize true model</strong> to define a true model and parameters that will be used in our following exercises. Please take a look at the parameters and come back to these two cells if you encounter a variable you don’t know in the future.</p></li>
<li><p>Complete the code to convert a given state sequence to corresponding spike rates for all cells at all times, and use provided code to visualize all spike trains.</p></li>
</ol>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper functions</span>
<span class="k">def</span> <span class="nf">plot_spike_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Plots the spike train for cells across trials and overlay the state.</span>

<span class="sd">      Args:</span>
<span class="sd">        X: (2d numpy array of binary values): The state sequence in a one-hot</span>
<span class="sd">                                              representation. (T, states)</span>
<span class="sd">        Y: (3d numpy array of floats):        The spike sequence.</span>
<span class="sd">                                              (trials, T, C)</span>
<span class="sd">        dt (float):                           Interval for a bin.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_trials</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">trial_T</span> <span class="o">=</span> <span class="n">T</span> <span class="o">*</span> <span class="n">dt</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">.7</span> <span class="o">*</span> <span class="p">(</span><span class="mf">12.8</span> <span class="o">+</span> <span class="mf">6.4</span><span class="p">),</span> <span class="mf">.7</span> <span class="o">*</span> <span class="mf">9.6</span><span class="p">))</span>

    <span class="c1"># plot state sequence</span>
    <span class="n">starts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">stops</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">T</span><span class="p">]</span>
    <span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">starts</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="n">stops</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="n">rect</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="n">dt</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">dt</span><span class="p">,</span> <span class="n">n_trials</span> <span class="o">*</span> <span class="n">C</span><span class="p">,</span>
                                 <span class="n">facecolor</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab10&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>

    <span class="c1"># plot rasters</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">trial_T</span><span class="p">],</span> <span class="p">[</span><span class="n">c</span> <span class="o">*</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="n">n_trials</span><span class="p">],</span>
                     <span class="n">color</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;tab10&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">r</span><span class="p">,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">tmp</span><span class="p">,</span> <span class="n">tmp</span><span class="p">))</span> <span class="o">*</span> <span class="n">dt</span><span class="p">,</span>
                                      <span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">n_trials</span> <span class="o">+</span> <span class="n">r</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span>
                                       <span class="n">c</span> <span class="o">*</span> <span class="n">n_trials</span> <span class="o">+</span> <span class="n">r</span> <span class="o">+</span> <span class="mf">.9</span><span class="p">),</span>
                                      <span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_trials</span> <span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">),</span>
               <span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;time (s)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cell number&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">run_em</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Run EM for the HMM spiking model.</span>

<span class="sd">  Args:</span>
<span class="sd">    epochs (int):       Number of epochs of EM to run</span>
<span class="sd">    Y (numpy 3d array): Tensor of recordings, has shape (n_trials, T, C)</span>
<span class="sd">    psi (numpy vector): Initial probabilities for each state</span>
<span class="sd">    A (numpy matrix):   Transition matrix, A[i,j] represents the prob to switch</span>
<span class="sd">                        from j to i. Has shape (K,K)</span>
<span class="sd">    L (numpy matrix):   Poisson rate parameter for different cells.</span>
<span class="sd">                        Has shape (C,K)</span>
<span class="sd">    dt (float):         Duration of a time bin</span>

<span class="sd">  Returns:</span>
<span class="sd">    save_vals (lists of floats): Data for later plotting</span>
<span class="sd">    lls (list of flots):         ll Before each EM step</span>
<span class="sd">    psi (numpy vector):          Estimated initial probabilities for each state</span>
<span class="sd">    A (numpy matrix):            Estimated transition matrix, A[i,j] represents</span>
<span class="sd">                                 the prob to switch from j to i. Has shape (K,K)</span>
<span class="sd">    L (numpy matrix):            Estimated Poisson rate parameter for different</span>
<span class="sd">                                 cells. Has shape (C,K)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">save_vals</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">lls</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

      <span class="c1"># Run E-step</span>
      <span class="n">ll</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
      <span class="n">lls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>  <span class="c1"># log the data log likelihood for current cycle</span>

      <span class="k">if</span> <span class="n">e</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch: </span><span class="si">{</span><span class="n">e</span><span class="si">:</span><span class="s1">3d</span><span class="si">}</span><span class="s1">, ll = </span><span class="si">{</span><span class="n">ll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>  <span class="c1"># log progress</span>
      <span class="c1"># Run M-step</span>
      <span class="n">psi_new</span><span class="p">,</span> <span class="n">A_new</span><span class="p">,</span> <span class="n">L_new</span> <span class="o">=</span> <span class="n">m_step</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>

      <span class="sd">&quot;&quot;&quot;Booking keeping for later plotting</span>
<span class="sd">      Calculate the difference of parameters for later</span>
<span class="sd">      interpolation/extrapolation</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">dp</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">dL</span> <span class="o">=</span> <span class="n">psi_new</span> <span class="o">-</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A_new</span> <span class="o">-</span> <span class="n">A</span><span class="p">,</span> <span class="n">L_new</span> <span class="o">-</span> <span class="n">L</span>
      <span class="c1"># Calculate LLs and ECLLs for later plotting</span>
      <span class="k">if</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">plot_epochs</span><span class="p">:</span>
          <span class="n">b_min</span> <span class="o">=</span> <span class="o">-</span><span class="nb">min</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">psi</span><span class="p">[</span><span class="n">dp</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dp</span><span class="p">[</span><span class="n">dp</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">dA</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dA</span><span class="p">[</span><span class="n">dA</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">dL</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dL</span><span class="p">[</span><span class="n">dL</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])])</span>
          <span class="n">b_max</span> <span class="o">=</span> <span class="o">-</span><span class="nb">max</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">psi</span><span class="p">[</span><span class="n">dp</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dp</span><span class="p">[</span><span class="n">dp</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">dA</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dA</span><span class="p">[</span><span class="n">dA</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]),</span>
                        <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">dL</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">dL</span><span class="p">[</span><span class="n">dL</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">])])</span>
          <span class="n">b_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mf">.99</span> <span class="o">*</span> <span class="n">b_min</span><span class="p">,</span> <span class="n">b_lims</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
          <span class="n">b_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="mf">.99</span> <span class="o">*</span> <span class="n">b_max</span><span class="p">,</span> <span class="n">b_lims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
          <span class="n">bs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">b_min</span><span class="p">,</span> <span class="n">b_max</span><span class="p">,</span> <span class="n">num_plot_vals</span><span class="p">)</span>
          <span class="n">bs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])))))</span>
          <span class="n">bs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span>
          <span class="n">lls_for_plot</span> <span class="o">=</span> <span class="p">[]</span>
          <span class="n">eclls_for_plot</span> <span class="o">=</span> <span class="p">[]</span>
          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
              <span class="n">ll</span> <span class="o">=</span> <span class="n">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dp</span><span class="p">,</span> <span class="n">A</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dA</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dL</span><span class="p">,</span> <span class="n">dt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
              <span class="n">lls_for_plot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
              <span class="n">rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dL</span><span class="p">)</span> <span class="o">*</span> <span class="n">dt</span>
              <span class="n">ecll</span> <span class="o">=</span> <span class="p">((</span><span class="n">gamma</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">psi</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dp</span><span class="p">)</span> <span class="o">+</span>
                       <span class="p">(</span><span class="n">xi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">dA</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span>
                       <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
                       <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="n">T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>
              <span class="n">eclls_for_plot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ecll</span><span class="p">)</span>
              <span class="k">if</span> <span class="n">b</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                  <span class="n">diff_ll</span> <span class="o">=</span> <span class="n">ll</span> <span class="o">-</span> <span class="n">ecll</span>
          <span class="n">lls_for_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lls_for_plot</span><span class="p">)</span>
          <span class="n">eclls_for_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eclls_for_plot</span><span class="p">)</span> <span class="o">+</span> <span class="n">diff_ll</span>
          <span class="n">save_vals</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">lls_for_plot</span><span class="p">,</span> <span class="n">eclls_for_plot</span><span class="p">))</span>
      <span class="c1"># return new parameter</span>
      <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">psi_new</span><span class="p">,</span> <span class="n">A_new</span><span class="p">,</span> <span class="n">L_new</span>

  <span class="n">ll</span> <span class="o">=</span> <span class="n">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">lls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch: </span><span class="si">{</span><span class="n">epochs</span><span class="si">:</span><span class="s1">3d</span><span class="si">}</span><span class="s1">, ll = </span><span class="si">{</span><span class="n">ll</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">save_vals</span><span class="p">,</span> <span class="n">lls</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span>

<span class="k">def</span> <span class="nf">plot_lls</span><span class="p">(</span><span class="n">lls</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plots log likelihoods at each epoch.</span>
<span class="sd">  Args:</span>
<span class="sd">    lls (list of floats) log likelihoods at each epoch.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">epochs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span> <span class="p">,</span> <span class="n">lls</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
  <span class="n">span</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span> <span class="o">-</span> <span class="n">span</span> <span class="o">*</span> <span class="mf">0.05</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span> <span class="o">+</span> <span class="n">span</span> <span class="o">*</span> <span class="mf">0.05</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;log likelihood&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_lls_eclls</span><span class="p">(</span><span class="n">plot_epochs</span><span class="p">,</span> <span class="n">save_vals</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plots log likelihoods at each epoch.</span>
<span class="sd">  Args:</span>
<span class="sd">    plot_epochs (list of ints):  Which epochs were saved to plot.</span>
<span class="sd">    save_vals (lists of floats): Different likelihoods from EM for plotting.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">rows</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">plot_epochs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">save_vals</span><span class="p">))</span> <span class="o">/</span> <span class="mi">3</span><span class="p">))</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">.7</span> <span class="o">*</span> <span class="mf">6.4</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">.7</span> <span class="o">*</span> <span class="mf">4.8</span> <span class="o">*</span> <span class="n">rows</span><span class="p">))</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

  <span class="n">minll</span><span class="p">,</span> <span class="n">maxll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">lls_for_plot</span><span class="p">,</span> <span class="n">eclls_for_plot</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">save_vals</span><span class="p">)):</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.15</span><span class="p">,</span> <span class="mf">2.15</span><span class="p">])</span>
      <span class="n">min_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">lls_for_plot</span><span class="p">,</span> <span class="n">eclls_for_plot</span><span class="p">))</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
      <span class="n">max_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">lls_for_plot</span><span class="p">,</span> <span class="n">eclls_for_plot</span><span class="p">))</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

      <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">min_val</span><span class="p">,</span> <span class="n">lls_for_plot</span><span class="p">[</span><span class="n">bs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]],</span> <span class="s1">&#39;--b&#39;</span><span class="p">)</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">min_val</span><span class="p">,</span> <span class="n">lls_for_plot</span><span class="p">[</span><span class="n">bs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]],</span> <span class="s1">&#39;--b&#39;</span><span class="p">)</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta^</span><span class="si">{</span><span class="n">plot_epochs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span>
                          <span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta^</span><span class="si">{</span><span class="n">plot_epochs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">])</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>


      <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">lls_for_plot</span><span class="p">)</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">eclls_for_plot</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">min_val</span> <span class="o">&lt;</span> <span class="n">minll</span><span class="p">:</span> <span class="n">minll</span> <span class="o">=</span> <span class="n">min_val</span>
      <span class="k">if</span> <span class="n">max_val</span> <span class="o">&gt;</span> <span class="n">maxll</span><span class="p">:</span> <span class="n">maxll</span> <span class="o">=</span> <span class="n">max_val</span>

      <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log likelihood&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
          <span class="n">l</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">lines</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="p">[</span><span class="s1">&#39;LL&#39;</span><span class="p">,</span> <span class="s1">&#39;ECLL&#39;</span><span class="p">],</span> <span class="n">framealpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_learnt_vs_true</span><span class="p">(</span><span class="n">L_true</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">A_true</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Plot and compare the true and learnt parameters.</span>

<span class="sd">  Args:</span>
<span class="sd">    L_true (numpy array): True L.</span>
<span class="sd">    L (numpy array):      Estimated L.</span>
<span class="sd">    A_true (numpy array): True A.</span>
<span class="sd">    A (numpy array):      Estimated A.</span>
<span class="sd">    dt (float):           Bin length.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">C</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">L_true</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">*</span> <span class="mf">1.05</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">L_true</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">*</span> <span class="mf">1.05</span><span class="p">],</span> <span class="s1">&#39;--b&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
          <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">L_true</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">L</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="p">),</span>
                   <span class="n">marker</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># this line will fail for K &gt; 3</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True firing rate (Hz)&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Inferred firing rate (Hz)&#39;</span><span class="p">)</span>
  <span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="o">^</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="o">^</span><span class="mi">6</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="o">^</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="o">^</span><span class="mi">6</span><span class="p">],</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
  <span class="n">l</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">lines</span><span class="p">[</span><span class="o">-</span><span class="n">C</span> <span class="o">-</span> <span class="n">K</span><span class="p">:],</span>
                 <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;cell </span><span class="si">{</span><span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;state </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">)])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">),</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">ymax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A_true</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">A_true</span><span class="p">)))</span> <span class="o">/</span> <span class="n">dt</span> <span class="o">*</span> <span class="mf">1.05</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="p">],</span> <span class="s1">&#39;--b&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">:</span> <span class="k">continue</span>
          <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">A_true</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">dt</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">dt</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True transition rate (Hz)&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Inferred transition rate (Hz)&#39;</span><span class="p">)</span>
  <span class="n">l</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">lines</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="p">[</span><span class="s1">&#39;state 1 -&gt; 2&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;state 1 -&gt; 3&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;state 2 -&gt; 1&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;state 2 -&gt; 3&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;state 3 -&gt; 1&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;state 3 -&gt; 2&#39;</span>
                                    <span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-and-simulation-parameters">
<h3>Model and simulation parameters<a class="headerlink" href="#model-and-simulation-parameters" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># model and data parameters</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># number of cells</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># number of states</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.002</span>  <span class="c1"># seconds</span>
<span class="n">trial_T</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># seconds</span>
<span class="n">n_frozen_trials</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># used to plot multiple trials with the same state sequence</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># number of trials (each has it&#39;s own state sequence)</span>

<span class="c1"># for random data</span>
<span class="n">max_firing_rate</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Hz</span>
<span class="n">max_transition_rate</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Hz</span>

<span class="c1"># needed to plot LL and ECLL for every M-step</span>
<span class="c1"># **This substantially slows things down!!**</span>
<span class="n">num_plot_vals</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># resolution of the plot (this is the expensive part)</span>
<span class="n">b_lims</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># lower limit on graph (b = 0 is start-of-M-step LL; b = 1 is end-of-M-step LL)</span>
<span class="n">plot_epochs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">))</span>  <span class="c1"># list of epochs to plot</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="initialize-true-model">
<h3>Initialize true model<a class="headerlink" href="#initialize-true-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">trial_T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># initial state distribution</span>
<span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">psi</span> <span class="o">=</span> <span class="n">psi</span> <span class="o">/</span> <span class="n">psi</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># off-diagonal transition rates sampled uniformly</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_transition_rate</span> <span class="o">*</span> <span class="n">dt</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">))</span> <span class="o">*</span> <span class="n">A</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># hand-crafted firing rates make good plots</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">.02</span><span class="p">,</span> <span class="mf">.8</span><span class="p">,</span> <span class="mf">.37</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">.7</span><span class="p">,</span> <span class="mf">.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">.92</span><span class="p">,</span> <span class="mf">.07</span><span class="p">,</span> <span class="mf">.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">.25</span><span class="p">,</span> <span class="mf">.42</span><span class="p">,</span> <span class="mf">.75</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">.15</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.85</span><span class="p">]</span>
<span class="p">])</span> <span class="o">*</span> <span class="n">max_firing_rate</span>     <span class="c1"># (C,K)</span>

<span class="c1"># Save true parameters for comparison later</span>
<span class="n">psi_true</span> <span class="o">=</span> <span class="n">psi</span>
<span class="n">A_true</span> <span class="o">=</span> <span class="n">A</span>
<span class="n">L_true</span> <span class="o">=</span> <span class="n">L</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generate-data-with-frozen-sequence-and-plot">
<h3>Generate data with frozen sequence and plot<a class="headerlink" href="#generate-data-with-frozen-sequence-and-plot" title="Permalink to this headline">¶</a></h3>
<p>Given a state sequence <code class="docutils literal notranslate"><span class="pre">[0,1,1,3,2,...]</span></code>, we’ll first convert each state in to sequence in to the so-called “one-hot” coding. For example, with 5 total states, the one-hot coding of state <code class="docutils literal notranslate"><span class="pre">0</span></code> is <code class="docutils literal notranslate"><span class="pre">[1,0,0,0,0]</span></code> and the coding for state <code class="docutils literal notranslate"><span class="pre">3</span></code> is <code class="docutils literal notranslate"><span class="pre">[0,0,0,1,0]</span></code>. Suppose we now have a sequence of length <code class="docutils literal notranslate"><span class="pre">T</span></code>, the one-hot coding of this sequence <code class="docutils literal notranslate"><span class="pre">Xf</span></code> will have shape <code class="docutils literal notranslate"><span class="pre">(T,K)</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="c1"># sample n_frozen_trials state sequences</span>
<span class="n">Xf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">Xf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">psi</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">Xf</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">Xf</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],:]</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

<span class="c1"># switch to one-hot encoding of the state</span>
<span class="n">Xf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)[</span><span class="n">Xf</span><span class="p">]</span>  <span class="c1"># (T,K)</span>

<span class="c1"># get the Y values</span>
<span class="n">Rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">Xf</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">dt</span>  <span class="c1"># (T,C)</span>

<span class="n">Rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">Rates</span><span class="p">,</span> <span class="p">[</span><span class="n">n_frozen_trials</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (n_trials, T, C)</span>
<span class="n">Yf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">Rates</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_spike_train</span><span class="p">(</span><span class="n">Xf</span><span class="p">,</span> <span class="n">Yf</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generate-data-for-em-learning">
<h3>Generate data for EM learning<a class="headerlink" href="#generate-data-for-em-learning" title="Permalink to this headline">¶</a></h3>
<p>The previous dataset is generated with the same state sequence for visualization. Now let’s generate <code class="docutils literal notranslate"><span class="pre">n_trials</span></code> trials of observations, each one with its own randomly generated sequence</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="c1"># sample n_trials state sequences</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">psi_true</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_trials</span><span class="p">))</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
  <span class="n">X</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">A_true</span><span class="p">[</span><span class="n">X</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_trials</span><span class="p">))</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># switch to one-hot encoding of the state</span>
<span class="n">one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">K</span><span class="p">])</span>

<span class="c1"># get the Y values</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">L_true</span> <span class="o">@</span> <span class="n">X</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>  <span class="c1"># (n_trials, T, C)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Y has shape: (n_trial=</span><span class="si">{}</span><span class="s2">,T=</span><span class="si">{}</span><span class="s2">,C=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="optional-section-em-algorithm-for-hmm">
<h2><strong>Optional</strong> Section: EM algorithm for HMM<a class="headerlink" href="#optional-section-em-algorithm-for-hmm" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 6: EM Tutorial</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;umU4wUWlKvg&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
<p>Finding the optimal values of parameters that maximizes the data likelihood is practically infeasible since we need to integrating out all latent variables <span class="math notranslate nohighlight">\(x_{1:T}\)</span>. The time needed is exponential to <span class="math notranslate nohighlight">\(T\)</span>. Thus as an alternative approach, we use the Expectation-Maximization algorithm, which iteratively performing a E-step followed by a M-step and is guaranteed to not decrease(usually increase) the data likelihood after each EM cycle.</p>
<p>In this section we will briefly review the EM algorithm  for HMM and list</p>
<ul class="simple">
<li><p>Recursive equations for forward and backward probabilities <span class="math notranslate nohighlight">\(a_i(t)\)</span> and <span class="math notranslate nohighlight">\(b_i(t)\)</span></p></li>
<li><p>Expressions for singleton and pairwise marginal distributions after seeing data: <span class="math notranslate nohighlight">\(\gamma_{i}(t):=p_{\theta}\left(x_{t}=i | Y_{1: T}\right)\)</span> and <span class="math notranslate nohighlight">\(\xi_{i j}(t) = p_{\theta}(x_t=i,x_{t+1}=j|Y_{1:T})\)</span></p></li>
<li><p>Closed-form solutions for updated values of <span class="math notranslate nohighlight">\(A,\psi,\lambda\)</span> which increases data likelihood</p></li>
</ul>
<hr class="docutils" />
<div class="section" id="e-step-forward-backward-algorithm">
<h3>E-step: Forward-backward algorithm<a class="headerlink" href="#e-step-forward-backward-algorithm" title="Permalink to this headline">¶</a></h3>
<p>In the forward pass, we calculate the <strong>forward probabilities</strong>, or the joint probability of <span class="math notranslate nohighlight">\(x_t\)</span> and current and past data <span class="math notranslate nohighlight">\(Y_{1:t}\)</span>: <span class="math notranslate nohighlight">\(a_i(t):=p(x_t=i,Y_{1:t})\)</span> recursively by</p>
<div class="math notranslate nohighlight">
\[a_i(t) = p_(y_t|x_i=t)\sum_j A_{ji} a_j(t-1)\]</div>
<p>In contrast to the intro, now <span class="math notranslate nohighlight">\(A_{ji}\)</span> means <strong>the transition probability from state <span class="math notranslate nohighlight">\(j\)</span> to state <span class="math notranslate nohighlight">\(i\)</span>.</strong></p>
<p>The backward pass calculate the <strong>backward probabilities</strong> <span class="math notranslate nohighlight">\(b_i(t):=p_{\theta}(Y_{t+1:T}|x_t=i)\)</span>, which is the likelihood of observing all future data points given current state <span class="math notranslate nohighlight">\(x_t\)</span>. The recursion of <span class="math notranslate nohighlight">\(b_i(t)\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[ b_i(t) = \sum_j p_{\theta}(y_{t+1}|x_{t+1}=j)b_j(t+1)A_{ij} \]</div>
<p>Combining all past and future information, the <strong>singleton and pairwise marginal distributions</strong> are given by</p>
<div class="math notranslate nohighlight">
\[ \gamma_{i}(t):=p_{\theta}\left(x_{t}=i | Y_{1: T}\right)=\frac{a_{i}(t) b_{i}(t)}{p_{\theta}\left(Y_{1: T}\right)} \]</div>
<div class="math notranslate nohighlight">
\[  \xi_{i j}(t) = p_{\theta}(x_t=i,x_{t+1}=j|Y_{1:T}) =\frac{b_{j}(t+1)p_{\theta}\left(y_{t+1} | x_{t+1}=j\right) A_{i j} a_{i}(t)}{p_{\theta}\left(Y_{1: T}\right)} \]</div>
<p>where <span class="math notranslate nohighlight">\(p_{\theta}(Y_{1:T})=\sum_i a_i(T)\)</span>.</p>
</div>
<hr class="docutils" />
<div class="section" id="m-step">
<h3>M-step<a class="headerlink" href="#m-step" title="Permalink to this headline">¶</a></h3>
<p>The M-step for HMM has a closed-form solution. First the new transition matrix is given by
$<span class="math notranslate nohighlight">\( 
 A_{ij} =\frac{\sum_{t=1}^{T-1} \xi_{i j}(t)}{\sum_{t=1}^{T-1} \gamma_{i}(t)}
\)</span>$</p>
<p>which is the expected empirical transition probabilities.
New initial probabilities and parameters of the emission models are also given by their empirical values given single and pairwise marginal distributions:</p>
<div class="math notranslate nohighlight">
\[ \psi_i = \frac{1}{N}\sum_{trials}\gamma_i(1) \]</div>
<div class="math notranslate nohighlight">
\[ \lambda_{i}^{c}=\frac{\sum_{t} \gamma_{i}(t) y_{t}^{c}}{\sum_{t} \gamma_{i}(t) d t}\]</div>
</div>
<hr class="docutils" />
<div class="section" id="e-step-forward-and-backward-algorithm">
<h3>E-step: forward and backward algorithm<a class="headerlink" href="#e-step-forward-and-backward-algorithm" title="Permalink to this headline">¶</a></h3>
<p><strong>(Optional)</strong></p>
<p>In this section you will read through the code for the forward-backward algorithm and understand how to implement the computation efficiently in <code class="docutils literal notranslate"><span class="pre">numpy</span></code> by calculating the recursion for all trials at once.</p>
<hr class="docutils" />
<p>Let’s re-write the forward and backward recursions in a more compact form:</p>
<div class="math notranslate nohighlight">
\[ a_i^t = \sum_j A_{ji}o_j^t a_j^{t-1}  \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}b^t_i = \sum_j A_{ij} o_j^{t+1}b_j^{t+1} $$ where $o_j^{t}=p(y_{t}|x_{t}=j)$.\\
Let's take the backward recursion for example. In practice we will handle all trials together since they are independent of each other. After adding a trial index $l$ to the recursion equations, the backward recursion becomes:\\$$b^t_{li} = \sum_j A_{ij} o_{lj}^{t+1}b_{lj}^{t+1} \end{aligned}\end{align} \]</div>
<p>What we have in hand are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">A</span></code>: matrix of size <code class="docutils literal notranslate"><span class="pre">(K,K)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">o^{t+1}</span></code>: array of size <code class="docutils literal notranslate"><span class="pre">(N,K)</span></code> is the log data likelihood for all trials at a given time</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b^{t+1}</span></code>: array of size <code class="docutils literal notranslate"><span class="pre">(N,K)</span></code> is the backward probability for all trials at a given time</p></li>
</ul>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> stands for the number of trials.</p>
<p>The index size and meaning doesn’t match for these three arrays: the index is <span class="math notranslate nohighlight">\(i\)</span> for <span class="math notranslate nohighlight">\(A\)</span> in the first dimension and is <span class="math notranslate nohighlight">\(l\)</span> for <span class="math notranslate nohighlight">\(o\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, so we can’t just multiply them together. However, we can do this by viewing vectors <span class="math notranslate nohighlight">\(o^{t+1}_{l\cdot}\)</span> and <span class="math notranslate nohighlight">\(b^{t+1}_{l\cdot}\)</span> as a matrix with 1 row and re-write the backward equation as:</p>
<div class="math notranslate nohighlight">
\[b^t_{li} = \sum_j A_{ij} o_{l1j}^{t+1}b_{l1j}^{t+1} \]</div>
<p>Now we can just multiply these three arrays element-wise and sum over the last dimension.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, we can achieve this by indexing the array with <code class="docutils literal notranslate"><span class="pre">None</span></code> at the location we want to insert a dimension. Take <code class="docutils literal notranslate"><span class="pre">b</span></code> with size <code class="docutils literal notranslate"><span class="pre">(N,T,K)</span></code> for example,<code class="docutils literal notranslate"><span class="pre">b[:,t,:]</span></code> will have shape <code class="docutils literal notranslate"><span class="pre">(N,K)</span></code>, <code class="docutils literal notranslate"><span class="pre">b[:,t,None,:]</span></code> will have shape <code class="docutils literal notranslate"><span class="pre">(N,1,K)</span></code> and <code class="docutils literal notranslate"><span class="pre">b[:,t,:,None]</span></code> will have shape <code class="docutils literal notranslate"><span class="pre">(N,K,1)</span></code>.</p>
<p>So the backward recursion computation can be implemented as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="p">[:,</span><span class="n">t</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">*</span> <span class="n">o</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="kc">None</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[:,</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="kc">None</span><span class="p">,:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<p>In addition to the trick introduced above, in this exercise we will work in the <strong>log scale</strong> for numerical stability.</p>
<p><strong>Suggestions</strong></p>
<ol class="simple">
<li><p>Take a look at the code for the forward recursion and backward recursion.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculate the E-step for the HMM spiking model.</span>

<span class="sd">  Args:</span>
<span class="sd">    Y (numpy 3d array): tensor of recordings, has shape (n_trials, T, C)</span>
<span class="sd">    psi (numpy vector): initial probabilities for each state</span>
<span class="sd">    A (numpy matrix):   transition matrix, A[i,j] represents the prob to</span>
<span class="sd">                        switch from i to j. Has shape (K,K)</span>
<span class="sd">    L (numpy matrix):   Poisson rate parameter for different cells.</span>
<span class="sd">                        Has shape (C,K)</span>
<span class="sd">    dt (float):         Bin length</span>

<span class="sd">  Returns:</span>
<span class="sd">    ll (float):             data log likelihood</span>
<span class="sd">    gamma (numpy 3d array): singleton marginal distribution.</span>
<span class="sd">                            Has shape (n_trials, T, K)</span>
<span class="sd">    xi (numpy 4d array):    pairwise marginal distribution for adjacent</span>
<span class="sd">                            nodes . Has shape (n_trials, T-1, K, K)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">n_trials</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">T</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">K</span> <span class="o">=</span> <span class="n">psi</span><span class="o">.</span><span class="n">size</span>
  <span class="n">log_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
  <span class="n">log_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

  <span class="n">log_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
  <span class="n">log_obs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">L</span> <span class="o">*</span> <span class="n">dt</span><span class="p">)</span><span class="o">.</span><span class="n">logpmf</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># n_trials, T, K</span>

  <span class="c1"># forward pass</span>
  <span class="n">log_a</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_obs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">psi</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">log_A</span> <span class="o">+</span> <span class="n">log_a</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:</span> <span class="p">,</span><span class="kc">None</span><span class="p">]</span>  <span class="c1"># (n_trials, K,K)</span>
    <span class="n">maxtmp</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_trials,K)</span>
    <span class="n">log_a</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_obs</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">maxtmp</span> <span class="o">+</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tmp</span> <span class="o">-</span> <span class="n">maxtmp</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)))</span>

  <span class="c1"># backward pass</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">log_A</span> <span class="o">+</span> <span class="n">log_b</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">log_obs</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">maxtmp</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">log_b</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">maxtmp</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tmp</span> <span class="o">-</span> <span class="n">maxtmp</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

  <span class="c1"># data log likelihood</span>
  <span class="n">maxtmp</span> <span class="o">=</span> <span class="n">log_a</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_a</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxtmp</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">maxtmp</span>

  <span class="c1"># singleton and pairwise marginal distributions</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_a</span> <span class="o">+</span> <span class="n">log_b</span> <span class="o">-</span> <span class="n">ll</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_a</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">log_obs</span> <span class="o">+</span> <span class="n">log_b</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span>
              <span class="n">log_A</span> <span class="o">-</span> <span class="n">ll</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">ll</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="n">T</span> <span class="o">/</span> <span class="n">dt</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">xi</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title EXERCISE 7: Implement the M-step Video</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;H4GGTg_9BaE&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="exercise-5-implement-the-m-step">
<h3>EXERCISE 5: Implement the M-step<a class="headerlink" href="#exercise-5-implement-the-m-step" title="Permalink to this headline">¶</a></h3>
<p>In this exercise you will complete the M-step for this HMM using closed form solutions mentioned before.</p>
<p><strong>Suggestions</strong></p>
<ol class="simple">
<li><p>Calculate new initial probabilities as empirical counts of singleton marginals</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \psi_i = \frac{1}{N}\sum_{trials}\gamma_i(1) \]</div>
<ol class="simple">
<li><p>Remember the extra trial dimension and average over all trials</p></li>
</ol>
<p><strong>For reference:</strong></p>
<p>New transition matrix is calculated as empirical counts of transition events from marginals</p>
<div class="math notranslate nohighlight">
\[ A_{ij} =\frac{\sum_{t=1}^{T-1} \xi_{i j}(t)}{\sum_{t=1}^{T-1} \gamma_{i}(t)}\]</div>
<p>New spiking rates for each cell and each state are given by</p>
<div class="math notranslate nohighlight">
\[  \lambda_{i}^{c}=\frac{\sum_{t} \gamma_{i}(t) y_{t}^{c}}{\sum_{t} \gamma_{i}(t) d t} \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">m_step</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">dt</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculate the M-step updates for the HMM spiking model.</span>

<span class="sd">  Args:</span>
<span class="sd">    gamma ():       Number of epochs of EM to run</span>
<span class="sd">    xi (numpy 3d array): Tensor of recordings, has shape (n_trials, T, C)</span>
<span class="sd">    dt (float):         Duration of a time bin</span>

<span class="sd">  Returns:</span>
<span class="sd">    psi_new (numpy vector): Updated initial probabilities for each state</span>
<span class="sd">    A_new (numpy matrix):   Updated transition matrix, A[i,j] represents the</span>
<span class="sd">                            prob. to switch from j to i. Has shape (K,K)</span>
<span class="sd">    L_new (numpy matrix):   Updated Poisson rate parameter for different</span>
<span class="sd">                            cells. Has shape (C,K)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`m_step` need to be implemented&quot;</span><span class="p">)</span>
  <span class="c1">############################################################################</span>
  <span class="c1"># Insert your code here to:</span>
  <span class="c1">#    Calculate the new prior probabilities in each state at time 0</span>
  <span class="c1">#    Hint: Take the first time step and average over all trials</span>
  <span class="c1">###########################################################################</span>
  <span class="n">psi_new</span> <span class="o">=</span> <span class="o">...</span>
  <span class="c1"># Make sure the probabilities are normalized</span>
  <span class="n">psi_new</span> <span class="o">/=</span> <span class="n">psi_new</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

  <span class="c1"># Calculate new transition matrix</span>
  <span class="n">A_new</span> <span class="o">=</span> <span class="n">xi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="c1"># Calculate new firing rates</span>
  <span class="n">L_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">gamma</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">gamma</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">dt</span>
  <span class="k">return</span> <span class="n">psi_new</span><span class="p">,</span> <span class="n">A_new</span><span class="p">,</span> <span class="n">L_new</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D2_HiddenDynamics/solutions/W3D2_Tutorial2_Solution_ab737584.py"><em>Click for solution</em></a></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 8: Running and plotting EM</span>
<span class="c1"># Insert the ID of the corresponding youtube video</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;6UTsXxE3hG0&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtu.be/&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="run-em">
<h3>Run EM<a class="headerlink" href="#run-em" title="Permalink to this headline">¶</a></h3>
<p>####Initialization for parameters</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">101</span><span class="p">)</span>
<span class="c1"># number of EM steps</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># initial state distribution</span>
<span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">psi</span> <span class="o">=</span> <span class="n">psi</span> <span class="o">/</span> <span class="n">psi</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># off-diagonal transition rates sampled uniformly</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span> <span class="o">*</span> <span class="n">max_transition_rate</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">))</span> <span class="o">*</span> <span class="n">A</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># firing rates sampled uniformly</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="n">max_firing_rate</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># LL for true vs. initial parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;LL for true 𝜃:    </span><span class="si">{</span><span class="n">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi_true</span><span class="p">,</span> <span class="n">A_true</span><span class="p">,</span> <span class="n">L_true</span><span class="p">,</span> <span class="n">dt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;LL for initial 𝜃: </span><span class="si">{</span><span class="n">e_step</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Run EM</span>
<span class="n">save_vals</span><span class="p">,</span> <span class="n">lls</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">run_em</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">psi</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># EM doesn&#39;t guarantee the order of learnt latent states are the same as that of true model</span>
<span class="c1"># so we need to sort learnt parameters</span>

<span class="c1"># Compare all true and estimated latents across cells</span>
<span class="n">cost_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">L_true</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span> <span class="n">L</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">true_ind</span><span class="p">,</span> <span class="n">est_ind</span> <span class="o">=</span> <span class="n">linear_sum_assignment</span><span class="p">(</span><span class="n">cost_mat</span><span class="p">)</span>

<span class="n">psi</span> <span class="o">=</span> <span class="n">psi</span><span class="p">[</span><span class="n">est_ind</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">est_ind</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:,</span> <span class="n">est_ind</span><span class="p">]</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">L</span><span class="p">[:,</span> <span class="n">est_ind</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="optional-section-plotting-the-training-process-and-learnt-model">
<h2><strong>Optional</strong> Section: Plotting the training process and learnt model<a class="headerlink" href="#optional-section-plotting-the-training-process-and-learnt-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="plotting-progress-during-em">
<h3>Plotting progress during EM!<a class="headerlink" href="#plotting-progress-during-em" title="Permalink to this headline">¶</a></h3>
<p>Now you can</p>
<ul class="simple">
<li><p>Plot the likelihood during training</p></li>
<li><p>Plot the M-step log likelihood versus expected complete log likelihood(ECLL) to get an intuition of how EM works and the convexity of ECLL</p></li>
<li><p>Plot learnt parameters versus true parameters</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the log likelihood after each epoch of EM</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_lls</span><span class="p">(</span><span class="n">lls</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For each saved epoch, plot the log likelihood and expected complete log likelihood</span>
<span class="c1"># for the initial and final parameter values</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_lls_eclls</span><span class="p">(</span><span class="n">plot_epochs</span><span class="p">,</span> <span class="n">save_vals</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plot-learnt-parameters-vs-true-parameters">
<h3>Plot learnt parameters vs. true parameters<a class="headerlink" href="#plot-learnt-parameters-vs-true-parameters" title="Permalink to this headline">¶</a></h3>
<p>Now we will plot the (sorted) learnt parameters with true parameters to see if we successfully recovered all the parameters</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare true and learnt parameters</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_learnt_vs_true</span><span class="p">(</span><span class="n">L_true</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">A_true</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W3D2_HiddenDynamics/student"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="W3D2_Tutorial1.html" title="previous page">Tutorial 1: Sequential Probability Ratio Test</a>
    <a class='right-next' id="next-link" href="W3D2_Tutorial3.html" title="next page">Tutorial 3: 1D Kalman Filter</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>