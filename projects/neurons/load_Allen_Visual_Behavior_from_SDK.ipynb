{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1CeSqXjhY2k"
   },
   "source": [
    "# Overview\n",
    "The following notebook demonstrates how to load neural data for all imaging planes in one 2-photon imaging session into a single 'tidy' dataframe, make  simple event-triggered plots, and do some basic analysis using scikit-learn.\n",
    "\n",
    "This is designed to demonstrate a simple method for interacting with the visual behavior data. Many aspects of the dataset are not explored here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT9844s2hkvw"
   },
   "source": [
    "# Set up environment and import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Maa0v-hvoD"
   },
   "source": [
    "We have built a package called `mindscope_utilities` which contains some useful convenience functions. The `allenSDK` is a dependency of this package and will be automatically installed when you install `mindscope_utilities` per the instrutions below.\n",
    "\n",
    "We will first install `mindscope_utilities` into our colab environment by running the commands below. When this cell is complete, click on the `RESTART RUNTIME` button that appears at the end of the output. Note that running this cell will produce a long list of outputs and some error messages. Clicking `RESTART RUNTIME` at the end will resolve these issues. \n",
    "\n",
    "You can minimize the cell after you are done to hide the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPF6qq5jLmNL"
   },
   "source": [
    "#### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xm2F5-0aIN2"
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install mindscope_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iT7O4WaiYAH"
   },
   "source": [
    "#### Next we will import packages we need later in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgFHDxOcaJut"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mindscope_utilities\n",
    "import mindscope_utilities.visual_behavior_ophys as ophys\n",
    "\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjFtKoQ2id_6"
   },
   "source": [
    "# Load the session and experiment summary tables\n",
    "\n",
    "The AllenSDK provides functionality for downloading tables that describe all sessions and experiments (individual imaging planes) in the Visual Behavior 2P dataset. We first download the data cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvamvtoUbWTR"
   },
   "outputs": [],
   "source": [
    "data_storage_directory = \"/temp\" # Note: this path must exist on your local drive\n",
    "cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_storage_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i-bdFCLQImS"
   },
   "source": [
    "- `Ophys_session_table` contains metadata describing imaging sessions. If more than one plane was imaged during a session, one ophys session id will be associated multiple ophys experiment ids. Each ophys session id will also have a unique behavior session id. \n",
    "- `Behavior_session_table` contains metadata describing behavioral sessions, which may or may not be during imaging. Behavior session ids that do not have ophys session ids were training sessions. \n",
    "- `Ophys_experiment_table` contains metadata describing imaging experiments (aka imaging planes). When mesoscope is used, one ophys session may contain up to 8 unique experiments (two visual areas by four imaging depths). Some imaging planes may not be released due to quality control issues, thus each ophys session id is associated with anywhere from one to eight unique experiment ids. Ophys experiment ids are unique and do not repeat across sessions. To find the same imaging plane that was matched across multiple sessions, use the `ophys_container_id` column that can be found in both `ophys_session_table` and `ophys_experiment_table`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL5LV5fti5tc"
   },
   "source": [
    "Then we can access the session and experiment tables directly.\n",
    "\n",
    "Note that a 'session' is a single behavioral session. Sessions that are performed on the mesoscope will have multiple (up to 8) 'experiments' associated with them, where an experiment is a distinct imaging plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOCbdeNbd7w8"
   },
   "outputs": [],
   "source": [
    "session_table = cache.get_ophys_session_table()\n",
    "experiment_table = cache.get_ophys_experiment_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUguZ4QGjQF7"
   },
   "source": [
    "We can then view the contents of the session table. Note that this contains a lot of useful metadata about each session. One of the columns, `ophys_experiment_id` provides a list of the experiments (aka imaging planes) that are associated with each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "t5JqR9KxjUB_",
    "outputId": "e21fc68f-508f-4e3f-947e-d1c8ac768790"
   },
   "outputs": [],
   "source": [
    "session_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0BhnucakDQi"
   },
   "source": [
    "The experiment table has one row per experiment. Note that the `ophys_session_id` column links each experiment to its associated session in the session_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "VNAmRU0od-RC",
    "outputId": "d00c8d04-34c0-49bf-f9c7-2b70800cf8d5"
   },
   "outputs": [],
   "source": [
    "experiment_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0st3qOgskSQo"
   },
   "source": [
    "# Load one example session\n",
    "We are going to select one session from this table, session 854060305. This is a session with Sst-IRES-Cre mouse, which expressed GCaMP6f in Sst+ inhibitory interneurons. There were 6 simultaneously acquired imaging planes for this session. \n",
    "We can view metadata for this session as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zTik58MeBIP",
    "outputId": "fa16fa33-50dc-4266-81a7-5a569eb7cd57"
   },
   "outputs": [],
   "source": [
    "ophys_session_id = 854060305\n",
    "session_table.loc[ophys_session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZA65m3Lkk1o"
   },
   "source": [
    "# Download all associated experiments\n",
    "\n",
    "Each session consists of one or more 'experiments', in which each experiment is a single imaging plane\n",
    "\n",
    "Each mesoscope session has up to 8 experiments associated with the session. We will load all sessions into a dictionary with the experiment IDs as the keys\n",
    "\n",
    "The first time that this cell is run, the associated NWB files will be downloaded to your local `data_storage_directory`. Subsequent runs of this cell will be faster since the data will already be cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovs_vUtreDwQ"
   },
   "outputs": [],
   "source": [
    "experiments = {}\n",
    "ophys_experiment_ids = session_table.loc[ophys_session_id]['ophys_experiment_id']\n",
    "for ophys_experiment_id in ophys_experiment_ids:\n",
    "    experiments[ophys_experiment_id] = cache.get_behavior_ophys_experiment(ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the max projection and one cell ROI for one of the experiments\n",
    "We can view the `cell_specimen_table` for one experiment, which contains information about each identified cell in that experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments[ophys_experiment_ids[1]]\n",
    "experiment.cell_specimen_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize the max projection and one of the identified ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,8), sharex=True, sharey=True)\n",
    "ax[0].imshow(experiment.max_projection, cmap='gray')\n",
    "ax[0].set_title('max projection')\n",
    "\n",
    "cell_specimen_id = experiment.cell_specimen_table.index[2]\n",
    "ax[1].imshow(experiment.cell_specimen_table.loc[cell_specimen_id]['roi_mask'])\n",
    "ax[1].set_title('ROI mask for cell_specimen_id = {}'.format(cell_specimen_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VluMgUHNaJT"
   },
   "source": [
    "## Load neural data into memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7X3NexHk52M"
   },
   "source": [
    "The cell below will load the neural data into memory in the pandas 'tidy' format by iterating over each of the 6 experiments and using some helpful tools from the `visual_behavior_ophys` module of the `mindscope_utilities` package that was imported above as `ophys`. \n",
    "\n",
    "It will also include a subset of metadata from `ophys_experiment_table` to facilitate splitting by depth, structure (aka cortical area), cre line (aka cell class), etc.\n",
    "\n",
    "Note that 'tidy' data means that each row represents only one observation. Observations are stacked vertically. Thus, the `timestamps` colums will repeat for every cell in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBK4b8tveHFY",
    "outputId": "78166b3e-c312-4672-95e7-c082ebaebb81"
   },
   "outputs": [],
   "source": [
    "neural_data = []\n",
    "for ophys_experiment_id in tqdm(experiments.keys()): #tqdm is a package that shows progress bars for items that are iterated over\n",
    "    this_experiment = experiments[ophys_experiment_id]\n",
    "    this_experiment_neural_data = ophys.build_tidy_cell_df(this_experiment)\n",
    "    \n",
    "    # add some columns with metadata for the experiment\n",
    "    metadata_keys = [\n",
    "        'ophys_experiment_id',\n",
    "        'ophys_session_id',\n",
    "        'targeted_structure',\n",
    "        'imaging_depth',\n",
    "        'equipment_name',\n",
    "        'cre_line',\n",
    "        'mouse_id',\n",
    "        'sex',\n",
    "    ]\n",
    "    for metadata_key in metadata_keys:\n",
    "        this_experiment_neural_data[metadata_key] = this_experiment.metadata[metadata_key]\n",
    "        \n",
    "    # append the data for this experiment to a list\n",
    "    neural_data.append(this_experiment_neural_data)\n",
    "    \n",
    "# concatate the list of dataframes into a single dataframe\n",
    "neural_data = pd.concat(neural_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVWZxgjplDxi"
   },
   "source": [
    "We can then look at some attributes of the `neural_data` dataframe we have created.\n",
    "\n",
    "It is ~2.5 million rows long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlRuy7swe3mm",
    "outputId": "39e54a6d-4ea7-4163-8608-4cab507ee371"
   },
   "outputs": [],
   "source": [
    "len(neural_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYWDNmNAlPYw"
   },
   "source": [
    "It is so long because has one row for each timestamp for each cell. \n",
    "\n",
    "Below are the first 5 entries. Again, note that the `tidy` format means that each row has only one observation, which represents a single GCaMP6 fluorescnce value for a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "kqDLR0nXfG1e",
    "outputId": "b7459c26-7798-4f02-e302-f72b9e7ed569"
   },
   "outputs": [],
   "source": [
    "neural_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2glAmOHcXUqL"
   },
   "source": [
    "- The `cell_roi_id` column contains unique roi ids for all cells in a given experiment, which do not repeat across ophys sessions. \n",
    "- The `cell_specimen_id` column contains unique ids for cells that were matched across ophys sessions. Thus, a cell that was imaged in more than one session has multiple roi ids but one cell specimen id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L81e1f_nn1UB"
   },
   "source": [
    "# Examine Cell IDs\n",
    "We can get the unique Cell IDs in our dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fmU08NcrE1-",
    "outputId": "fe2e9fec-4c16-4f8e-9289-e5042a9d59a2"
   },
   "outputs": [],
   "source": [
    "cell_ids = neural_data['cell_specimen_id'].unique()\n",
    "print('there are {} unique cells'.format(len(cell_ids)))\n",
    "print('cell ids are: {}'.format(cell_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INjMP1-flW-p"
   },
   "source": [
    "If we wanted to get the timeseries for one cell, we could query the `neural_data` dataframe. For example, to get the full timeseries for the cell with `cell_specimen_id = 1086557208`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "QEq35_lAlmIT",
    "outputId": "f03cc7a2-1d5d-4b5c-c1fe-3a5bd9ed50ad"
   },
   "outputs": [],
   "source": [
    "single_cell_timeseries = neural_data.query('cell_specimen_id == 1086557208')\n",
    "single_cell_timeseries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6HyNqkRdtuL"
   },
   "source": [
    "Each cell has three types of traces:\n",
    "- `dff` column is the Calcium fluorescence signal, normalized to background fluorescence.\n",
    "- `events` column is deconvolved events from dff trace, which approximates neural firing rate and removes the slow decay of the Calcium signal (for more details, you can read EVENT DETECTION section in [Visual Behavior whitepaper](https://portal.brain-map.org/explore/circuits/visual-behavior-2p)).\n",
    "- `filtered_events` column is events smoothed with a half-gaussian kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GyejppnmGTq"
   },
   "source": [
    "We can then plot DeltaF/F for this cell for the full experiment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "DCzS8NJel7aG",
    "outputId": "26461d8f-74a7-46f8-b44f-d55efe030b00"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "single_cell_timeseries.plot(\n",
    "    x = 'timestamps',\n",
    "    y = 'dff',\n",
    "    ax = ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxGdf_w4mNvm"
   },
   "source": [
    "# Load stimulus data into memory\n",
    "The stimulus table is shared across all experiments (imaging planes) in a session. We can therefore use the stimulus table for just one experiment.\n",
    "\n",
    "We are going to drop the `image_set` column because it is not informative for our purposes. We can then view the first 10 rows of the stimulus table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "qaWD7-uXrCce",
    "outputId": "6f3c5acd-977c-42dd-9708-c6157ea4aa32"
   },
   "outputs": [],
   "source": [
    "stimulus_table = experiments[ophys_experiment_ids[0]].stimulus_presentations.drop(columns = ['image_set']) # dropping the 'image_set' column to avoid confusion. Image_set column contains a unique string for set of images presented in a session.\n",
    "stimulus_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the `stimulus_templates` attribute\n",
    "Note that the `unwarped` column contains the image before the application of a spherical warp. All of the pixels labeled 'NaN' will be off-screen (not visible to the mouse) after the warp is applied.\n",
    "\n",
    "All experiments in a given session will share the same `stimulus_templates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments[ophys_experiment_ids[0]]\n",
    "experiment.stimulus_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the unwarped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4,figsize = (20,8), sharex = True, sharey=True)\n",
    "for ii,image_name in enumerate(experiment.stimulus_templates.index):\n",
    "    ax.flatten()[ii].imshow(experiment.stimulus_templates.loc[image_name]['unwarped'], cmap='gray')\n",
    "    ax.flatten()[ii].set_title(image_name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the warped images\n",
    "This represents what was actually on the screen during the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4,figsize = (20,8), sharex = True, sharey=True)\n",
    "for ii,image_name in enumerate(experiment.stimulus_templates.index):\n",
    "    ax.flatten()[ii].imshow(experiment.stimulus_templates.loc[image_name]['warped'], cmap='gray')\n",
    "    ax.flatten()[ii].set_title(image_name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLC-mAhkm1BD"
   },
   "source": [
    "## Describe stimulus omissions\n",
    "An important feature of the task is that stimuli are shown at a very regular cadence (250 ms on, 500 ms off), but stimuli are randomly omitted with a probability of ~5%. These unexpected and random stimulus omissions could be perceived as an expectation violation by the mouse.\n",
    "\n",
    "Omitted stimuli are denoted in the `stimulus_table` by the `omitted` column. `True` means that the stimulus that would have been shown at that time was actually omitted (and was replaced by an extended gray screen between stimuli).\n",
    "\n",
    "We can look at the first 10 examples of omitted stimuli as follows. Note that each 'omitted' stimulus still has a 'start_time' and a 'stop_time' associated with it. This actually represents the time that a stimulus would have been shown, had it not been omitted.\n",
    "\n",
    "Stimulus omissions are also indicated in the `image_name` column by the string `omitted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "F8Mo2iErl6Q_",
    "outputId": "ec9c5ddc-4f4c-4cff-86ea-a578c1f7a6cc"
   },
   "outputs": [],
   "source": [
    "stimulus_table.query('omitted').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiJ0E5d8aJn2"
   },
   "source": [
    "# Create an event triggered response dataframe relative to omissions\n",
    "If we want to see how a given cell responds when regularly flashed stimuli are omitted, we can calculate the response around each of the stimulus omissions. The `mindscope_utilities` package has a convenience function to do this. We give the function:\n",
    "* a dataframe of interest (containing activity from one cell)\n",
    "* the t and y values of interest\n",
    "* the event times\n",
    "* how much time before and after each event we are interested in\n",
    "* the desired sampling rate of the output - this is the rate onto which the response will be interpolated\n",
    "\n",
    "The function will return a new dataframe with the response for the given cell, aligned to each of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-CNfHQk5rIRf",
    "outputId": "960020ee-6858-4b7f-efa2-3b8f92405dab"
   },
   "outputs": [],
   "source": [
    "cell_id = cell_ids[11]\n",
    "etr = mindscope_utilities.event_triggered_response(\n",
    "    data = neural_data.query('cell_specimen_id == @cell_id'),\n",
    "    t = 'timestamps',\n",
    "    y = 'dff',\n",
    "    event_times = stimulus_table.query('omitted')['start_time'],\n",
    "    t_before=3,\n",
    "    t_after=3,\n",
    "    output_sampling_rate = 50,\n",
    ")\n",
    "etr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ubI3kNcs7L"
   },
   "source": [
    "We can see that the output has colums for\n",
    "* `time` - this is our new timebase relative to the events. In this case, it ranges from -3 to 3\n",
    "* `dff` - this is the deltaF/F value surrounding each event, interpolated onto the new timebase. If, when calling the `event_triggered_response` function we had passed `y = 'events'`, this column would be events instead of dff.\n",
    "* `event_number` - this is an integer representing the count of each event. In this example, there were 185 omissions, so they are numbered from 0 to 184\n",
    "* `event_time` - this is the time of each event\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UwLWg7oePyU"
   },
   "source": [
    "The docstring for the `event_triggered_response` function can be viewed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0igYNWQrKcr"
   },
   "outputs": [],
   "source": [
    "mindscope_utilities.event_triggered_response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpGT9s4QP2Lg"
   },
   "source": [
    "## Plot an event triggered response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoKFLU-deYrr"
   },
   "source": [
    "The output format of the `event_triggered_response` function is designed to plug directly into Seaborn's `lineplot` plotting function. We can then view the mean response to omitted stimuli with 95% confidence intervals very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "cyn708IGrZRW",
    "outputId": "126a868f-b446-48f0-d9e4-c33215859340"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=etr,\n",
    "    x='time',\n",
    "    y='dff',\n",
    "    n_boot=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hAabwUXkBvE"
   },
   "source": [
    "Note that the regular, image-driven responses with a 750 ms inter-stimulus interval are visible everywhere except at t=0, which is when the unexpectedly omitted stimulus occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPFH2DffP-VM"
   },
   "source": [
    "### Make a function to plot an event triggered average in one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWeZ7SEYe33O"
   },
   "source": [
    "If we make a wrapper function that combines the process of calculating and plotting the event triggered response, it can be called in a single line below. By having `event_query` input variable, we can use this function to plot responses to any event of interest (omisisons, changes, hits/misses, specific images, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T03BFcu4rglO"
   },
   "outputs": [],
   "source": [
    "def make_event_triggered_plot(df, x, y, event_query, ax, t_before=3, t_after=3):\n",
    "    etr = mindscope_utilities.event_triggered_response(\n",
    "        data = df,\n",
    "        t = 'timestamps',\n",
    "        y = y,\n",
    "        event_times = stimulus_table.query(event_query)['start_time'],\n",
    "        t_before=t_before,\n",
    "        t_after=t_before,\n",
    "        output_sampling_rate = 50,\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        data=etr,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        n_boot=500,\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isIrd2_ZfDol"
   },
   "source": [
    "Now plot the omission triggered response for the same cell using filtered events (these events extracted from the deltaF/F timeseries using an event extraction algorithm, then smoothed with a half-gaussian kernel) instead of dff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "F7K00JKkrwCE",
    "outputId": "c8548305-0109-4baf-b1ab-90cc02601631"
   },
   "outputs": [],
   "source": [
    "cell_id = cell_ids[11]\n",
    "fig, ax = plt.subplots()\n",
    "make_event_triggered_plot(\n",
    "    df = neural_data.query('cell_specimen_id == @cell_id'), \n",
    "    x = 'time', \n",
    "    y = 'filtered_events', \n",
    "    event_query = 'omitted', \n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOe_vyEqfXjq"
   },
   "source": [
    "## Plot the responses for 10 sample cells\n",
    "We can then iterate over 10 randomly chosen cells and plot their activity during omissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "wdals8NxrykH",
    "outputId": "f832f733-2a51-4113-879f-e201373f852b"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "fig, ax = plt.subplots()\n",
    "for cell_id in tqdm(np.random.choice(cell_ids, size=10, replace=False)):\n",
    "    \n",
    "    make_event_triggered_plot(\n",
    "        df = neural_data.query('cell_specimen_id == @cell_id'), \n",
    "        x = 'time', \n",
    "        y = 'dff', \n",
    "        event_query = 'omitted', \n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdjd-yraQOGW"
   },
   "source": [
    "Interestingly, not all SST cells in this session do the same thing! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9eVnLeofiPS"
   },
   "source": [
    "## Calculate the mean response for each of the individual imaging planes in this experiment\n",
    "By iterating over experiment IDs, we can also calculate the mean response for each of the 6 imaging planes. Do Sst cells in different visual areas respond to omissions in a distinct way? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv3WnyQZfzo5"
   },
   "source": [
    "We will first use a Pandas `groupby` and `mean` operations to get the mean timeseries for each cell in that imaging plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VcSMD2-sKUT"
   },
   "outputs": [],
   "source": [
    "mean_dff_by_experiment = (\n",
    "    neural_data\n",
    "    .groupby(['ophys_experiment_id','timestamps'])['dff']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "D6uCg4itr4Fp",
    "outputId": "88cceba7-434b-4533-9296-1a16463b7320"
   },
   "outputs": [],
   "source": [
    "mean_dff_by_experiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe1njJLzgGoM"
   },
   "source": [
    "We can then iterate over our 6 experiment IDs and use our `make_event_triggered_plot` wrapper function to calculate and plot the omission triggered response for that imaging plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "_3xm4pDjsOeI",
    "outputId": "13964820-f494-4395-8d56-2f8e3c88aa74"
   },
   "outputs": [],
   "source": [
    "# set up a new figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make an empty list that we will fill with strings for the legend\n",
    "legend_text = []\n",
    "\n",
    "# iterate over every `ophys_experiment_id`\n",
    "for ophys_experiment_id in tqdm(ophys_experiment_ids):\n",
    "    \n",
    "    make_event_triggered_plot(\n",
    "        df = mean_dff_by_experiment.query('ophys_experiment_id == @ophys_experiment_id'), \n",
    "        x = 'time', \n",
    "        y = 'dff', \n",
    "        event_query = 'omitted', \n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # get some metadata to add to the legend\n",
    "    this_exp = neural_data.query('ophys_experiment_id == @ophys_experiment_id')\n",
    "    structure = this_exp['targeted_structure'].iloc[0]\n",
    "    depth = this_exp['imaging_depth'].iloc[0]\n",
    "\n",
    "    # append a string to our list of legend text\n",
    "    legend_text.append('structure = {}\\ndepth = {} um'.format(structure, depth))\n",
    "    \n",
    "# Put the legend out of the figure\n",
    "plt.legend(legend_text, bbox_to_anchor=(1.05, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6zG8q3Fkk1E"
   },
   "source": [
    "There are clearly some large differences in the way that Sst cells respond to these unexpected stimulus omissions by area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ7fawyvkx4q"
   },
   "source": [
    "This example could be extended to include cells from the other two cre-lines in the dataset: The VIP-Cre line which labels VIP+ inhibitory interneurons and the Slc17a7 line, which is a pan-excitatory line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fow7ST7gkjGv",
    "outputId": "4f0e8a07-bc60-40ee-e608-af01dd73c791"
   },
   "outputs": [],
   "source": [
    "session_table['cre_line'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lky56yhqlRuD"
   },
   "source": [
    "In addition, responses to different stimuli could be explored, along with responses relative to other behavioral measures, such as licking.\n",
    "\n",
    "For a full description of the dataset and all available data streams, see the Visual Behavior Project Description at:\n",
    "https://portal.brain-map.org/explore/circuits/visual-behavior-2p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAA4CrHOv4WM"
   },
   "source": [
    "# Set up data for scikit learn\n",
    "What if we wanted to use scikit-learn for a decoding or clustering analysis? We'd need to get the data into a standard format for scikit learn, which is often a feature matrix (`X`) and a vector of labels (`y`).\n",
    "\n",
    "Instead of just omissions, let's now look at the responses to each of the stimuli in this session, which consists of 8 unique images, plus the omitted stimuli (which we characterize as a unique stimulus type). First, we will calculate an event triggered response to each stimulus start time in the stimulus table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f71hczX5ZplU",
    "outputId": "91b257fc-b6de-4891-d683-2f7139aef0a0"
   },
   "outputs": [],
   "source": [
    "full_etr = []\n",
    "# iterate over each unique cell\n",
    "for cell_specimen_id in tqdm(neural_data['cell_specimen_id'].unique()):\n",
    "    # calculate the event triggered response for this cell to every stimulus\n",
    "    full_etr_this_cell = mindscope_utilities.event_triggered_response(\n",
    "        neural_data.query('cell_specimen_id == @cell_specimen_id'),\n",
    "        t = 'timestamps',\n",
    "        y = 'dff',\n",
    "        event_times = stimulus_table['start_time'],\n",
    "        t_before = 0,\n",
    "        t_after = 0.75,\n",
    "        output_sampling_rate = 30 \n",
    "    )\n",
    "    # add a column identifying the cell_specimen_id\n",
    "    full_etr_this_cell['cell_specimen_id'] = cell_specimen_id\n",
    "    # append to our list\n",
    "    full_etr.append(full_etr_this_cell)\n",
    "\n",
    "# concatenate our list of dataframes into a single dataframe\n",
    "full_etr = pd.concat(full_etr)\n",
    "\n",
    "# cast these numeric columns to int and float, respectively \n",
    "full_etr['event_number'] = full_etr['event_number'].astype(int)\n",
    "full_etr['event_time'] = full_etr['event_number'].astype(float)\n",
    "# rename 'event_number' as \n",
    "full_etr.rename(columns = {'event_number': 'stimulus_presentations_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjX7lHSBy4yN"
   },
   "source": [
    "One way to construct a feature matrix might be to build it such that dimensions are `trials x cells`. Thus:\n",
    "\n",
    "* Each row would be one trial, where a trial is defined as a unique image presentation\n",
    "* Each column would represent the average response of a given cell on that image presentation.\n",
    "\n",
    "To do so, let's construct another intermediate dataframe called `average_responses` that contains the average response of each cell (in the 750 ms window we've selected above) to each image presentation. We'll do this using a Pandas groupby to group by `cell_specimen_id` and `stimulus_presentations_id` (aka trial).\n",
    "\n",
    "We're also going to merge in our stimulus metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Y-x2nh_DzE3W",
    "outputId": "cbe93ea1-bc18-48a1-a92a-dd2f8524fd68"
   },
   "outputs": [],
   "source": [
    "average_responses = full_etr.groupby(['cell_specimen_id','stimulus_presentations_id'])[['dff']].mean().reset_index().merge(\n",
    "    stimulus_table,\n",
    "    on = 'stimulus_presentations_id',\n",
    "    how = 'left'\n",
    ")\n",
    "average_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP9WgFEk04L8"
   },
   "source": [
    "Now we can construct a dataframe called `features_and_labels` that will contain one row per trial, one column per cell, plus columns with the image_index and image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "sgIfhUPdz517",
    "outputId": "ec99929a-753b-4e67-b588-3307664f8165"
   },
   "outputs": [],
   "source": [
    "features_and_labels = average_responses.pivot(\n",
    "    index = 'stimulus_presentations_id',\n",
    "    columns = 'cell_specimen_id',\n",
    "    values = 'dff'\n",
    ").merge(\n",
    "    stimulus_table[['image_index','image_name']],\n",
    "    on = 'stimulus_presentations_id',\n",
    "    how = 'left'\n",
    ")\n",
    "features_and_labels.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqjQf3MJ8BXi"
   },
   "source": [
    "The X matrix can be extracted by getting the columns associated with the cell_specimen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "gZOGAT9E1M8c",
    "outputId": "38293b69-ca52-4bd0-dd71-aa41274067d7"
   },
   "outputs": [],
   "source": [
    "X = features_and_labels[cell_ids]\n",
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59BsKHD68IxV"
   },
   "source": [
    "And `y` is just the `image_name` column (it could also be the `image_index` column if you want a numeric value instead of a string to represent the image identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoYH3JOH1S8d",
    "outputId": "19e0c837-c3bc-48d3-e1b5-9efd2020c1de"
   },
   "outputs": [],
   "source": [
    "y = features_and_labels['image_name']\n",
    "y.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDazypBl4HJr"
   },
   "source": [
    "## Dimensionality reduction\n",
    "Now we can use t-SNE, which will project our 53-dimensional feature space (53 neurons in the session) into two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-98MTVsEwdwg"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHapUJaQ4Tn8"
   },
   "source": [
    "And visualize the results, with colors representing each unique stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "5SJ65rQW1YVH",
    "outputId": "0ed96b7c-6dbb-4049-d1df-4084018b7180"
   },
   "outputs": [],
   "source": [
    "features_and_labels['tsne-2d-one'] = X_embedded[:,0]\n",
    "features_and_labels['tsne-2d-two'] = X_embedded[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = sns.scatterplot(\n",
    "    data=features_and_labels,\n",
    "    x=\"tsne-2d-one\", \n",
    "    y=\"tsne-2d-two\",\n",
    "    hue=\"image_name\",\n",
    "    hue_order = np.sort(features_and_labels['image_name'].unique()),\n",
    "    palette=sns.color_palette()[:9],\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2rpWpxb3vMH"
   },
   "source": [
    "This demonstrates that the time-averaged population responses to at least some of the stimuli seem to fall into distinct clusters in our 53-dimensional space, while others appear more overlapped. This implies that a decoding analysis might be more successful at decoding some stimuli than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdOofIOVD02x"
   },
   "source": [
    "## Train a simple decoder\n",
    "We can use an SVM decoder from scikit learn to ask how well we can decode image identity from the feature matrix we have constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1xNTZq6DCPe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Kk8NtexE7zn"
   },
   "source": [
    "Split our data into train and test sets, instantiate the model, then fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGOZTQAl2fLZ",
    "outputId": "e1197b44-b52e-4325-aea0-9d149da2a319"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model = svm.SVC(probability=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pihBi7OFCz-"
   },
   "source": [
    "Use the model to make predictions on the held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxItJPeHDMGq"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmeRVIW7FGam"
   },
   "source": [
    "Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxTBXfoPC9xC",
    "outputId": "1d3e321d-020f-456f-d9ab-658d6f65044d"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATQYAcM4FIVQ"
   },
   "source": [
    "Evaluate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "K2o1IESEDW7n",
    "outputId": "d23eb631-fb1d-451a-af7b-4256e0b9db53"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred), \n",
    "    columns = ['predicted_{}'.format(im) for im in model.classes_],\n",
    "    index = ['actual_{}'.format(im) for im in model.classes_]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWygkKjoGOE6"
   },
   "source": [
    "This tells us that the model can decode some stimuli well (im035, im075 and im106, for example), while it struggles more with others (im000 and omissions, for example). Do the stimuli that the decoder succeeds in classifying align with those that cluster cleanly in t-SNE space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6WyMYt5Rp8e"
   },
   "source": [
    "### Follow up exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B38C_hbHRiF6"
   },
   "source": [
    "Can you create event triggered averages and perform decoding using other events of interest, such as licks or rewards? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rCv5DhXFRpF1",
    "outputId": "de0b4c0c-91d2-405b-dbf9-913ef568b3ce"
   },
   "outputs": [],
   "source": [
    "# Lick and reward data are available for each experiment\n",
    "licks = experiments[ophys_experiment_id].licks\n",
    "licks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "CnJ_dNWtRzkZ",
    "outputId": "bc1b9510-bcd3-4427-83bf-a855a5a03c51"
   },
   "outputs": [],
   "source": [
    "rewards = experiments[ophys_experiment_id].rewards\n",
    "rewards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jcXz7yNS0on"
   },
   "source": [
    "To see the full list of all attributes available for each experiment via the AllenSDK, uncomment the cell below and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7EmbvAiSNwU"
   },
   "outputs": [],
   "source": [
    "# help(experiments[ophys_experiment_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uujECFhSllz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Allen Visual Behavior from SDK.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mindscope_utilities",
   "language": "python",
   "name": "mindscope_utilities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
