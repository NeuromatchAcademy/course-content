{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_hcp_task_with_behaviour.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxn5tK5icpbe"
      },
      "source": [
        "# Load HCP parcellated task data \n",
        "# **(VERSION WITH BEHAVIOURAL DATA)**\n",
        "\n",
        "The HCP dataset comprises task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest.\n",
        "\n",
        "In order to use this dataset, please electronically sign the HCP data use terms at [ConnectomeDB](https://db.humanconnectome.org). Instructions for this are on pp. 24-25 of the [HCP Reference Manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf).\n",
        "\n",
        "In this notebook, NMA provides code for downloading the data and doing some basic visualisation and processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXasV6tWdJls"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "R5gwQwxPdLst"
      },
      "source": [
        "#@title Figure settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtTptKa7dZFW"
      },
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./hcp\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "\n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 100\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in seconds\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated twice in each subject\n",
        "RUNS   = ['LR','RL']\n",
        "N_RUNS = 2\n",
        "\n",
        "# There are 7 tasks. Each has a number of 'conditions'\n",
        "# TIP: look inside the data folders for more fine-graned conditions\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AiC1xJnnS5g"
      },
      "source": [
        "> For a detailed description of the tasks have a look pages 45-54 of the [HCP reference manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsD9_4LVeDSW"
      },
      "source": [
        "# Downloading data\n",
        "\n",
        "The task data are shared in different files, but they will unpack into the same directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZwgMo5Id_ZG"
      },
      "source": [
        "fname = \"hcp_task.tgz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/2y3fw/download\n",
        "  !tar -xzf $fname -C $HCP_DIR --strip-components=1\n",
        "\n",
        "\n",
        "subjects = np.loadtxt(os.path.join(HCP_DIR,'subjects_list.txt'),dtype='str')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ0ACENEwXmk"
      },
      "source": [
        "## Understanding the folder organisation\n",
        "\n",
        "The data folder has the following organisation:\n",
        "\n",
        "- hcp\n",
        "  - regions.npy (information on the brain parcellation)\n",
        "  - subjects_list.txt (list of subject IDs)\n",
        "  - subjects (main data folder)\n",
        "    - [subjectID] (subject-specific subfolder)\n",
        "      - EXPERIMENT (one folder per experiment)\n",
        "        - RUN (one folder per run)\n",
        "          - data.npy (the parcellated time series data)\n",
        "          - EVs (EVs folder)\n",
        "            - [ev1.txt] (one file per condition)\n",
        "            - [ev2.txt]\n",
        "            - Stats.txt (behavioural data [where available] - averaged per run)\n",
        "            - Sync.txt (ignore this file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8b5QWPpeSKp"
      },
      "source": [
        "## Loading region information\n",
        "\n",
        "Downloading this dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
        "\n",
        "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
        "\n",
        "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePPgyLQWeOj2"
      },
      "source": [
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6lb8rQdey6_"
      },
      "source": [
        "# Help functions\n",
        "\n",
        "We provide two helper functions: one for loading the time series from a single suject and a single run, and one for loading an EV file for each task. \n",
        "\n",
        "An EV file (EV:Explanatory Variable) describes the task experiment in terms of stimulus onset, duration, and amplitude. These can be used to model the task time series data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3b1Z3x1ebkt"
      },
      "source": [
        "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "  \n",
        "  Args:\n",
        "    subject (str):      subject ID to load\n",
        "    experiment (str):   Name of experiment \n",
        "    run (int):          (0 or 1)\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_run  = RUNS[run]\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
        "  bold_file = \"data.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "\n",
        "def load_evs(subject, experiment, run):\n",
        "  \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
        "\n",
        "  Args:\n",
        "    subject (str): subject ID to load\n",
        "    experiment (str) : Name of experiment\n",
        "    run (int): 0 or 1\n",
        "\n",
        "  Returns\n",
        "    evs (list of lists): A list of frames associated with each condition\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  task_key = f'tfMRI_{experiment}_{RUNS[run]}'\n",
        "  for cond in EXPERIMENTS[experiment]['cond']:    \n",
        "    ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
        "    frames_list.append(frames)\n",
        "\n",
        "  return frames_list\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2xGUdrAmuym"
      },
      "source": [
        "# Example run\n",
        "\n",
        "Let's load the timeseries data for the MOTOR experiment from a single subject and a single run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7vk9y1GfwcW",
        "outputId": "56ffd0cd-4e4f-4db6-ae15-dec8c09b1b11"
      },
      "source": [
        "my_exp  = 'MOTOR'\n",
        "my_subj = subjects[1]\n",
        "my_run  = 1\n",
        "\n",
        "data = load_single_timeseries(subject=my_subj,experiment=my_exp,run=my_run,remove_mean=True)\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(360, 284)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U93-Pqllm7VU"
      },
      "source": [
        "As you can see the time series data contains 284 time points in 360 regions of interest (ROIs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVBiRQfVnqfY"
      },
      "source": [
        "Now in order to understand how to model these data, we need to relate the time series to the experimental manipulation. This is described by the EV files. Let us load the EVs for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5bQo6IemfdI"
      },
      "source": [
        "evs = load_evs(subject=my_subj, experiment=my_exp,run=my_run)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKrhg4_Ervwz"
      },
      "source": [
        "For the motor task, this evs variable contains a list of 5 arrays corresponding to the 5 conditions. \n",
        "\n",
        "Now let's use these evs to compare the average activity during the left foot ('lf') and right foot ('rf') conditions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhLIpwe5miNX"
      },
      "source": [
        "# we need a little function that averages all frames from any given condition\n",
        "\n",
        "def average_frames(data, evs, experiment, cond):    \n",
        "    idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
        "    return np.mean(np.concatenate([np.mean(data[:,evs[idx][i]],axis=1,keepdims=True) for i in range(len(evs[idx]))],axis=-1),axis=1)\n",
        "\n",
        "\n",
        "lf_activity = average_frames(data, evs, my_exp, 'lf')\n",
        "rf_activity = average_frames(data, evs, my_exp, 'rf')\n",
        "contrast    = lf_activity-rf_activity   # difference between left and right hand movement"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ0StH_kACUw"
      },
      "source": [
        "# Plot activity level in each ROI for both conditions\n",
        "plt.plot(lf_activity,label='left foot')\n",
        "plt.plot(rf_activity,label='right foot')\n",
        "plt.xlabel('ROI')\n",
        "plt.ylabel('activity')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ERPIp7tsX-C"
      },
      "source": [
        "Now let's plot these activity vectors. We will also make use of the ROI names to find out which brain areas show highest activity in these conditions. But since there are so many areas, we will group them by network.\n",
        "\n",
        "A powerful tool for organising and plotting this data is the combination of pandas and seaborn. Below is an example where we use pandas to create a table for the activity data and we use seaborn oto visualise it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og0F_4dkrsHO"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.DataFrame({'lf_activity' : lf_activity,\n",
        "                   'rf_activity' : rf_activity,\n",
        "                   'network'     : region_info['network'],\n",
        "                   'hemi'        : region_info['hemi']})\n",
        "\n",
        "fig,(ax1,ax2) = plt.subplots(1,2)\n",
        "sns.barplot(y='network', x='lf_activity', data=df, hue='hemi',ax=ax1)\n",
        "sns.barplot(y='network', x='rf_activity', data=df, hue='hemi',ax=ax2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWzklttG9zp5"
      },
      "source": [
        "You should be able to notice that for the somatosensory network, brain activity in the right hemi is higher for the left foot movement and vice versa for the left hemi and right foot. But this may be subtle at the single subject/session level (these are quick 3-4min scans). \n",
        "\n",
        "\n",
        "Let us boost thee stats by averaging across all subjects and runs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-6MfK_KsVgI"
      },
      "source": [
        "group_contrast = 0\n",
        "for s in subjects:\n",
        "  for r in [0,1]:\n",
        "    data = load_single_timeseries(subject=s,experiment=my_exp,run=r,remove_mean=True)\n",
        "    evs = load_evs(subject=s, experiment=my_exp,run=r)\n",
        "\n",
        "    lf_activity = average_frames(data, evs, my_exp, 'lf')\n",
        "    rf_activity = average_frames(data, evs, my_exp, 'rf')\n",
        "\n",
        "    contrast    = lf_activity-rf_activity\n",
        "    group_contrast        += contrast\n",
        "\n",
        "group_contrast /= (len(subjects)*2)  # remember: 2 sessions per subject\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFD2BpxWsmwC"
      },
      "source": [
        "\n",
        "df = pd.DataFrame({'contrast':group_contrast,'network':region_info['network'],'hemi':region_info['hemi']})\n",
        "# we will plot the left foot minus right foot contrast so we only need one plot\n",
        "sns.barplot(y='network', x='contrast', data=df, hue='hemi')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k8wuJmIKiCS"
      },
      "source": [
        "# Visualising the results on a brain\n",
        "\n",
        "Finally, we will visualise these resuts on the cortical surface of an average brain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o6dc0hqwX4n"
      },
      "source": [
        "\n",
        "# This uses the nilearn package\n",
        "!pip install nilearn --quiet\n",
        "from nilearn import plotting, datasets\n",
        "\n",
        "# NMA provides an atlas \n",
        "fname = f\"{HCP_DIR}/atlas.npz\"\n",
        "if not os.path.exists(fname):\n",
        "  !wget -qO $fname https://osf.io/j5kuc/download\n",
        "with np.load(fname) as dobj:\n",
        "  atlas = dict(**dobj)\n",
        "\n",
        "# Try both hemispheres (L->R and left->right)\n",
        "fsaverage = datasets.fetch_surf_fsaverage()\n",
        "surf_contrast = group_contrast[atlas[\"labels_L\"]]\n",
        "plotting.view_surf(fsaverage['infl_left'],\n",
        "                   surf_contrast,\n",
        "                   vmax=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc_LTG8xMDBI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}