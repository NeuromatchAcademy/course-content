{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CalMS21 Introduction and example training code",
      "provenance": [],
      "collapsed_sections": [
        "jsaWsU8dILfN",
        "YFcS9p3haBkK",
        "zZYfbeJIrNt9",
        "nPfKXGb1vTRd",
        "aEZqmoHJJl4j"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4w4yZOfV5le"
      },
      "source": [
        "<h1>\n",
        "Behavior classification starter kit üêÅüêÄ\n",
        "</h1>\n",
        "This code is adapted from a notebook created by Dipam Chakraborty at AIcrowd for the <a href=https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications>Multi-Agent Behavior Challenge</a>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsaWsU8dILfN"
      },
      "source": [
        "# Import necessary modules and packages üìö\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4CVVoCjIN95"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwykJ9kzvTRZ"
      },
      "source": [
        "# Download the dataset üì≤\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "The CalMS21 dataset is hosted by Caltech at https://data.caltech.edu/records/1991. For now, we'll focus on the Task 1 data, which can be downloaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eua-L3A_okrf"
      },
      "source": [
        "urllib.request.urlretrieve('https://data.caltech.edu/tindfiles/serve/a86f4297-a087-4f40-9ed4-765779105c2c/', 'task1.zip')\n",
        "urllib.request.urlretrieve('https://data.caltech.edu/tindfiles/serve/ca84a583-ea06-440a-995c-c184bcb0291c/', 'calms21_convert_to_npy.py')\n",
        "!unzip task1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MLxouZUvTRa"
      },
      "source": [
        "The dataset files are stored as json files. For ease of handling, we'll first convert them to .npy files using the script we just downloaded, `calms21_convert_to_npy.py`. The output of this script is a pair of files, `calms21_task1_train.npy` and `calms21_task1_test.npy`.\n",
        "\n",
        "If you include the optional `parse_treba` flag, the script will create files `calms21_task1_train_features.npy` and `calms21_task1_test_features.npy`, which contain 32 features created using <a href=https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Task_Programming_Learning_Data_Efficient_Behavior_Representations_CVPR_2021_paper.html>Task Programming</a>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IezJP7janpBy"
      },
      "source": [
        "!python calms21_convert_to_npy.py  --input_directory '.' --output_directory '.'\n",
        "!python calms21_convert_to_npy.py  --input_directory '.' --output_directory '.' --parse_treba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjooSFrVoIxz"
      },
      "source": [
        "#Load the data üíæ\n",
        "The following loader function can be used to unpack the `.npy` files containing your train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RopVoFl1vTRb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_task1_data(data_path):\n",
        "    \"\"\" \n",
        "    Load data for task 1:\n",
        "        The vocaubulary tells you how to map behavior names to class ids;\n",
        "        it is the same for all sequences in this dataset.\n",
        "    \"\"\"\n",
        "    data_dict = np.load(data_path, allow_pickle=True).item()\n",
        "    dataset = data_dict['annotator-id_0']\n",
        "    # Get any sequence key.\n",
        "    sequence_id = list(data_dict['annotator-id_0'].keys())[0]\n",
        "    vocabulary = data_dict['annotator-id_0'][sequence_id]['metadata']['vocab']\n",
        "    return dataset, vocabulary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB5RakCHomqC"
      },
      "source": [
        "training_data, vocab = load_task1_data('./calms21_task1_train.npy')\n",
        "test_data, _ = load_task1_data('./calms21_task1_test.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcS9p3haBkK"
      },
      "source": [
        "## Dataset Specifications\n",
        "\n",
        "`training_data` and `test_data` are both dictionaries with a key for each Sequence in the dataset, where a Sequence is a single resident-intruder assay. Each Sequence contains the following fields:\n",
        "\n",
        "<ul>\n",
        "<li><b>keypoints</b>: tracked locations of body parts on the two interacting mice. These are produced using a Stacked Hourglass network trained on 15,000 hand-labeled frames.\n",
        "<ul>\n",
        "<li>Dimensions: (# frames) x (mouse ID) x (x, y coordinate) x (body part).\n",
        "<li>Units: pixels; coordinates are relative to the entire image. Original image dimensions are 1024 x 570.\n",
        "</ul>\n",
        "<li><b>scores</b>: confidence estimates for the tracked keypoints.\n",
        "<ul>\n",
        "<li>Dimensions: (# frames) x (mouse ID) x (body part).\n",
        "<li>Units: unitless, range 0 (lowest confidence) to 1 (highest confidence).\n",
        "</ul>\n",
        "<li> <b>annotations</b>: behaviors id as an integer annotated at each frame by a domain expert. See below for the behavior id to behavior name mappings.\n",
        "<ul>\n",
        "<li>Dimensions: (# frames) .\n",
        "</ul>\n",
        "<li><b>metadata</b>: The recorded metadata is annotator_id which is represented by an int, and the vocab, containing a dictionary which maps behavior names to integer ids in annotations.\n",
        "</ul>\n",
        "\n",
        "The 'taskprog_features' file contains the additional field:\n",
        "\n",
        "<ul>\n",
        "<li><b>features</b>: pre-computed features from a model trained with task programming on the trajectory data of the CalMS21 unlabeled videos set.\n",
        "<ul>\n",
        "<li>Dimensions: (# frames) x (feature dim = 32).\n",
        "</li>\n",
        "</ul>\n",
        "</ul>\n",
        "\n",
        "<b>NOTE:</b> for all keypoints, mouse 0 is the resident (black) mouse and mouse 1 is the intruder (white) mouse. There are 7 tracked body parts, ordered (nose, left ear, right ear, neck, left hip, right hip, tail base)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUkb35tcf4bn"
      },
      "source": [
        "## What does the data look like? üîç\n",
        "\n",
        "### Data overview\n",
        "\n",
        "As described above, our dataset consists of train and test sets, which are both dictionaries of Sequences, and an accompanying vocabulary telling us which behavior is which:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYIC5DW_oRPb"
      },
      "source": [
        "print(\"Sample dataset keys: \", list(training_data.keys())[:3])\n",
        "print(\"Vocabulary: \", vocab)\n",
        "print(\"Number of train Sequences: \", len(training_data))\n",
        "print(\"Number of test Sequences: \", len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luUly4Y1i4L-"
      },
      "source": [
        "### Sample overview\n",
        "Next let's take a look at one example Sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh9dj4wG3rV0"
      },
      "source": [
        "sequence_names = list(training_data.keys())\n",
        "sample_sequence_key = sequence_names[0]\n",
        "single_sequence = training_data[sample_sequence_key]\n",
        "print(\"Name of our sample sequence: \", sample_sequence_key)\n",
        "print(\"Sequence keys: \", single_sequence.keys())\n",
        "print(\"Sequence metadata: \", single_sequence['metadata'])\n",
        "print(f\"Number of Frames in Sequence \\\"{sample_sequence_key}\\\": \", len(single_sequence['annotations']))\n",
        "print(f\"Keypoints data shape of Sequence \\\"{sample_sequence_key}\\\": \", single_sequence['keypoints'].shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGK1pjV81oc"
      },
      "source": [
        "# Helper functions for visualization üíÅ\n",
        "\n",
        "\n",
        "This cell contains some helper functions that we'll use to create an animation of the mouse movements. You can ignore the contents, but be sure to run it or the next section won't work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf7CXiH85odZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from matplotlib import colors\n",
        "from matplotlib import rc\n",
        " \n",
        "rc('animation', html='jshtml')\n",
        " \n",
        "# Note: Image processing may be slow if too many frames are animated.                \n",
        " \n",
        "#Plotting constants\n",
        "FRAME_WIDTH_TOP = 1024\n",
        "FRAME_HEIGHT_TOP = 570\n",
        " \n",
        "RESIDENT_COLOR = 'lawngreen'\n",
        "INTRUDER_COLOR = 'skyblue'\n",
        " \n",
        "PLOT_MOUSE_START_END = [(0, 1), (0, 2), (1, 3), (2, 3), (3, 4),\n",
        "                        (3, 5), (4, 6), (5, 6), (1, 2)]\n",
        " \n",
        "class_to_color = {'other': 'white', 'attack' : 'red', 'mount' : 'green',\n",
        "                  'investigation': 'orange'}\n",
        " \n",
        "class_to_number = {s: i for i, s in enumerate(vocab)}\n",
        " \n",
        "number_to_class = {i: s for i, s in enumerate(vocab)}\n",
        " \n",
        "def num_to_text(anno_list):\n",
        "  return np.vectorize(number_to_class.get)(anno_list)\n",
        " \n",
        "def set_figax():\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        " \n",
        "    img = np.zeros((FRAME_HEIGHT_TOP, FRAME_WIDTH_TOP, 3))\n",
        " \n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.imshow(img)\n",
        " \n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        " \n",
        "    return fig, ax\n",
        " \n",
        "def plot_mouse(ax, pose, color):\n",
        "    # Draw each keypoint\n",
        "    for j in range(7):\n",
        "        ax.plot(pose[j, 0], pose[j, 1], 'o', color=color, markersize=5)\n",
        " \n",
        "    # Draw a line for each point pair to form the shape of the mouse\n",
        " \n",
        "    for pair in PLOT_MOUSE_START_END:\n",
        "        line_to_plot = pose[pair, :]\n",
        "        ax.plot(line_to_plot[:, 0], line_to_plot[\n",
        "                :, 1], color=color, linewidth=1)\n",
        " \n",
        "def animate_pose_sequence(video_name, keypoint_sequence, start_frame = 0, stop_frame = 100, \n",
        "                          annotation_sequence = None):\n",
        "    # Returns the animation of the keypoint sequence between start frame\n",
        "    # and stop frame. Optionally can display annotations.\n",
        "    seq = keypoint_sequence.transpose((0,1,3,2))\n",
        " \n",
        "    image_list = []\n",
        "    \n",
        "    counter = 0\n",
        "    for j in range(start_frame, stop_frame):\n",
        "        if counter%20 == 0:\n",
        "          print(\"Processing frame \", j)\n",
        "        fig, ax = set_figax()\n",
        "        plot_mouse(ax, seq[j, 0, :, :], color=RESIDENT_COLOR)\n",
        "        plot_mouse(ax, seq[j, 1, :, :], color=INTRUDER_COLOR)\n",
        "        \n",
        "        if annotation_sequence is not None:\n",
        "          annot = annotation_sequence[j]\n",
        "          annot = number_to_class[annot]\n",
        "          plt.text(50, -20, annot, fontsize = 16, \n",
        "                   bbox=dict(facecolor=class_to_color[annot], alpha=0.5))\n",
        " \n",
        "        ax.set_title(\n",
        "            video_name + '\\n frame {:03d}.png'.format(j))\n",
        " \n",
        "        ax.axis('off')\n",
        "        fig.tight_layout(pad=0)\n",
        "        ax.margins(0)\n",
        " \n",
        "        fig.canvas.draw()\n",
        "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(),\n",
        "                                        dtype=np.uint8)\n",
        "        image_from_plot = image_from_plot.reshape(\n",
        "            fig.canvas.get_width_height()[::-1] + (3,)) \n",
        " \n",
        "        image_list.append(image_from_plot)\n",
        " \n",
        "        plt.close()\n",
        "        counter = counter + 1\n",
        " \n",
        "    # Plot animation.\n",
        "    fig = plt.figure()\n",
        "    plt.axis('off')\n",
        "    im = plt.imshow(image_list[0])\n",
        " \n",
        "    def animate(k):\n",
        "        im.set_array(image_list[k])\n",
        "        return im,\n",
        "    ani = animation.FuncAnimation(fig, animate, frames=len(image_list), blit=True)\n",
        "    return ani\n",
        " \n",
        "def plot_behavior_raster(annotation_sequence, start_frame = 0, stop_frame = 100, title=\"Behavior Labels\"):\n",
        "  # Plot annotations as a behavior raster\n",
        " \n",
        "  # Map annotations to a number.\n",
        "  annotation_num = []\n",
        "  for item in annotation_sequence[start_frame:stop_frame]:\n",
        "    annotation_num.append(class_to_number[item])\n",
        " \n",
        "  all_classes = list(set(annotation_sequence[start_frame:stop_frame]))\n",
        " \n",
        "  cmap = colors.ListedColormap(['red', 'orange', 'green', 'white'])\n",
        "  bounds=[-0.5,0.5,1.5, 2.5, 3.5]\n",
        "  norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        " \n",
        "  height = 200\n",
        "  arr_to_plot = np.repeat(np.array(annotation_num)[:,np.newaxis].transpose(),\n",
        "                                                  height, axis = 0)\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize = (16, 3))\n",
        "  ax.imshow(arr_to_plot, interpolation = 'none',cmap=cmap, norm=norm)\n",
        " \n",
        "  ax.set_yticks([])\n",
        "  ax.set_xlabel('Frame Number')\n",
        "  plt.title(title)\n",
        " \n",
        "  import matplotlib.patches as mpatches\n",
        " \n",
        "  legend_patches = []\n",
        "  for item in all_classes:\n",
        "    legend_patches.append(mpatches.Patch(color=class_to_color[item], label=item))\n",
        " \n",
        "  plt.legend(handles=legend_patches,loc='center left', bbox_to_anchor=(1, 0.5))\n",
        " \n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7WPezuV86QA"
      },
      "source": [
        "# Visualize the animals' movements üé•\n",
        "\n",
        "Let's make some gifs of our sample sequence to get a sense of what the raw data looks like! You can change the values of `start_frame` and `stop_frame` to look around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSUZW-9J53kI"
      },
      "source": [
        "#@title\n",
        "keypoint_sequence = single_sequence['keypoints']\n",
        "annotation_sequence = single_sequence['annotations']\n",
        "\n",
        "ani = animate_pose_sequence(sample_sequence_key,\n",
        "                            keypoint_sequence, \n",
        "                            start_frame = 5000,\n",
        "                            stop_frame = 5100,\n",
        "                            annotation_sequence = annotation_sequence)\n",
        "\n",
        "# Display the animaion on colab\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jhj4BtOkjM5"
      },
      "source": [
        "### We can also look at a **behavior raster**, which shows what behavior was annotated on each frame of this video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u28cZOL8G29"
      },
      "source": [
        "annotation_sequence = single_sequence['annotations']\n",
        "text_sequence = num_to_text(annotation_sequence)\n",
        " \n",
        "plot_behavior_raster(\n",
        "    text_sequence,\n",
        "    start_frame=0,\n",
        "    stop_frame=len(annotation_sequence)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOs2KLHe9Ip1"
      },
      "source": [
        "# Basic exploratory data analysis ü§ì\n",
        "Each Sequence has different amounts of each behavior, depending on what the mice do during the assay. Here, we get the percentage of frames of each behavior in each sequence. We can use this to split the training set into train and validation sets in a stratified way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoqaPWYM9gdi"
      },
      "source": [
        "def get_percentage(sequence_key):\n",
        "  anno_seq = num_to_text(training_data[sequence_key]['annotations'])\n",
        "  counts = {k: np.mean(np.array(anno_seq) == k)*100.0 for k in vocab}\n",
        "  return counts\n",
        "\n",
        "anno_percentages = {k: get_percentage(k) for k in training_data}\n",
        "\n",
        "anno_perc_df = pd.DataFrame(anno_percentages).T\n",
        "print(\"Percentage of frames in every sequence for every class\")\n",
        "anno_perc_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcGC3YGTU_9X"
      },
      "source": [
        "## Percent of frames of each behavior in the full training set\n",
        "Having looked at behavior distributions in a couple example Sequences, let's now look at the average over the entire training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HFOPsoF_LAy"
      },
      "source": [
        "all_annotations = []\n",
        "for sk in training_data:\n",
        "  anno = training_data[sk]['annotations']\n",
        "  all_annotations.extend(list(anno))\n",
        "all_annotations = num_to_text(all_annotations)\n",
        "classes, counts = np.unique(all_annotations, return_counts=True)\n",
        "pd.DataFrame({\"Behavior\": classes,\n",
        "              \"Percentage Frames\": counts/len(all_annotations)*100.0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgxwPih4uUS4"
      },
      "source": [
        "# Split training data into train/validation sets\n",
        "Because we don't want to overfit to our test set, we'll create a new validation set to test on while we're experimenting with our model.\n",
        "\n",
        "We'll use the first cell to create some helper functions, and then implement the split in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMVVpWZud-_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def num_to_text(number_to_class, anno_list):\n",
        "    \"\"\" \n",
        "    Convert list of class numbers to list of class names\n",
        "    \"\"\"\n",
        "    return np.vectorize(number_to_class.get)(anno_list)\n",
        "\n",
        "\n",
        "def split_validation(orig_pose_dictionary, vocabulary, seed=2021,\n",
        "               test_size=0.5, split_videos=False):\n",
        "    \"\"\" \n",
        "    Split data into train and validation sets:\n",
        "    * Full sequences are either put into train or validation to avoid data leakage\n",
        "    * By default, the \"attack\" behavior's presence is used to stratify the split\n",
        "    * Optionally, the sequences may be split into half and treated as separate sequences\n",
        "    \"\"\"\n",
        "\n",
        "    if test_size == 0.0:\n",
        "        return orig_pose_dictionary, None\n",
        "\n",
        "    number_to_class = {v: k for k, v in vocabulary.items()}\n",
        "    if split_videos:\n",
        "        pose_dictionary = {}\n",
        "        for key in orig_pose_dictionary:\n",
        "            key_pt1 = key + '_part1'\n",
        "            key_pt2 = key + '_part2'\n",
        "            anno_len = len(orig_pose_dictionary[key]['annotations'])\n",
        "            split_idx = anno_len//2\n",
        "            pose_dictionary[key_pt1] = {\n",
        "                'annotations': orig_pose_dictionary[key]['annotations'][:split_idx],\n",
        "                'keypoints': orig_pose_dictionary[key]['keypoints'][:split_idx]}\n",
        "            pose_dictionary[key_pt2] = {\n",
        "                'annotations': orig_pose_dictionary[key]['annotations'][split_idx:],\n",
        "                'keypoints': orig_pose_dictionary[key]['keypoints'][split_idx:]}\n",
        "    else:\n",
        "        pose_dictionary = orig_pose_dictionary\n",
        "\n",
        "    def get_percentage(sequence_key):\n",
        "        anno_seq = num_to_text(\n",
        "            number_to_class, pose_dictionary[sequence_key]['annotations'])\n",
        "        counts = {k: np.mean(np.array(anno_seq) == k) for k in vocabulary}\n",
        "        return counts\n",
        "\n",
        "    anno_percentages = {k: get_percentage(k) for k in pose_dictionary}\n",
        "\n",
        "    anno_perc_df = pd.DataFrame(anno_percentages).T\n",
        "\n",
        "    rng_state = np.random.RandomState(seed)\n",
        "    try:\n",
        "        idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
        "                                              stratify=anno_perc_df['attack'] > 0,\n",
        "                                              test_size=test_size,\n",
        "                                              random_state=rng_state)\n",
        "    except:\n",
        "        idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
        "                                              test_size=test_size,\n",
        "                                              random_state=rng_state)\n",
        "\n",
        "    train_data = {k: pose_dictionary[k] for k in idx_train}\n",
        "    val_data = {k: pose_dictionary[k] for k in idx_val}\n",
        "    return train_data, val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUNQAXsdu1Ni"
      },
      "source": [
        "train, val = split_validation(training_data, vocab, test_size=0.25)\n",
        "print(\"Number of Sequences in train set: \", len(train))\n",
        "print(\"Number of Sequences in validation set: \", len(val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_f_AS5X6RTK"
      },
      "source": [
        "# Training and testing a baseline model implemented in Tensorflow üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "\n",
        "The CalMS21 dataset is accompanied by a set of baseline models, implemented in Tensorflow. These can be found with accompanying documentation at <a href=https://gitlab.aicrowd.com/aicrowd/research/mab-e/mab-e-baselines>https://gitlab.aicrowd.com/aicrowd/research/mab-e/mab-e-baselines</a>.\n",
        "\n",
        "The baseline model is a simple neural network that takes as input a \"Trajectory\" - a short snippet of a sequence showing what the mice are doing before and after the \"current\" frame (the frame on which behavior is to be classified.) We tested multiple simple baseline architectures in the CalMS21 paper, found at <a href=https://arxiv.org/abs/2104.02710>https://arxiv.org/abs/2104.02710</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R7GBwyI7KW3"
      },
      "source": [
        "! git clone http://gitlab.aicrowd.com/aicrowd/research/mab-e/mab-e-baselines.git\n",
        "%cd mab-e-baselines\n",
        "! pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLxIf7aQ7iIv"
      },
      "source": [
        "!cp ../calms21_task1_train.npy data/train.npy\n",
        "!cp ../calms21_task1_test.npy data/test.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2pcLQF0_48q"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import tqdm\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRekQr5l_cnb"
      },
      "source": [
        "## Seeding helper\n",
        "Its good practice to seed before every run, so you can reproduce your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIlrKqEO_Y6d"
      },
      "source": [
        "def seed_everything(seed):\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "seed=2021\n",
        "seed_everything(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkG5mu9J9F-R"
      },
      "source": [
        "## Generator üîå\n",
        "\n",
        "The generator is used to feed data into our models. It randomly samples frames from the Sequences in our training set, and for each frame takes the animals' poses in a small window in time around that frame (we call these snippets Trajectories.)\n",
        "\n",
        "It also provides support for data augmentation:\n",
        "1.   Random rotation\n",
        "2.   Random translation\n",
        "\n",
        "üöß Note that the same augmentation is applied across all frames in a selected window, e.g - Random rotation by 10 degrees will rotate all frames in the input window by the same angle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_uAY5Oy9Bf2"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_input_dim(feature_dim, architechture, past_frames, future_frames):\n",
        "    \"\"\"\n",
        "    Data is arranged as [t, flattened_feature_dimensions]\n",
        "           where t => [past_frames + 1 + future_frames]\n",
        "\n",
        "    In this version, we flatten the feature dimensions\n",
        "    But another generator, inherited from this class,\n",
        "    could very well retain the actual structure of the mice\n",
        "    coordinates.\n",
        "    \"\"\"\n",
        "    flat_dim = np.prod(feature_dim)\n",
        "    if architechture != 'fully_connected':\n",
        "        input_dim = ((past_frames + future_frames + 1), flat_dim,)\n",
        "    else:\n",
        "        input_dim = (flat_dim * (past_frames + future_frames + 1),)\n",
        "    return input_dim\n",
        "\n",
        "\n",
        "def mabe_generator(data, augment, shuffle, sequence_key, kwargs):\n",
        "    if data is not None:\n",
        "        return MABe_Data_Generator(data,\n",
        "                               augment=augment,\n",
        "                               shuffle=shuffle,\n",
        "                               sequence_key=sequence_key,\n",
        "                               **kwargs)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "class MABe_Data_Generator(keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Generates window of frames from sequence data\n",
        "    * Each window comprises of past and future frames\n",
        "    * Frame skip > 1 can be used to increased for subsampling\n",
        "    * Augments by rotation and shifting frames\n",
        "    * Boundaries are padded with zeros for when window exceeds the limits\n",
        "    \"\"\"\n",
        "    def __init__(self,  pose_dict,\n",
        "                 class_to_number,\n",
        "                 batch_size=2,\n",
        "                 input_dimensions=(2, 2, 7),\n",
        "                 augment=False,\n",
        "                 past_frames=100,\n",
        "                 future_frames=100,\n",
        "                 frame_skip=1,\n",
        "                 shuffle=True,\n",
        "                 sequence_key = 'keypoints'):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = input_dimensions\n",
        "\n",
        "        self.classname_to_index_map = class_to_number\n",
        "        self.n_classes = len(self.classname_to_index_map)\n",
        "\n",
        "        self.past_frames = past_frames\n",
        "        self.future_frames = future_frames\n",
        "        self.frame_skip = frame_skip\n",
        "\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "\n",
        "        self.sequence_key = sequence_key\n",
        "\n",
        "        # Raw Data Containers\n",
        "        self.X = {}\n",
        "        self.y = []\n",
        "\n",
        "        # Setup Dimensions of data points\n",
        "        # self.setup_dimensions()\n",
        "\n",
        "        # Load raw pose dictionary\n",
        "        self.load_pose_dictionary(pose_dict)\n",
        "\n",
        "        # Setup Utilities\n",
        "        self.setup_utils()\n",
        "\n",
        "        # Generate a global index of all datapoints\n",
        "        self.generate_global_index()\n",
        "\n",
        "        # Epoch End preparations\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def load_pose_dictionary(self, pose_dict):\n",
        "        \"\"\" Load raw pose dictionary \"\"\"\n",
        "        self.pose_dict = pose_dict\n",
        "        self.video_keys = list(pose_dict.keys())\n",
        "\n",
        "    def setup_utils(self):\n",
        "        \"\"\" Set up padding utilities \"\"\"\n",
        "        self.setup_padding_utils()\n",
        "\n",
        "    def setup_padding_utils(self):\n",
        "        \"\"\" Prepare to pad frames \"\"\"\n",
        "        self.left_pad = self.past_frames * self.frame_skip\n",
        "        self.right_pad = self.future_frames * self.frame_skip\n",
        "\n",
        "        if self.sequence_key == 'keypoints':\n",
        "            self.pad_width = (self.left_pad, self.right_pad), (0, 0), (0, 0), (0, 0)\n",
        "        else:\n",
        "            self.pad_width = (self.left_pad, self.right_pad), (0, 0)\n",
        "\n",
        "    def classname_to_index(self, annotations_list):\n",
        "        \"\"\"\n",
        "        Converts a list of string classnames into numeric indices\n",
        "        \"\"\"\n",
        "        return np.vectorize(self.classname_to_index_map.get)(annotations_list)\n",
        "\n",
        "    def generate_global_index(self):\n",
        "        \"\"\" Define arrays to map video keys to frames \"\"\"\n",
        "        self.video_indices = []\n",
        "        self.frame_indices = []\n",
        "\n",
        "        self.action_annotations = []\n",
        "\n",
        "        # For all video keys....\n",
        "        for video_index, video_key in enumerate(self.video_keys):\n",
        "            # Extract all annotations\n",
        "            annotations = self.pose_dict[video_key]['annotations']\n",
        "            # add annotations to action_annotations\n",
        "            self.action_annotations.extend(annotations)\n",
        "\n",
        "            number_of_frames = len(annotations)\n",
        "\n",
        "            # Keep a record for video and frame indices\n",
        "            # Keep a record of video_indices\n",
        "            self.video_indices.extend([video_index] * number_of_frames)\n",
        "            # Keep a record of frame indices\n",
        "            self.frame_indices.extend(range(number_of_frames))\n",
        "            # Add padded keypoints for each video key\n",
        "            self.X[video_key] = np.pad(\n",
        "                self.pose_dict[video_key][self.sequence_key], self.pad_width)\n",
        "\n",
        "        self.y = np.array(self.action_annotations)\n",
        "        # self.y = self.classname_to_index(self.action_annotations) # convert text labels to indices\n",
        "        self.X_dtype = self.X[video_key].dtype  # Store D_types of X\n",
        "\n",
        "        # generate a global index list for all data points\n",
        "        self.indices = np.arange(len(self.frame_indices))\n",
        "\n",
        "    def __len__(self):\n",
        "        ct = len(self.indices) // self.batch_size\n",
        "        ct += int((len(self.indices) % self.batch_size) > 0)\n",
        "        return ct\n",
        "\n",
        "    def get_X(self, data_index):\n",
        "        \"\"\"\n",
        "        Obtains the X value from a particular global index\n",
        "        \"\"\"\n",
        "        # Obtain video key for this datapoint\n",
        "        video_key = self.video_keys[\n",
        "            self.video_indices[data_index]\n",
        "        ]\n",
        "        # Identify the (local) frame_index\n",
        "        # to offset original data padding\n",
        "        frame_index = self.frame_indices[data_index] + self.left_pad\n",
        "        # Slice from beginning of past frames to end of future frames\n",
        "        slice_start_index = frame_index - self.left_pad\n",
        "        slice_end_index = frame_index + self.frame_skip + self.right_pad\n",
        "        assert slice_start_index >= 0\n",
        "        _X = self.X[video_key][\n",
        "            slice_start_index:slice_end_index:self.frame_skip\n",
        "        ]\n",
        "        if self.augment:\n",
        "            _X = self.augment_fn(_X)\n",
        "        return _X\n",
        "\n",
        "    def augment_fn(self, to_augment):\n",
        "        \"\"\" \n",
        "        Augment sequences\n",
        "            * Rotation - All frames in the sequence are rotated by the same angle\n",
        "                using the euler rotation matrix\n",
        "            * Shift - All frames in the sequence are shifted randomly\n",
        "                but by the same amount\n",
        "        \"\"\"\n",
        "        if len(to_augment.shape) != 4:\n",
        "            x = to_augment[:, :28].reshape(-1, 2, 7, 2)\n",
        "        else:\n",
        "            x = to_augment\n",
        "\n",
        "        # Rotate\n",
        "        angle = (np.random.rand()-0.5) * (np.pi * 2)\n",
        "        c, s = np.cos(angle), np.sin(angle)\n",
        "        rot = np.array([[c, -s], [s, c]])\n",
        "        x = np.dot(x, rot)\n",
        "\n",
        "        # Shift - All get shifted together\n",
        "        shift = (np.random.rand(2)-0.5) * 2 * 0.25\n",
        "        x = x + shift\n",
        "\n",
        "        if len(to_augment.shape) != 4:\n",
        "\n",
        "            x = np.concatenate([x.reshape(-1, 28), to_augment[:, 28:]], axis = -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_size = self.batch_size\n",
        "        batch_indices = self.indices[\n",
        "            index*batch_size:(index+1)*batch_size]  # List indexing overflow gets clipped\n",
        "\n",
        "        batch_size = len(batch_indices)  # For the case when list indexing is clipped\n",
        "\n",
        "        X = np.empty((batch_size, *self.dim), self.X_dtype)\n",
        "\n",
        "        for batch_index, data_index in enumerate(batch_indices):\n",
        "            # Obtain the post-processed X value at the said data index\n",
        "            _X = self.get_X(data_index)\n",
        "            # Reshape the _X to the expected dimensions\n",
        "            X[batch_index] = np.reshape(_X, self.dim)\n",
        "\n",
        "        y_vals = self.y[batch_indices]\n",
        "        # Converting to one hot because F1 callback needs one hot\n",
        "        y = keras.utils.to_categorical(y_vals, num_classes=self.n_classes)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4GfYuM5-Sts"
      },
      "source": [
        "## Trainer üèãÔ∏è\n",
        "\n",
        "The trainer class implements a unified interface for using the data Generator we made above.\n",
        "\n",
        "It supports fully connected or 1D convolutional networks, as well as other hyperparameters for the model and the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9etQA5-1-TK9"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow_addons as tfa\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "from utils.model_utils import add_layer\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Custom Trainer class for sequential window data\n",
        "    Setup and manage training for different models\n",
        "    Supports different architectures\n",
        "    \"\"\"\n",
        "    def __init__(self, *,\n",
        "                 train_generator,\n",
        "                 val_generator,\n",
        "                 input_dim,\n",
        "                 num_classes,\n",
        "                 class_to_number=None,\n",
        "                 architecture=\"conv_1d\",\n",
        "                 test_generator=None,\n",
        "                 arch_params={}):\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.class_to_number = class_to_number\n",
        "\n",
        "        self.train_generator = train_generator\n",
        "        self.val_generator = val_generator\n",
        "        self.test_generator = test_generator\n",
        "\n",
        "        self.architecture = architecture\n",
        "        self.arch_params = arch_params\n",
        "\n",
        "    def delete_model(self):\n",
        "        self.model = None\n",
        "\n",
        "    def initialize_model(self, layer_channels=(512, 256), dropout_rate=0.,\n",
        "                         learning_rate=1e-3, conv_size=5):\n",
        "        \"\"\" Instantiate the model based on the architecture \"\"\"\n",
        "        inputs = layers.Input(self.input_dim)\n",
        "        x = layers.BatchNormalization()(inputs)\n",
        "\n",
        "        if self.architecture == 'lstm':\n",
        "            lstm_size = self.arch_params.lstm_size\n",
        "            x = layers.LSTM(lstm_size, activation='tanh')(x)\n",
        "\n",
        "        for ch in layer_channels:\n",
        "            x = add_layer(x, ch, drop=dropout_rate,\n",
        "                          architecture=self.architecture,\n",
        "                          arch_params=self.arch_params)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        metrics = [tfa.metrics.F1Score(num_classes=self.num_classes)]\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "        model = Model(inputs, x)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def _set_model(self, model):\n",
        "        \"\"\" Set an external, provide initialized and compiled keras model \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, epochs=20, class_weight=None, callbacks=[]):\n",
        "        \"\"\" Train the model for given epochs \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"Please Call trainer.initialize_model first\")\n",
        "            return\n",
        "\n",
        "        self.model.fit(self.train_generator,\n",
        "                    #    validation_data=self.val_generator,\n",
        "                       epochs=epochs,\n",
        "                       class_weight=class_weight,\n",
        "                       callbacks=callbacks)\n",
        "\n",
        "    def get_generator_by_mode(self, mode='validation'):\n",
        "        \"\"\" Select the generator - Train, Validation or Test\"\"\"\n",
        "        if mode == 'validation':\n",
        "            return self.val_generator\n",
        "        elif mode == 'train':\n",
        "            return self.train_generator\n",
        "        elif mode == 'test':\n",
        "            return self.test_generator\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_labels(self, generator):\n",
        "        \"\"\" Get all the ground truth labels\"\"\"\n",
        "        y_val = []\n",
        "        for _, y in generator:\n",
        "            y_val.extend(list(y))\n",
        "        y_val = np.argmax(np.array(y_val), axis=-1)\n",
        "        return y_val\n",
        "\n",
        "    def get_prediction_probabilities(self, generator):\n",
        "        \"\"\" Get all the model predictions \"\"\"\n",
        "        return self.model.predict(generator, verbose=True)\n",
        "\n",
        "    def get_predictions(self, generator):\n",
        "        \"\"\" Get all the model predictions \"\"\"\n",
        "        y_pred = self.get_prediction_probabilities(generator)\n",
        "        y_pred = np.argmax(y_pred, axis=-1)\n",
        "        return y_pred\n",
        "\n",
        "    def get_metrics(self, mode='validation'):\n",
        "        \"\"\"\n",
        "        Get metrics - F1, Precision, Recall for each class\n",
        "        \"mode\" can be set to use training or validation data\n",
        "        \"\"\"\n",
        "        generator = self.get_generator_by_mode(mode)\n",
        "\n",
        "        if generator is None:\n",
        "            return None\n",
        "\n",
        "        labels = self.get_labels(generator)\n",
        "        probabilites = self.get_prediction_probabilities(generator)\n",
        "        predictions = np.argmax(probabilites, axis=-1)\n",
        "\n",
        "        f1_scores = sklearn.metrics.f1_score(labels, predictions, average=None)\n",
        "        rec_scores = sklearn.metrics.precision_score(\n",
        "            labels, predictions, average=None)\n",
        "        prec_scores = sklearn.metrics.recall_score(\n",
        "            labels, predictions, average=None)\n",
        "\n",
        "        # Average precsion - all labels not equal to correct label are mistakes\n",
        "        ap_scores = []\n",
        "        for single_label in sorted(np.unique(labels)):\n",
        "            labels_l = labels == single_label\n",
        "            probabilites_l = probabilites[:, single_label] \n",
        "            ap_score_l = sklearn.metrics.average_precision_score(\n",
        "                labels_l, probabilites_l, average='micro')\n",
        "            ap_scores.append(ap_score_l)\n",
        "\n",
        "        classes = sorted(self.class_to_number, key=self.class_to_number.get)\n",
        "        metrics = pd.DataFrame({\"Class\": classes, \"F1\": f1_scores,\n",
        "                                \"Precision\": prec_scores, \"Recall\": rec_scores,\n",
        "                                \"Average Precision\": ap_scores})\n",
        "\n",
        "        if len(classes) > 2:\n",
        "            try:\n",
        "                average_scores = metrics[metrics['Class'] != 'other'].mean()\n",
        "                metrics = metrics.append(average_scores, ignore_index=True)\n",
        "                metrics.iloc[-1, 0] = 'Macro Average'\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6LvO7SU_BFe"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "We'll normalize the data based on the information that the frame size is 1024x570\n",
        "\n",
        "The original data is of shape (sequence length, mouse, x y coordinate, keypoint)\n",
        " = (length, 2, 2, 7)\n",
        "\n",
        " We'll swap the x y and the keypoint axis, which will help in rotation augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHQfl7FU-9yr"
      },
      "source": [
        "def normalize_data(orig_pose_dictionary):\n",
        "  for key in orig_pose_dictionary:\n",
        "    X = orig_pose_dictionary[key]['keypoints']\n",
        "    X = X.transpose((0,1,3,2)) #last axis is x, y coordinates\n",
        "    X[..., 0] = X[..., 0]/1024\n",
        "    X[..., 1] = X[..., 1]/570\n",
        "    orig_pose_dictionary[key]['keypoints'] = X\n",
        "  return orig_pose_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9g6aTzL_Juo"
      },
      "source": [
        "# Train function and inference\n",
        "\n",
        "The below function uses a set of hyperparameters we found with some tuning, though results can be improved with further tuning or other models.\n",
        "\n",
        "You can adjust hyperparameter values using the `config` dictionary defined in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcNnbZvD_I4C"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import tensorflow as tf\n",
        "from utils.load_data import load_mabe_data_task1\n",
        "from utils.dirs import create_dirs\n",
        "from utils.preprocessing import normalize_data, transpose_last_axis\n",
        "from utils.split_data import split_data\n",
        "from utils.seeding import seed_everything\n",
        "from trainers.mab_e_trainer import Trainer\n",
        "from data_generator.mab_e_data_generator import mabe_generator\n",
        "from data_generator.mab_e_data_generator import calculate_input_dim\n",
        "from utils.save_results import save_results\n",
        "\n",
        "\n",
        "def train_task1(results_dir, dataset, vocabulary, test_data, config,\n",
        "                pretrained_model_path=None, skip_training=False, read_features = False):\n",
        "\n",
        "\n",
        "    # Create directories if not present\n",
        "    create_dirs([results_dir])\n",
        "\n",
        "    # Seed for reproducibilty\n",
        "    seed_everything(config.seed)\n",
        "\n",
        "    if not read_features:\n",
        "      sequence_key = 'keypoints'\n",
        "      feature_dim = (2, 7, 2)\n",
        "    else:\n",
        "      sequence_key = 'features'      \n",
        "      feature_dim = (60)\n",
        "\n",
        "    # Transpose last axis, used for augmentation and normalization\n",
        "    dataset = transpose_last_axis(deepcopy(dataset), sequence_key = sequence_key)\n",
        "    test_data = transpose_last_axis(deepcopy(test_data), sequence_key = sequence_key)\n",
        "\n",
        "    # Normalize the x y coordinates\n",
        "    if config.normalize:\n",
        "        dataset = normalize_data(dataset, sequence_key = sequence_key)\n",
        "        test_data = normalize_data(test_data, sequence_key = sequence_key)\n",
        "\n",
        "    # Split the data\n",
        "    train_data, val_data = split_data(dataset,\n",
        "                                      seed=config.seed,\n",
        "                                      vocabulary=vocabulary,\n",
        "                                      test_size=config.val_size,\n",
        "                                      split_videos=config.split_videos)\n",
        "    num_classes = len(vocabulary)\n",
        "\n",
        "    # Calculate the input dimension based on past and future frames\n",
        "    # Also flattens the channels as required by the architecture\n",
        "    input_dim = calculate_input_dim(feature_dim,\n",
        "                                    config.architecture,\n",
        "                                    config.past_frames,\n",
        "                                    config.future_frames)\n",
        "\n",
        "    # Initialize data generators\n",
        "    common_kwargs = {\"batch_size\": config.batch_size,\n",
        "                     \"input_dimensions\": input_dim,\n",
        "                     \"past_frames\": config.past_frames,\n",
        "                     \"future_frames\": config.future_frames,\n",
        "                     \"class_to_number\": vocabulary,\n",
        "                     \"frame_skip\": config.frame_gap}\n",
        "\n",
        "    train_generator = mabe_generator(train_data,\n",
        "                                     augment=config.augment,\n",
        "                                     shuffle=True,\n",
        "                                     sequence_key=sequence_key,\n",
        "                                     kwargs=common_kwargs)\n",
        "\n",
        "    val_generator = mabe_generator(val_data,\n",
        "                                   augment=False,\n",
        "                                   shuffle=False,\n",
        "                                   sequence_key=sequence_key,                                   \n",
        "                                   kwargs=common_kwargs)\n",
        "\n",
        "    test_generator = mabe_generator(test_data,\n",
        "                                    augment=False,\n",
        "                                    shuffle=False,\n",
        "                                    sequence_key=sequence_key,\n",
        "                                    kwargs=common_kwargs)\n",
        "\n",
        "    trainer = Trainer(train_generator=train_generator,\n",
        "                      val_generator=val_generator,\n",
        "                      test_generator=test_generator,\n",
        "                      input_dim=input_dim,\n",
        "                      class_to_number=vocabulary,\n",
        "                      num_classes=num_classes,\n",
        "                      architecture=config.architecture,\n",
        "                      arch_params=config.architecture_parameters)\n",
        "\n",
        "    # In case of only using\n",
        "    if skip_training and pretrained_model_path is not None:\n",
        "        trainer.model = tf.keras.models.load_model(pretrained_model_path)\n",
        "\n",
        "        # Print model summary\n",
        "        trainer.model.summary()\n",
        "\n",
        "        print(\"Skipping Training\")\n",
        "    else:\n",
        "        # Model initialization\n",
        "        trainer.initialize_model(layer_channels=config.layer_channels,\n",
        "                                 dropout_rate=config.dropout_rate,\n",
        "                                 learning_rate=config.learning_rate)\n",
        "        # Print model summary\n",
        "        trainer.model.summary()\n",
        "\n",
        "        # Train model\n",
        "        trainer.train(epochs=config.epochs)\n",
        "\n",
        "    # Get metrics\n",
        "    train_metrics = trainer.get_metrics(mode='train')\n",
        "    val_metrics = trainer.get_metrics(mode='validation')\n",
        "    test_metrics = trainer.get_metrics(mode='test')\n",
        "\n",
        "    # Save the results\n",
        "    save_results(results_dir, 'task1',\n",
        "                 trainer.model, config,\n",
        "                 train_metrics, val_metrics, test_metrics)\n",
        "    \n",
        "    # return predictions on the test set\n",
        "    return trainer.get_predictions(test_generator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1oxwhd4JuEO"
      },
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "# Baseline config - Convolution 1D\n",
        "config = {\"seed\": 42,\n",
        "          \"val_size\": 0.2,\n",
        "          \"split_videos\": False,\n",
        "          \"normalize\": True,\n",
        "          \"past_frames\": 100,\n",
        "          \"future_frames\": 100,\n",
        "          \"frame_gap\": 2,\n",
        "          \"architecture\": \"conv_1D\",\n",
        "          \"architecture_parameters\": EasyDict({\"conv_size\": 5}),\n",
        "          \"batch_size\": 128,\n",
        "          \"learning_rate\": 1e-3,\n",
        "          \"dropout_rate\": 0.5,\n",
        "          \"layer_channels\": (128, 64, 32),\n",
        "          \"epochs\": 15,\n",
        "          \"augment\": False}\n",
        "\n",
        "config = EasyDict(config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vXzfGZ-LU80"
      },
      "source": [
        "Run this cell to train the network characterized by the settings in `config` above, and test it on the test set. If you want to keep the test set separate while doing hyperparameter exploration, you should split training_data into training and validation sets, and pass the validation set in place of the test set here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT1yX0ty_R_I"
      },
      "source": [
        "results_dir = '.'\n",
        "predictions = train_task1(results_dir,\n",
        "            dataset=training_data,\n",
        "            vocabulary=vocab,\n",
        "            test_data=test_data,\n",
        "            config=config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}