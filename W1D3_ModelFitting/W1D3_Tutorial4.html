
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorial 4: Multiple linear regression and polynomial regression &#8212; Neuromatch Computational Neuroscience</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial 5: Model Selection: Bias-variance trade-off" href="W1D3_Tutorial5.html" />
    <link rel="prev" title="Tutorial 3: Confidence intervals and bootstrapping" href="W1D3_Tutorial3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">Neuromatch Computational Neuroscience</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   NMA 2021
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D1_ModelTypes/README.html">
   Model Types (W1D1)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D1_ModelTypes/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D2_ModelingPractice/README.html">
   Modeling Practice (W1D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D2_ModelingPractice/W1D2_Tutorial1.html">
     Neuromatch Academy: Week 1, Day 2, Tutorial
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="README.html">
   Model Fitting (W1D3)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="W1D3_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D4_MachineLearning/README.html">
   Machine Learning (W1D4)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D4_MachineLearning/W1D4_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D4_MachineLearning/W1D4_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W1D5_DimensionalityReduction/README.html">
   Dimensionality Reduction (W1D5)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W1D5_DimensionalityReduction/W1D5_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../W2D2_LinearSystems/README.html">
   Linear Systems (W2D2)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial1.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial2.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial3.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../W2D2_LinearSystems/W2D2_Tutorial4.html">
     Neuromatch Academy 2020, Week 2, Day 2, Tutorial 4
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/W1D3_ModelFitting/W1D3_Tutorial4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/NeuromatchAcademy/course_content/blob/master/book/W1D3_ModelFitting/W1D3_Tutorial4.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Tutorial 4: Multiple linear regression and polynomial regression
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-multiple-linear-regression">
   Section 1: Multiple Linear Regression
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1-ordinary-least-squares-estimator">
     Exercise 1: Ordinary Least Squares Estimator
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-polynomial-regression">
   Section 2:  Polynomial Regression
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-1-design-matrix-for-polynomial-regression">
     Section 2.1: Design matrix for polynomial regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-2-structure-design-matrix">
       Exercise 2: Structure design matrix
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-2-fitting-polynomial-regression-models">
     Section 2.2: Fitting polynomial regression models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-3-fitting-polynomial-regression-models-with-different-orders">
       Exercise 3: Fitting polynomial regression models with different orders
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-2-4-evaluating-fit-quality">
     Section 2.4: Evaluating fit quality
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercise-4-compute-mse-and-compare-models">
       Exercise 4: Compute MSE and compare models
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W1D3_ModelFitting/W1D3_Tutorial4.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="section" id="tutorial-4-multiple-linear-regression-and-polynomial-regression">
<h1>Tutorial 4: Multiple linear regression and polynomial regression<a class="headerlink" href="#tutorial-4-multiple-linear-regression-and-polynomial-regression" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 1, Day 3: Model Fitting</strong></p>
<p><strong>By Neuromatch Academy</strong>
<strong>Content creators</strong>: Pierre-Étienne Fiquet, Anqi Wu, Alex Hyafil with help from Byron Galbraith, Ella Batty</p>
<p><strong>Content reviewers</strong>: Lina Teichmann, Saeed Salehi, Patrick Mineault, Michael Waskom</p>
<hr class="docutils" />
<p>#Tutorial Objectives</p>
<p>This is Tutorial 4 of a series on fitting models to data. We start with simple linear regression, using least squares optimization (Tutorial 1) and Maximum Likelihood Estimation (Tutorial 2). We will use bootstrapping to build confidence intervals around the inferred linear model parameters (Tutorial 3). We’ll finish our exploration of regression models by generalizing to multiple linear regression and polynomial regression (Tutorial 4). We end by learning how to choose between these various models. We discuss the bias-variance trade-off (Tutorial 5) and Cross Validation for model selection (Tutorial 6).</p>
<p>In this tutorial, we will generalize the regression model to incorporate multiple features.</p>
<ul class="simple">
<li><p>Learn how to structure inputs for regression using the ‘Design Matrix’</p></li>
<li><p>Generalize the MSE for multiple features using the ordinary least squares estimator</p></li>
<li><p>Visualize data and model fit in multiple dimensions</p></li>
<li><p>Fit polynomial regression models of different complexity</p></li>
<li><p>Plot and evaluate the polynomial regression fits</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Video 1: Multiple Linear Regression and Polynomial Regression</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;d4nfTki6Ejc&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Video available at https://youtube.com/watch?v=&quot;</span> <span class="o">+</span> <span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
<span class="n">video</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Video available at https://youtube.com/watch?v=d4nfTki6Ejc
</pre></div>
</div>
<div class="output text_html">
<iframe
    width="854"
    height="480"
    src="https://www.youtube.com/embed/d4nfTki6Ejc?fs=1"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure Settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Helper Functions</span>

<span class="k">def</span> <span class="nf">plot_fitted_polynomials</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot; Plot polynomials of different orders</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>
<span class="sd">    theta_hat (dict): polynomial regression weights for different orders</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">X_design</span> <span class="o">@</span> <span class="n">theta_hat</span><span class="p">[</span><span class="n">order</span><span class="p">]);</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;C0.&#39;</span><span class="p">);</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;order </span><span class="si">{</span><span class="n">o</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;polynomial fits&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-1-multiple-linear-regression">
<h1>Section 1: Multiple Linear Regression<a class="headerlink" href="#section-1-multiple-linear-regression" title="Permalink to this headline">¶</a></h1>
<p>Now that we have considered the univariate case and how to produce confidence intervals for our estimator, we turn to the general linear regression case, where we can have more than one regressor, or feature, in our input.</p>
<p>Recall that our original univariate linear model was given as</p>
<p>\begin{align}
y = \theta x + \epsilon
\end{align}</p>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the slope and <span class="math notranslate nohighlight">\(\epsilon\)</span> some noise. We can easily extend this to the multivariate scenario by adding another parameter for each additional feature</p>
<p>\begin{align}
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + … +\theta_d x_d + \epsilon
\end{align}</p>
<p>where <span class="math notranslate nohighlight">\(\theta_0\)</span> is the intercept and <span class="math notranslate nohighlight">\(d\)</span> is the number of features (it is also the dimensionality of our input).</p>
<p>We can condense this succinctly using vector notation for a single data point</p>
<p>\begin{align}
y_i = \boldsymbol{\theta}^{\top}\mathbf{x}_i + \epsilon
\end{align}</p>
<p>and fully in matrix form</p>
<p>\begin{align}
\mathbf{y} = \mathbf{X}\boldsymbol{\theta} + \mathbf{\epsilon}
\end{align}</p>
<p>where <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a vector of measurements, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a matrix containing the feature values (columns) for each input sample (rows), and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is our parameter vector.</p>
<p>This matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is often referred to as the “<a class="reference external" href="https://en.wikipedia.org/wiki/Design_matrix">design matrix</a>”.</p>
<p>For this tutorial we will focus on the two-dimensional case (<span class="math notranslate nohighlight">\(d=2\)</span>), which allows us to easily visualize our results. As an example, think of a situation where a scientist records the spiking response of a retinal ganglion cell to patterns of light signals that vary in contrast and in orientation. Then contrast and orientation values can be used as features / regressors to predict the cells response.</p>
<p>In this case our model can be writen as:</p>
<p>\begin{align}
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \epsilon
\end{align}</p>
<p>or in matrix form where</p>
<p>\begin{align}
\mathbf{X} =
\begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} \
1 &amp; x_{2,1} &amp; x_{2,2} \
\vdots &amp; \vdots &amp; \vdots \
1 &amp; x_{n,1} &amp; x_{n,2}
\end{bmatrix},
\boldsymbol{\theta} =
\begin{bmatrix}
\theta_0 \
\theta_1 \
\theta_2 \
\end{bmatrix}
\end{align}</p>
<p>For our actual exploration dataset we shall set <span class="math notranslate nohighlight">\(\boldsymbol{\theta}=[0, -2, -3]\)</span> and draw <span class="math notranslate nohighlight">\(N=40\)</span> noisy samples from <span class="math notranslate nohighlight">\(x \in [-2,2)\)</span>. Note that setting the value of <span class="math notranslate nohighlight">\(\theta_0 = 0\)</span> effectively ignores the offset term.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to simulate some data</span>

<span class="c1"># Set random seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>

<span class="c1"># Set parameters</span>
<span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">]</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">40</span>

<span class="c1"># Draw x and calculate y</span>
<span class="n">n_regressors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">noise</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span>
    <span class="n">zlabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/W1D3_Tutorial4_11_0.png" src="../_images/W1D3_Tutorial4_11_0.png" />
</div>
</div>
<p>Now that we have our dataset, we want to find an optimal vector of paramters <span class="math notranslate nohighlight">\(\boldsymbol{\hat\theta}\)</span>. Recall our analytic solution to minimizing MSE for a single regressor:</p>
<p>\begin{align}
\hat\theta = \frac{\sum_{i=1}^N x_i y_i}{\sum_{i=1}^N x_i^2}.
\end{align}</p>
<p>The same holds true for the multiple regressor case, only now expressed in matrix form</p>
<p>\begin{align}
\boldsymbol{\hat\theta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}.
\end{align}</p>
<p>This is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a> (OLS) estimator.</p>
<div class="section" id="exercise-1-ordinary-least-squares-estimator">
<h2>Exercise 1: Ordinary Least Squares Estimator<a class="headerlink" href="#exercise-1-ordinary-least-squares-estimator" title="Permalink to this headline">¶</a></h2>
<p>In this exercise you will implement the OLS approach to estimating <span class="math notranslate nohighlight">\(\boldsymbol{\hat\theta}\)</span> from the design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and measurement vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. You can use the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> symbol for matrix multiplication, <code class="docutils literal notranslate"><span class="pre">.T</span></code> for transpose, and <code class="docutils literal notranslate"><span class="pre">np.linalg.inv</span></code> for matrix inversion.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Ordinary least squares estimator for linear regression.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): design matrix of shape (n_samples, n_regressors)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: estimated parameter values of shape (n_regressors)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">######################################################################</span>
  <span class="c1">## TODO for students: solve for the optimal parameter vector using OLS</span>
  <span class="c1"># Fill out function and remove</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Student exercise: solve for theta_hat vector using OLS&quot;</span><span class="p">)</span>
  <span class="c1">######################################################################</span>

  <span class="c1"># Compute theta_hat using OLS</span>
  <span class="n">theta_hat</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">theta_hat</span>


<span class="c1"># Uncomment below to test your function</span>
<span class="c1"># theta_hat = ordinary_least_squares(X, y)</span>
<span class="c1"># print(theta_hat)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Ordinary least squares estimator for linear regression.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (ndarray): design matrix of shape (n_samples, n_regressors)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: estimated parameter values of shape (n_regressors)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Compute theta_hat using OLS</span>
  <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

  <span class="k">return</span> <span class="n">theta_hat</span>


<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">theta_hat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.13861386 -2.09395731 -3.16370742]
</pre></div>
</div>
</div>
</div>
<p>After filling in this function, you should see that <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> = [ 0.13861386, -2.09395731, -3.16370742]</p>
<p>Now that we have our <span class="math notranslate nohighlight">\(\mathbf{\hat\theta}\)</span>, we can obtain <span class="math notranslate nohighlight">\(\mathbf{\hat y}\)</span> and thus our mean squared error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute predicted data</span>
<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta_hat</span>

<span class="c1"># Compute MSE</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE = 0.91
</pre></div>
</div>
</div>
</div>
<p>Finally, the following code will plot a geometric visualization of the data points (blue) and fitted plane.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to visualize data and predicted plane</span>

<span class="n">theta_hat</span> <span class="o">=</span> <span class="n">ordinary_least_squares</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mi">50</span><span class="n">j</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mi">50</span><span class="n">j</span><span class="p">]</span>
<span class="n">y_hat_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">flatten</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">theta_hat</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">y_hat_grid</span> <span class="o">=</span> <span class="n">y_hat_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">y_hat_grid</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span>
                <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
          <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
          <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_hat</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
          <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span>
    <span class="n">zlabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/W1D3_Tutorial4_20_0.png" src="../_images/W1D3_Tutorial4_20_0.png" />
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="section-2-polynomial-regression">
<h1>Section 2:  Polynomial Regression<a class="headerlink" href="#section-2-polynomial-regression" title="Permalink to this headline">¶</a></h1>
<p>So far today, you learned how to predict outputs from inputs by fitting a linear regression model. We can now model all sort of relationships, including in neuroscience!</p>
<p>One potential problem with this approach is the simplicity of the model. Linear regression, as the name implies, can only capture a linear relationship between the inputs and outputs. Put another way, the predicted outputs are only a weighted sum of the inputs. What if there are more complicated computations happening? Luckily, many more complex models exist (and you will encounter many more over the next 3 weeks). One model that is still very simple to fit and understand, but captures more complex relationships, is <strong>polynomial regression</strong>, an extension of linear regression.</p>
<p>Since polynomial regression is an extension of linear regression, everything you learned so far will come in handy now! The goal is the same: we want to predict the dependent variable <span class="math notranslate nohighlight">\(y_{n}\)</span> given the input values <span class="math notranslate nohighlight">\(x_{n}\)</span>. The key change is the type of relationship between inputs and outputs that the model can capture.</p>
<p>Linear regression models predict the outputs as a weighted sum of the inputs:</p>
<div class="math notranslate nohighlight">
\[y_{n}= \theta_0 + \theta x_{n} + \epsilon_{n}\]</div>
<p>With polynomial regression, we model the outputs as a polynomial equation based on the inputs. For example, we can model the outputs as:</p>
<div class="math notranslate nohighlight">
\[y_{n}= \theta_0 + \theta_1 x_{n} + \theta_2 x_{n}^2 + \theta_3 x_{n}^3 + \epsilon_{n}\]</div>
<p>We can change how complex a polynomial is fit by changing the order of the polynomial. The order of a polynomial refers to the highest power in the polynomial. The equation above is a third order polynomial because the highest value x is raised to is 3. We could add another term (<span class="math notranslate nohighlight">\(+ \theta_4 x_{n}^4\)</span>) to model an order 4 polynomial and so on.</p>
<p>First, we will simulate some data to practice fitting polynomial regression models. We will generate random inputs <span class="math notranslate nohighlight">\(x\)</span> and then compute y according to <span class="math notranslate nohighlight">\(y = x^2 - x - 2 \)</span>, with some extra noise both in the input and the output to make the model fitting exercise closer to a real life situation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to simulate some data</span>

<span class="c1"># setting a fixed seed to our random number generator ensures we will always</span>
<span class="c1"># get the same psuedorandom number sequence</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>  <span class="c1"># inputs uniformly sampled from [-2, 2.5)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">2</span>   <span class="c1"># computing the outputs</span>

<span class="n">output_noise</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">y</span> <span class="o">+=</span> <span class="n">output_noise</span>  <span class="c1"># adding some output noise</span>

<span class="n">input_noise</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">x</span> <span class="o">+=</span> <span class="n">input_noise</span>  <span class="c1"># adding some input noise</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># produces a scatter plot</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/W1D3_Tutorial4_24_0.png" src="../_images/W1D3_Tutorial4_24_0.png" />
</div>
</div>
<div class="section" id="section-2-1-design-matrix-for-polynomial-regression">
<h2>Section 2.1: Design matrix for polynomial regression<a class="headerlink" href="#section-2-1-design-matrix-for-polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>Now we have the basic idea of polynomial regression and some noisy data, let’s begin! The key difference between fitting a linear regression model and a polynomial regression model lies in how we structure the input variables.</p>
<p>For linear regression, we used <span class="math notranslate nohighlight">\(X = x\)</span> as the input data. To add a constant bias (a y-intercept in a 2-D plot), we use <span class="math notranslate nohighlight">\(X = \big[ \boldsymbol 1, x \big]\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol 1\)</span> is a column of ones.  When fitting, we learn a weight for each column of this matrix. So we learn a weight that multiples with column 1 - in this case that column is all ones so we gain the bias parameter (<span class="math notranslate nohighlight">\(+ \theta_0\)</span>). We also learn a weight for every column, or every feature of x, as we learned in Section 1.</p>
<p>This matrix <span class="math notranslate nohighlight">\(X\)</span> that we use for our inputs is known as a <strong>design matrix</strong>. We want to create our design matrix so we learn weights for <span class="math notranslate nohighlight">\(x^2, x^3,\)</span> etc. Thus, we want to build our design matrix <span class="math notranslate nohighlight">\(X\)</span> for polynomial regression of order <span class="math notranslate nohighlight">\(k\)</span> as:</p>
<div class="math notranslate nohighlight">
\[X = \big[ \boldsymbol 1 , x^1, x^2 , \ldots , x^k \big],\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> is the vector the same length as <span class="math notranslate nohighlight">\(x\)</span> consisting of of all ones, and <span class="math notranslate nohighlight">\(x^p\)</span> is the vector or matrix <span class="math notranslate nohighlight">\(x\)</span> with all elements raised to the power <span class="math notranslate nohighlight">\(p\)</span>. Note that <span class="math notranslate nohighlight">\(\boldsymbol{1} = x^0\)</span> and <span class="math notranslate nohighlight">\(x^1 = x\)</span></p>
<div class="section" id="exercise-2-structure-design-matrix">
<h3>Exercise 2: Structure design matrix<a class="headerlink" href="#exercise-2-structure-design-matrix" title="Permalink to this headline">¶</a></h3>
<p>Create a function (<code class="docutils literal notranslate"><span class="pre">make_design_matrix</span></code>) that structures the design matrix given the input data and the order of the polynomial you wish to fit. We will print part of this design matrix for our data and order 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create the design matrix of inputs for use in polynomial regression</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    order (scalar): polynomial regression order</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: design matrix for polynomial regression of shape (samples, order+1)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1">########################################################################</span>
  <span class="c1">## TODO for students: create the design matrix ##</span>
  <span class="c1"># Fill out function and remove</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Student exercise: create the design matrix&quot;</span><span class="p">)</span>
  <span class="c1">########################################################################</span>

  <span class="c1"># Broadcast to shape (n x 1) so dimensions work</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

  <span class="c1">#if x has more than one feature, we don&#39;t want multiple columns of ones so we assign</span>
  <span class="c1"># x^0 here</span>
  <span class="n">design_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Loop through rest of degrees and stack columns (hint: np.hstack)</span>
  <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">design_matrix</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">design_matrix</span>


<span class="c1"># Uncomment to test your function</span>
<span class="n">order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># X_design = make_design_matrix(x, order)</span>
<span class="c1"># print(X_design[0:2, 0:2])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create the design matrix of inputs for use in polynomial regression</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (samples,)</span>
<span class="sd">    order (scalar): polynomial regression order</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray: design matrix for polynomial regression of shape (samples, order+1)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Broadcast to shape (n x 1) so dimensions work</span>
  <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>

  <span class="c1">#if x has more than one feature, we don&#39;t want multiple columns of ones so we assign</span>
  <span class="c1"># x^0 here</span>
  <span class="n">design_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Loop through rest of degrees and stack columns (hint: np.hstack)</span>
  <span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">design_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">design_matrix</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="n">degree</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">design_matrix</span>


<span class="n">order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_design</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.         -1.51194917]
 [ 1.         -0.35259945]]
</pre></div>
</div>
</div>
</div>
<p>You should see that the printed section of this design matrix is <code class="docutils literal notranslate"><span class="pre">[[</span> <span class="pre">1.</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-1.51194917]</span>&#160; <span class="pre">[</span> <span class="pre">1.</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">-0.35259945]]</span></code></p>
</div>
</div>
<div class="section" id="section-2-2-fitting-polynomial-regression-models">
<h2>Section 2.2: Fitting polynomial regression models<a class="headerlink" href="#section-2-2-fitting-polynomial-regression-models" title="Permalink to this headline">¶</a></h2>
<p>Now that we have the inputs structured correctly in our design matrix, fitting a polynomial regression is the same as fitting a linear regression model! All of the polynomial structure we need to learn is contained in how the inputs are structured in the design matrix. We can use the same least squares solution we computed in previous exercises.</p>
<div class="section" id="exercise-3-fitting-polynomial-regression-models-with-different-orders">
<h3>Exercise 3: Fitting polynomial regression models with different orders<a class="headerlink" href="#exercise-3-fitting-polynomial-regression-models-with-different-orders" title="Permalink to this headline">¶</a></h3>
<p>Here, we will fit polynomial regression models to find the regression coefficients (<span class="math notranslate nohighlight">\(\theta_0, \theta_1, \theta_2,\)</span> …) by solving the least squares problem. Create a function <code class="docutils literal notranslate"><span class="pre">solve_poly_reg</span></code> that loops over different order polynomials (up to <code class="docutils literal notranslate"><span class="pre">max_order</span></code>), fits that model, and saves out the weights for each. You may invoke the <code class="docutils literal notranslate"><span class="pre">ordinary_least_squares</span></code> function.</p>
<p>We will then qualitatively inspect the quality of our fits for each order by plotting the fitted polynomials on top of the data. In order to see smooth curves, we evaluate the fitted polynomials on a grid of <span class="math notranslate nohighlight">\(x\)</span> values (ranging between the largest and smallest of the inputs present in the dataset).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">solve_poly_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fit a polynomial regression model for each order 0 through max_order.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>
<span class="sd">    max_order (scalar): max order for polynomial fits</span>

<span class="sd">  Returns:</span>
<span class="sd">    dict: fitted weights for each polynomial model (dict key is order)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Create a dictionary with polynomial order as keys,</span>
  <span class="c1"># and np array of theta_hat (weights) as the values</span>
  <span class="n">theta_hats</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="c1"># Loop over polynomial orders from 0 through max_order</span>
  <span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>

    <span class="c1">##################################################################################</span>
    <span class="c1">## TODO for students: Create design matrix and fit polynomial model for this order</span>
    <span class="c1"># Fill out function and remove</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Student exercise: fit a polynomial model&quot;</span><span class="p">)</span>
    <span class="c1">##################################################################################</span>

    <span class="c1"># Create design matrix</span>
    <span class="n">X_design</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Fit polynomial model</span>
    <span class="n">this_theta</span> <span class="o">=</span> <span class="o">...</span>

    <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="n">this_theta</span>

  <span class="k">return</span> <span class="n">theta_hats</span>


<span class="c1"># Uncomment to test your function</span>
<span class="n">max_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># theta_hats = solve_poly_reg(x, y, max_order)</span>
<span class="c1"># plot_fitted_polynomials(x, y, theta_hats)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">solve_poly_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_order</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Fit a polynomial regression model for each order 0 through max_order.</span>

<span class="sd">  Args:</span>
<span class="sd">    x (ndarray): input vector of shape (n_samples)</span>
<span class="sd">    y (ndarray): vector of measurements of shape (n_samples)</span>
<span class="sd">    max_order (scalar): max order for polynomial fits</span>

<span class="sd">  Returns:</span>
<span class="sd">    dict: fitted weights for each polynomial model (dict key is order)</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Create a dictionary with polynomial order as keys,</span>
  <span class="c1"># and np array of theta_hat (weights) as the values</span>
  <span class="n">theta_hats</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="c1"># Loop over polynomial orders from 0 through max_order</span>
  <span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>

    <span class="c1"># Create design matrix</span>
    <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>

    <span class="c1"># Fit polynomial model</span>
    <span class="n">this_theta</span> <span class="o">=</span> <span class="n">ordinary_least_squares</span><span class="p">(</span><span class="n">X_design</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">]</span> <span class="o">=</span> <span class="n">this_theta</span>

  <span class="k">return</span> <span class="n">theta_hats</span>


<span class="n">max_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">theta_hats</span> <span class="o">=</span> <span class="n">solve_poly_reg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_order</span><span class="p">)</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_fitted_polynomials</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta_hats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/W1D3_Tutorial4_35_0.png" src="../_images/W1D3_Tutorial4_35_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="section-2-4-evaluating-fit-quality">
<h2>Section 2.4: Evaluating fit quality<a class="headerlink" href="#section-2-4-evaluating-fit-quality" title="Permalink to this headline">¶</a></h2>
<p>As with linear regression, we can compute mean squared error (MSE) to get a sense of how well the model fits the data.</p>
<p>We compute MSE as:</p>
<div class="math notranslate nohighlight">
\[ MSE = \frac 1 N ||y - \hat y||^2 = \frac 1 N \sum_{i=1}^N (y_i - \hat y_i)^2 \]</div>
<p>where the predicted values for each model are given by <span class="math notranslate nohighlight">\( \hat y = X \hat \theta\)</span>.</p>
<p><em>Which model (i.e. which polynomial order) do you think will have the best MSE?</em></p>
<div class="section" id="exercise-4-compute-mse-and-compare-models">
<h3>Exercise 4: Compute MSE and compare models<a class="headerlink" href="#exercise-4-compute-mse-and-compare-models" title="Permalink to this headline">¶</a></h3>
<p>We will compare the MSE for different polynomial orders with a bar plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mse_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_list</span><span class="p">:</span>

  <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>

  <span class="c1">############################################################################</span>
  <span class="c1"># TODO for students: Compute MSE (fill in ... sections)</span>
  <span class="c1">#############################################################################</span>

  <span class="c1"># Get prediction for the polynomial regression model of this order</span>
  <span class="n">y_hat</span> <span class="o">=</span> <span class="o">...</span>

  <span class="c1"># Compute the residuals</span>
  <span class="n">residuals</span> <span class="o">=</span> <span class="o">...</span>

  <span class="c1"># Compute the MSE</span>
  <span class="n">mse</span> <span class="o">=</span> <span class="o">...</span>

  <span class="n">mse_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Uncomment once above exercise is complete</span>
<span class="c1"># ax.bar(order_list, mse_list)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Comparing Polynomial Fits&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Polynomial order&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;MSE&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.5, 1.0, &#39;Comparing Polynomial Fits&#39;),
 Text(0.5, 0, &#39;Polynomial order&#39;),
 Text(0, 0.5, &#39;MSE&#39;)]
</pre></div>
</div>
<img alt="../_images/W1D3_Tutorial4_38_1.png" src="../_images/W1D3_Tutorial4_38_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="n">mse_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">max_order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_list</span><span class="p">:</span>

  <span class="n">X_design</span> <span class="o">=</span> <span class="n">make_design_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>

  <span class="c1"># Get prediction for the polynomial regression model of this order</span>
  <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X_design</span> <span class="o">@</span> <span class="n">theta_hats</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>

  <span class="c1"># Compute the residuals</span>
  <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

  <span class="c1"># Compute the MSE</span>
  <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">residuals</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

  <span class="n">mse_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>

<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">order_list</span><span class="p">,</span> <span class="n">mse_list</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Comparing Polynomial Fits&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Polynomial order&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;MSE&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/W1D3_Tutorial4_39_0.png" src="../_images/W1D3_Tutorial4_39_0.png" />
</div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Linear regression generalizes naturally to multiple dimensions</p></li>
<li><p>Linear algebra affords us the mathematical tools to reason and solve such problems beyond the two dimensional case</p></li>
<li><p>To change from a linear regression model to a polynomial regression model, we only have to change how the input data is structured</p></li>
<li><p>We can choose the complexity of the model by changing the order of the polynomial model fit</p></li>
<li><p>Higher order polynomial models tend to have lower MSE on the data they’re fit with</p></li>
</ul>
<p><strong>Note</strong>: In practice, multidimensional least squares problems can be solved very efficiently (thanks to numerical routines such as LAPACK).</p>
<p><strong>Suggested readings</strong></p>
<p><a class="reference external" href="http://vmls-book.stanford.edu/">Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares</a>
Stephen Boyd and Lieven Vandenberghe</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./W1D3_ModelFitting"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="W1D3_Tutorial3.html" title="previous page">Tutorial 3: Confidence intervals and bootstrapping</a>
    <a class='right-next' id="next-link" href="W1D3_Tutorial5.html" title="next page">Tutorial 5: Model Selection: Bias-variance trade-off</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Neuromatch<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>